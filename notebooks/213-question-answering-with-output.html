
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Interactive question answering with OpenVINO &#8212; OpenVINO™  documentation</title>
    
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/tabs.css" type="text/css" />
    <script src="../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/tabs.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/custom.js"></script>
    <script src="../_static/js/graphs.js"></script>
    <script src="../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/notebooks/213-question-answering-with-output.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PaddlePaddle Image Classification with OpenVINO" href="214-vision-paddle-classification-with-output.html" />
    <link rel="prev" title="Style Transfer on ONNX Models with OpenVINO" href="212-onnx-style-transfer-with-output.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/get_started.html">
  Get Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/resources.html">
  Resources
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/latest/notebooks/213-question-answering-with-output.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/cn/latest/notebooks/213-question-answering-with-output.html">Chinese</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="001-hello-world-with-output.html">
   Hello Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="002-openvino-api-with-output.html">
   OpenVINO API Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003-hello-segmentation-with-output.html">
   Hello Image Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="004-hello-detection-with-output.html">
   Hello Object Detection
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="101-tensorflow-to-openvino-with-output.html">
   Convert a TensorFlow Model to OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="102-pytorch-onnx-to-openvino-with-output.html">
   Convert a PyTorch Model to ONNX and OpenVINO IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="103-paddle-onnx-to-openvino-classification-with-output.html">
   Convert a PaddlePaddle Model to ONNX and OpenVINO IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="104-model-tools-with-output.html">
   Working with Open Model Zoo Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="105-language-quantize-bert-with-output.html">
   Quantize NLP models with OpenVINO Post-Training Optimization Tool ​
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="106-auto-device-with-output.html">
   Automatic Device Selection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="111-detection-quantization-with-output.html">
   Object Detection Quantization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="112-pytorch-post-training-quantization-nncf-with-output.html">
   Post-Training Quantization of PyTorch models with NNCF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="113-image-classification-quantization-with-output.html">
   Quantization of Image Classification Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="114-quantization-simplified-mode-with-output.html">
   INT8 Quantization with Post-training Optimization Tool (POT) in Simplified Mode tutorial
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="201-vision-monodepth-with-output.html">
   Monodepth Estimation with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-image-with-output.html">
   Single Image Super Resolution with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-video-with-output.html">
   Video Super Resolution with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="205-vision-background-removal-with-output.html">
   Image Background Removal with U^2-Net and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="206-vision-paddlegan-anime-with-output.html">
   Photos to Anime with PaddleGAN and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="207-vision-paddlegan-superresolution-with-output.html">
   Super Resolution with PaddleGAN and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="208-optical-character-recognition-with-output.html">
   Optical Character Recognition (OCR) with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="209-handwritten-ocr-with-output.html">
   Handwritten Chinese and Japanese OCR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="210-ct-scan-live-inference-with-output.html">
   Live Inference and Benchmark CT-scan Data with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="211-speech-to-text-with-output.html">
   Speech to Text with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="212-onnx-style-transfer-with-output.html">
   Style Transfer on ONNX Models with OpenVINO
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Interactive question answering with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="214-vision-paddle-classification-with-output.html">
   PaddlePaddle Image Classification with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="215-image-inpainting-with-output.html">
   Image In-painting with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="217-vision-deblur-with-output.html">
   Deblur Photos with DeblurGAN-v2 and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="218-vehicle-detection-and-recognition-with-output.html">
   Vehicle Detection And Recognition with OpenVINO
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-with-output.html">
   From Training to Deployment with TensorFlow and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-pot-with-output.html">
   Post-Training Quantization with TensorFlow Classification Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="302-pytorch-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using PyTorch framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="305-tensorflow-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using TensorFlow Framework
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="401-object-detection-with-output.html">
   Live Object Detection with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="402-pose-estimation-with-output.html">
   Live Human Pose Estimation with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="403-action-recognition-webcam-with-output.html">
   Human Action Recognition with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="405-paddle-ocr-webcam-with-output.html">
   PaddleOCR with OpenVINO
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-model">
   The model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#download-the-model">
     Download the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-the-model">
     Load the model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#processing">
   Processing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preprocessing">
     Preprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#postprocessing">
     Postprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#main-processing-function">
     Main Processing Function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#run">
   Run
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#run-on-local-paragraphs">
     Run on local paragraphs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#run-on-websites">
     Run on websites
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <section id="interactive-question-answering-with-openvino">
<h1>Interactive question answering with OpenVINO<a class="headerlink" href="#interactive-question-answering-with-openvino" title="Permalink to this headline">¶</a></h1>
<p>This demo shows interactive question answering with OpenVINO. We use
<a class="reference external" href="https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/intel/bert-small-uncased-whole-word-masking-squad-int8-0002">small BERT-large-like
model</a>
distilled and quantized to INT8 on SQuAD v1.1 training set from larger
BERT-large model. The model comes from <a class="reference external" href="https://github.com/openvinotoolkit/open_model_zoo/">Open Model
Zoo</a>. At the
bottom of this notebook, you will see live inference results from your
inputs.</p>
<section id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">¶</a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">urllib</span> <span class="kn">import</span> <span class="n">parse</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span>

<span class="kn">import</span> <span class="nn">html_reader</span> <span class="k">as</span> <span class="nn">reader</span>
<span class="kn">import</span> <span class="nn">tokens_bert</span> <span class="k">as</span> <span class="nn">tokens</span>
</pre></div>
</div>
</section>
<section id="the-model">
<h2>The model<a class="headerlink" href="#the-model" title="Permalink to this headline">¶</a></h2>
<section id="download-the-model">
<h3>Download the model<a class="headerlink" href="#download-the-model" title="Permalink to this headline">¶</a></h3>
<p>We use <code class="docutils literal notranslate"><span class="pre">omz_downloader</span></code>, which is a command-line tool from the
<code class="docutils literal notranslate"><span class="pre">openvino-dev</span></code> package. <code class="docutils literal notranslate"><span class="pre">omz_downloader</span></code> automatically creates a
directory structure and downloads the selected model. If the model is
already downloaded, this step is skipped.</p>
<p>You can download and use any of the following models:
<code class="docutils literal notranslate"><span class="pre">bert-large-uncased-whole-word-masking-squad-0001</span></code>,
<code class="docutils literal notranslate"><span class="pre">bert-large-uncased-whole-word-masking-squad-int8-0001</span></code>,
<code class="docutils literal notranslate"><span class="pre">bert-small-uncased-whole-word-masking-squad-0001</span></code>,
<code class="docutils literal notranslate"><span class="pre">bert-small-uncased-whole-word-masking-squad-0002</span></code>,
<code class="docutils literal notranslate"><span class="pre">bert-small-uncased-whole-word-masking-squad-int8-0002</span></code>, just change
the model name below. Any of these models are already converted to
OpenVINO Intermediate Representation (IR), so there is no need to use
<code class="docutils literal notranslate"><span class="pre">omz_converter</span></code>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># directory where model will be downloaded</span>
<span class="n">base_model_dir</span> <span class="o">=</span> <span class="s2">&quot;model&quot;</span>

<span class="c1"># desired precision</span>
<span class="n">precision</span> <span class="o">=</span> <span class="s2">&quot;FP16-INT8&quot;</span>

<span class="c1"># model name as named in Open Model Zoo</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;bert-small-uncased-whole-word-masking-squad-int8-0002&quot;</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;model/intel/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">precision</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.xml&quot;</span>
<span class="n">model_weights_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;model/intel/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">precision</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.bin&quot;</span>

<span class="n">download_command</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;omz_downloader &quot;</span> \
                   <span class="sa">f</span><span class="s2">&quot;--name </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> &quot;</span> \
                   <span class="sa">f</span><span class="s2">&quot;--precision </span><span class="si">{</span><span class="n">precision</span><span class="si">}</span><span class="s2"> &quot;</span> \
                   <span class="sa">f</span><span class="s2">&quot;--output_dir </span><span class="si">{</span><span class="n">base_model_dir</span><span class="si">}</span><span class="s2"> &quot;</span> \
                   <span class="sa">f</span><span class="s2">&quot;--cache_dir </span><span class="si">{</span><span class="n">base_model_dir</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="o">!</span> <span class="nv">$download_command</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">################|| Downloading bert-small-uncased-whole-word-masking-squad-int8-0002 ||################</span>

<span class="o">==========</span> <span class="n">Downloading</span> <span class="n">model</span><span class="o">/</span><span class="n">intel</span><span class="o">/</span><span class="n">bert</span><span class="o">-</span><span class="n">small</span><span class="o">-</span><span class="n">uncased</span><span class="o">-</span><span class="n">whole</span><span class="o">-</span><span class="n">word</span><span class="o">-</span><span class="n">masking</span><span class="o">-</span><span class="n">squad</span><span class="o">-</span><span class="n">int8</span><span class="o">-</span><span class="mi">0002</span><span class="o">/</span><span class="n">vocab</span><span class="o">.</span><span class="n">txt</span>


<span class="o">==========</span> <span class="n">Downloading</span> <span class="n">model</span><span class="o">/</span><span class="n">intel</span><span class="o">/</span><span class="n">bert</span><span class="o">-</span><span class="n">small</span><span class="o">-</span><span class="n">uncased</span><span class="o">-</span><span class="n">whole</span><span class="o">-</span><span class="n">word</span><span class="o">-</span><span class="n">masking</span><span class="o">-</span><span class="n">squad</span><span class="o">-</span><span class="n">int8</span><span class="o">-</span><span class="mi">0002</span><span class="o">/</span><span class="n">FP16</span><span class="o">-</span><span class="n">INT8</span><span class="o">/</span><span class="n">bert</span><span class="o">-</span><span class="n">small</span><span class="o">-</span><span class="n">uncased</span><span class="o">-</span><span class="n">whole</span><span class="o">-</span><span class="n">word</span><span class="o">-</span><span class="n">masking</span><span class="o">-</span><span class="n">squad</span><span class="o">-</span><span class="n">int8</span><span class="o">-</span><span class="mf">0002.</span><span class="n">xml</span>


<span class="o">==========</span> <span class="n">Downloading</span> <span class="n">model</span><span class="o">/</span><span class="n">intel</span><span class="o">/</span><span class="n">bert</span><span class="o">-</span><span class="n">small</span><span class="o">-</span><span class="n">uncased</span><span class="o">-</span><span class="n">whole</span><span class="o">-</span><span class="n">word</span><span class="o">-</span><span class="n">masking</span><span class="o">-</span><span class="n">squad</span><span class="o">-</span><span class="n">int8</span><span class="o">-</span><span class="mi">0002</span><span class="o">/</span><span class="n">FP16</span><span class="o">-</span><span class="n">INT8</span><span class="o">/</span><span class="n">bert</span><span class="o">-</span><span class="n">small</span><span class="o">-</span><span class="n">uncased</span><span class="o">-</span><span class="n">whole</span><span class="o">-</span><span class="n">word</span><span class="o">-</span><span class="n">masking</span><span class="o">-</span><span class="n">squad</span><span class="o">-</span><span class="n">int8</span><span class="o">-</span><span class="mf">0002.</span><span class="n">bin</span>
</pre></div>
</div>
</section>
<section id="load-the-model">
<h3>Load the model<a class="headerlink" href="#load-the-model" title="Permalink to this headline">¶</a></h3>
<p>Downloaded models are located in a fixed structure, which indicates
vendor, model name and precision. Only a few lines of code are required
to run the model. First, we create an Inference Engine object. Then we
read the network architecture and model weights from the .xml and .bin
files. Finally, we compile the network for the desired device. You can
choose <code class="docutils literal notranslate"><span class="pre">CPU</span></code> or <code class="docutils literal notranslate"><span class="pre">GPU</span></code> in the case of this model.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># initialize inference engine</span>
<span class="n">core</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="c1"># read the network and corresponding weights from file</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">model_weights_path</span><span class="p">)</span>
<span class="c1"># load the model on the CPU (you can use GPU as well)</span>
<span class="n">compiled_model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="c1"># get input and output names of nodes</span>
<span class="n">input_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">compiled_model</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">output_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">compiled_model</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>

<span class="c1"># get network input size</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="n">compiled_model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p>Input keys are the names of the input nodes and output keys contain
names of output nodes of the network. In the case of the BERT-large-like
model, we have four inputs and two outputs.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">any_name</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">input_keys</span><span class="p">],</span> <span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">any_name</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">output_keys</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">([</span><span class="s1">&#39;input_ids&#39;</span><span class="p">,</span> <span class="s1">&#39;attention_mask&#39;</span><span class="p">,</span> <span class="s1">&#39;token_type_ids&#39;</span><span class="p">,</span> <span class="s1">&#39;position_ids&#39;</span><span class="p">],</span>
 <span class="p">[</span><span class="s1">&#39;output_s&#39;</span><span class="p">,</span> <span class="s1">&#39;output_e&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
</section>
<section id="processing">
<h2>Processing<a class="headerlink" href="#processing" title="Permalink to this headline">¶</a></h2>
<p>NLP models usually take a list of tokens as standard input. A token is a
single word converted to some integer. To provide the proper input, we
need the vocabulary for such mapping. We also define some special tokens
like separators or padding and a function to load the content from
provided URLs.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># path to vocabulary file</span>
<span class="n">vocab_file_path</span> <span class="o">=</span> <span class="s2">&quot;data/vocab.txt&quot;</span>

<span class="c1"># create dictionary with words and their indices</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">load_vocab_file</span><span class="p">(</span><span class="n">vocab_file_path</span><span class="p">)</span>

<span class="c1"># define special tokens</span>
<span class="n">cls_token</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="s2">&quot;[CLS]&quot;</span><span class="p">]</span>
<span class="n">pad_token</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="s2">&quot;[PAD]&quot;</span><span class="p">]</span>
<span class="n">sep_token</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="s2">&quot;[SEP]&quot;</span><span class="p">]</span>


<span class="c1"># function to load text from given urls</span>
<span class="k">def</span> <span class="nf">load_context</span><span class="p">(</span><span class="n">sources</span><span class="p">):</span>
    <span class="n">input_urls</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">paragraphs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">source</span> <span class="ow">in</span> <span class="n">sources</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">parse</span><span class="o">.</span><span class="n">urlparse</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">([</span><span class="n">result</span><span class="o">.</span><span class="n">scheme</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">netloc</span><span class="p">]):</span>
            <span class="n">input_urls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">paragraphs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>

    <span class="n">paragraphs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">reader</span><span class="o">.</span><span class="n">get_paragraphs</span><span class="p">(</span><span class="n">input_urls</span><span class="p">))</span>
    <span class="c1"># produce one big context string</span>
    <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">paragraphs</span><span class="p">)</span>
</pre></div>
</div>
<section id="preprocessing">
<h3>Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this headline">¶</a></h3>
<p>The input size in this case is 384 tokens long. The main input
(<code class="docutils literal notranslate"><span class="pre">input_ids</span></code>) to used BERT model consist of two parts: question tokens
and context tokens separated by some special tokens. If question +
context are shorter than 384 tokens, padding tokens are added. If
question + context is longer than 384 tokens, the context must be split
into parts and the question with different parts of context must be fed
to the network many times. We use overlapping, so neighbor parts of the
context are overlapped by half size of the context part (if the context
part equals 300 tokens, neighbor context parts overlap with 150 tokens).
We also need to provide: <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>, which is a sequence of
integer values representing the mask of valid values in the input;
<code class="docutils literal notranslate"><span class="pre">token_type_ids</span></code>, which is a sequence of integer values representing
the segmentation of the <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> into question and context;
<code class="docutils literal notranslate"><span class="pre">position_ids</span></code>, which is a sequence of integer values from 0 to 383
representing the position index for each input token. To know more about
input, please read
<a class="reference external" href="https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/intel/bert-small-uncased-whole-word-masking-squad-int8-0002#input">this</a>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generator of a sequence of inputs</span>
<span class="k">def</span> <span class="nf">prepare_input</span><span class="p">(</span><span class="n">question_tokens</span><span class="p">,</span> <span class="n">context_tokens</span><span class="p">):</span>
    <span class="c1"># length of question in tokens</span>
    <span class="n">question_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">question_tokens</span><span class="p">)</span>
    <span class="c1"># context part size</span>
    <span class="n">context_len</span> <span class="o">=</span> <span class="n">input_size</span> <span class="o">-</span> <span class="n">question_len</span> <span class="o">-</span> <span class="mi">3</span>

    <span class="k">if</span> <span class="n">context_len</span> <span class="o">&lt;</span> <span class="mi">16</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Question is too long in comparison to input size. No space for context&quot;</span><span class="p">)</span>

    <span class="c1"># take parts of context with overlapping by 0.5</span>
    <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">context_tokens</span><span class="p">)</span> <span class="o">-</span> <span class="n">context_len</span><span class="p">),</span> <span class="n">context_len</span> <span class="o">//</span> <span class="mi">2</span><span class="p">):</span>
        <span class="c1"># part of context</span>
        <span class="n">part_context_tokens</span> <span class="o">=</span> <span class="n">context_tokens</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">start</span> <span class="o">+</span> <span class="n">context_len</span><span class="p">]</span>
        <span class="c1"># input: question and context separated by special tokens</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">cls_token</span><span class="p">]</span> <span class="o">+</span> <span class="n">question_tokens</span> <span class="o">+</span> <span class="p">[</span><span class="n">sep_token</span><span class="p">]</span> <span class="o">+</span> <span class="n">part_context_tokens</span> <span class="o">+</span> <span class="p">[</span><span class="n">sep_token</span><span class="p">]</span>
        <span class="c1"># 1 for any index if there is no padding token, 0 otherwise</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="c1"># 0 for question tokens, 1 for context part</span>
        <span class="n">token_type_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">question_len</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">part_context_tokens</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># add padding at the end</span>
        <span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="p">),</span> <span class="n">pad_number</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                                                                      <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                                                                      <span class="n">token_type_ids</span><span class="o">=</span><span class="n">token_type_ids</span><span class="p">)</span>

        <span class="c1"># create input to feed the model</span>
        <span class="n">input_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">input_ids</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">attention_mask</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
            <span class="s2">&quot;token_type_ids&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">token_type_ids</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
        <span class="p">}</span>

        <span class="c1"># some models require additional position_ids</span>
        <span class="k">if</span> <span class="s2">&quot;position_ids&quot;</span> <span class="ow">in</span> <span class="p">[</span><span class="n">i_key</span><span class="o">.</span><span class="n">any_name</span> <span class="k">for</span> <span class="n">i_key</span> <span class="ow">in</span> <span class="n">input_keys</span><span class="p">]:</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">))</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">position_ids</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="k">yield</span> <span class="n">input_dict</span><span class="p">,</span> <span class="n">pad_number</span><span class="p">,</span> <span class="n">start</span>


<span class="c1"># function to add padding</span>
<span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="p">):</span>
    <span class="c1"># how many padding tokens</span>
    <span class="n">diff_input_size</span> <span class="o">=</span> <span class="n">input_size</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">diff_input_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># add padding to all inputs</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span> <span class="o">+</span> <span class="p">[</span><span class="n">pad_token</span><span class="p">]</span> <span class="o">*</span> <span class="n">diff_input_size</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">diff_input_size</span>
        <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">token_type_ids</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">diff_input_size</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="p">),</span> <span class="n">diff_input_size</span>
</pre></div>
</div>
</section>
<section id="postprocessing">
<h3>Postprocessing<a class="headerlink" href="#postprocessing" title="Permalink to this headline">¶</a></h3>
<p>The results from the network are raw (logits). We need to use the
softmax function to get the probability distribution. Then, we are
looking for the best answer in the current part of the context (the
highest score) and we return the score and the context range for the
answer.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># based on https://github.com/openvinotoolkit/open_model_zoo/blob/bf03f505a650bafe8da03d2747a8b55c5cb2ef16/demos/common/python/openvino/model_zoo/model_api/models/bert.py#L163</span>
<span class="k">def</span> <span class="nf">postprocess</span><span class="p">(</span><span class="n">output_start</span><span class="p">,</span> <span class="n">output_end</span><span class="p">,</span> <span class="n">question_tokens</span><span class="p">,</span> <span class="n">context_tokens_start_end</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">start_idx</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">get_score</span><span class="p">(</span><span class="n">logits</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span> <span class="o">/</span> <span class="n">out</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># get start-end scores for context</span>
    <span class="n">score_start</span> <span class="o">=</span> <span class="n">get_score</span><span class="p">(</span><span class="n">output_start</span><span class="p">)</span>
    <span class="n">score_end</span> <span class="o">=</span> <span class="n">get_score</span><span class="p">(</span><span class="n">output_end</span><span class="p">)</span>

    <span class="c1"># index of first context token in tensor</span>
    <span class="n">context_start_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">question_tokens</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
    <span class="c1"># index of last+1 context token in tensor</span>
    <span class="n">context_end_idx</span> <span class="o">=</span> <span class="n">input_size</span> <span class="o">-</span> <span class="n">padding</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="c1"># find product of all start-end combinations to find the best one</span>
    <span class="n">max_score</span><span class="p">,</span> <span class="n">max_start</span><span class="p">,</span> <span class="n">max_end</span> <span class="o">=</span> <span class="n">find_best_answer_window</span><span class="p">(</span><span class="n">start_score</span><span class="o">=</span><span class="n">score_start</span><span class="p">,</span>
                                                            <span class="n">end_score</span><span class="o">=</span><span class="n">score_end</span><span class="p">,</span>
                                                            <span class="n">context_start_idx</span><span class="o">=</span><span class="n">context_start_idx</span><span class="p">,</span>
                                                            <span class="n">context_end_idx</span><span class="o">=</span><span class="n">context_end_idx</span><span class="p">)</span>

    <span class="c1"># convert to context text start-end index</span>
    <span class="n">max_start</span> <span class="o">=</span> <span class="n">context_tokens_start_end</span><span class="p">[</span><span class="n">max_start</span> <span class="o">+</span> <span class="n">start_idx</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">max_end</span> <span class="o">=</span> <span class="n">context_tokens_start_end</span><span class="p">[</span><span class="n">max_end</span> <span class="o">+</span> <span class="n">start_idx</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">max_score</span><span class="p">,</span> <span class="n">max_start</span><span class="p">,</span> <span class="n">max_end</span>


<span class="c1"># based on https://github.com/openvinotoolkit/open_model_zoo/blob/bf03f505a650bafe8da03d2747a8b55c5cb2ef16/demos/common/python/openvino/model_zoo/model_api/models/bert.py#L188</span>
<span class="k">def</span> <span class="nf">find_best_answer_window</span><span class="p">(</span><span class="n">start_score</span><span class="p">,</span> <span class="n">end_score</span><span class="p">,</span> <span class="n">context_start_idx</span><span class="p">,</span> <span class="n">context_end_idx</span><span class="p">):</span>
    <span class="n">context_len</span> <span class="o">=</span> <span class="n">context_end_idx</span> <span class="o">-</span> <span class="n">context_start_idx</span>
    <span class="n">score_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
        <span class="n">start_score</span><span class="p">[</span><span class="n">context_start_idx</span><span class="p">:</span><span class="n">context_end_idx</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">context_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
        <span class="n">end_score</span><span class="p">[</span><span class="n">context_start_idx</span><span class="p">:</span><span class="n">context_end_idx</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">context_len</span><span class="p">)),</span>
    <span class="p">)</span>
    <span class="c1"># reset candidates with end before start</span>
    <span class="n">score_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">score_mat</span><span class="p">)</span>
    <span class="c1"># reset long candidates (&gt;16 words)</span>
    <span class="n">score_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">score_mat</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
    <span class="c1"># find the best start-end pair</span>
    <span class="n">max_s</span><span class="p">,</span> <span class="n">max_e</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">score_mat</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">argmax</span><span class="p">(),</span> <span class="n">score_mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">max_score</span> <span class="o">=</span> <span class="n">score_mat</span><span class="p">[</span><span class="n">max_s</span><span class="p">,</span> <span class="n">max_e</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">max_score</span><span class="p">,</span> <span class="n">max_s</span><span class="p">,</span> <span class="n">max_e</span>
</pre></div>
</div>
<p>Firstly, we need to create a list of tokens from the context and the
question. Then, we are looking for the best answer by trying different
parts of the context. The best answer should come with the highest
score.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_best_answer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
    <span class="c1"># convert context string to tokens</span>
    <span class="n">context_tokens</span><span class="p">,</span> <span class="n">context_tokens_start_end</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">text_to_tokens</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span>
                                                                     <span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">)</span>
    <span class="c1"># convert question string to tokens</span>
    <span class="n">question_tokens</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">text_to_tokens</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">question</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># iterate through different parts of context</span>
    <span class="k">for</span> <span class="n">network_input</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">start_idx</span> <span class="ow">in</span> <span class="n">prepare_input</span><span class="p">(</span><span class="n">question_tokens</span><span class="o">=</span><span class="n">question_tokens</span><span class="p">,</span>
                                                           <span class="n">context_tokens</span><span class="o">=</span><span class="n">context_tokens</span><span class="p">):</span>
        <span class="c1"># get output layers</span>
        <span class="n">output_start_key</span> <span class="o">=</span> <span class="n">compiled_model</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="s2">&quot;output_s&quot;</span><span class="p">)</span>
        <span class="n">output_end_key</span> <span class="o">=</span> <span class="n">compiled_model</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="s2">&quot;output_e&quot;</span><span class="p">)</span>

        <span class="c1"># openvino inference</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">compiled_model</span><span class="p">(</span><span class="n">network_input</span><span class="p">)</span>
        <span class="c1"># postprocess the result getting the score and context range for the answer</span>
        <span class="n">score_start_end</span> <span class="o">=</span> <span class="n">postprocess</span><span class="p">(</span><span class="n">output_start</span><span class="o">=</span><span class="n">result</span><span class="p">[</span><span class="n">output_start_key</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                      <span class="n">output_end</span><span class="o">=</span><span class="n">result</span><span class="p">[</span><span class="n">output_end_key</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                      <span class="n">question_tokens</span><span class="o">=</span><span class="n">question_tokens</span><span class="p">,</span>
                                      <span class="n">context_tokens_start_end</span><span class="o">=</span><span class="n">context_tokens_start_end</span><span class="p">,</span>
                                      <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                                      <span class="n">start_idx</span><span class="o">=</span><span class="n">start_idx</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score_start_end</span><span class="p">)</span>

    <span class="c1"># find the highest score</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="c1"># return the part of the context, which is already an answer</span>
    <span class="k">return</span> <span class="n">context</span><span class="p">[</span><span class="n">answer</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span><span class="n">answer</span><span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="n">answer</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="main-processing-function">
<h3>Main Processing Function<a class="headerlink" href="#main-processing-function" title="Permalink to this headline">¶</a></h3>
<p>Run question answering on specific knowledge base (websites) and iterate
through the questions.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">run_question_answering</span><span class="p">(</span><span class="n">sources</span><span class="p">,</span> <span class="n">example_question</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Context: </span><span class="si">{</span><span class="n">sources</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">load_context</span><span class="p">(</span><span class="n">sources</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">context</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error: Empty context or outside paragraphs&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">example_question</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">answer</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">get_best_answer</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">example_question</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Question: </span><span class="si">{</span><span class="n">example_question</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Answer: </span><span class="si">{</span><span class="n">answer</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Score: </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Time: </span><span class="si">{</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">question</span> <span class="o">=</span> <span class="nb">input</span><span class="p">()</span>
            <span class="c1"># if no question - break</span>
            <span class="k">if</span> <span class="n">question</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># measure processing time</span>
            <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">answer</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">get_best_answer</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
            <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>

            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Question: </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Answer: </span><span class="si">{</span><span class="n">answer</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Score: </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Time: </span><span class="si">{</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="run">
<h2>Run<a class="headerlink" href="#run" title="Permalink to this headline">¶</a></h2>
<section id="run-on-local-paragraphs">
<h3>Run on local paragraphs<a class="headerlink" href="#run-on-local-paragraphs" title="Permalink to this headline">¶</a></h3>
<p>Change sources to your own to answer your questions. You can use as many
sources as you want. Usually, you need to wait a few seconds for the
answer, but the longer context the longer the waiting time. The model is
very limited and sensitive for the input. The answer can depend on
whether there is a question mark at the end. The model will try to
answer any of your questions even there is no good answer in the
context, so in that case, you can see random results.</p>
<p>Sample source: Computational complexity theory paragraph (from
<a class="reference external" href="https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/Computational_complexity_theory.html">here</a>)</p>
<p>Sample questions: - What is the term for a task that generally lends
itself to being solved by a computer? - By what main attribute are
computational problems classified utilizing computational complexity
theory? - What branch of theoretical computer science deals with broadly
classifying computational problems by difficulty and class of
relationship?</p>
<p>If you want to stop the processing just put an empty string.</p>
<p><em>Note: Firstly, run the code below and then put your questions in the
box.</em></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sources</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Computational complexity theory is a branch of the theory of computation in theoretical computer &quot;</span>
           <span class="s2">&quot;science that focuses on classifying computational problems according to their inherent difficulty, &quot;</span>
           <span class="s2">&quot;and relating those classes to each other. A computational problem is understood to be a task that &quot;</span>
           <span class="s2">&quot;is in principle amenable to being solved by a computer, which is equivalent to stating that the &quot;</span>
           <span class="s2">&quot;problem may be solved by mechanical application of mathematical steps, such as an algorithm.&quot;</span><span class="p">]</span>

<span class="n">run_question_answering</span><span class="p">(</span><span class="n">sources</span><span class="p">,</span> <span class="n">example_question</span><span class="o">=</span><span class="s2">&quot;What is the term for a task that generally lends itself to being solved by a computer?&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Context: [&#39;Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.&#39;]
Question: What is the term for a task that generally lends itself to being solved by a computer?
Answer: computational problem
Score: 0.48
Time: 0.10s
</pre></div>
</div>
</section>
<section id="run-on-websites">
<h3>Run on websites<a class="headerlink" href="#run-on-websites" title="Permalink to this headline">¶</a></h3>
<p>You can also provide urls. Note that the context (knowledge base) is
built from website paragraphs. If some information is outside the
paragraphs, the algorithm won’t able to find it.</p>
<p>Sample source: <a class="reference external" href="https://en.wikipedia.org/wiki/OpenVINO">OpenVINO
wiki</a></p>
<p>Sample questions: - What does OpenVINO mean? - What is the license for
OpenVINO? - Where can you deploy OpenVINO code?</p>
<p>If you want to stop the processing just put an empty string.</p>
<p><em>Note: Firstly, run the code below and then put your questions in the
box.</em></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sources</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://en.wikipedia.org/wiki/OpenVINO&quot;</span><span class="p">]</span>

<span class="n">run_question_answering</span><span class="p">(</span><span class="n">sources</span><span class="p">,</span> <span class="n">example_question</span><span class="o">=</span><span class="s2">&quot;What does OpenVINO mean?&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Context: [&#39;https://en.wikipedia.org/wiki/OpenVINO&#39;]
Question: What does OpenVINO mean?
Answer: Open Visual Inference and Neural network Optimization
Score: 0.94
Time: 0.17s
</pre></div>
</div>
</section>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="212-onnx-style-transfer-with-output.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="214-vision-paddle-classification-with-output.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>