
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>OpenVINO API Tutorial &#8212; OpenVINO™  documentation</title>
    
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/tabs.css" type="text/css" />
    <script src="../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/tabs.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/custom.js"></script>
    <script src="../_static/js/graphs.js"></script>
    <script src="../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/notebooks/002-openvino-api-with-output.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Hello Image Segmentation" href="003-hello-segmentation-with-output.html" />
    <link rel="prev" title="Hello Image Classification" href="001-hello-world-with-output.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/resources.html">
  Resources
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/latest/notebooks/002-openvino-api-with-output.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/cn/latest/notebooks/002-openvino-api-with-output.html">Chinese</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="001-hello-world-with-output.html">
   Hello Image Classification
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   OpenVINO API Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003-hello-segmentation-with-output.html">
   Hello Image Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="004-hello-detection-with-output.html">
   Hello Object Detection
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="101-tensorflow-to-openvino-with-output.html">
   Convert a TensorFlow Model to OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="102-pytorch-onnx-to-openvino-with-output.html">
   Convert a PyTorch Model to ONNX and OpenVINO IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="103-paddle-onnx-to-openvino-classification-with-output.html">
   Convert a PaddlePaddle Model to ONNX and OpenVINO IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="104-model-tools-with-output.html">
   Working with Open Model Zoo Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="105-language-quantize-bert-with-output.html">
   Quantize NLP models with OpenVINO Post-Training Optimization Tool ​
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="106-auto-device-with-output.html">
   Automatic Device Selection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="111-detection-quantization-with-output.html">
   Object Detection Quantization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="112-pytorch-post-training-quantization-nncf-with-output.html">
   Post-Training Quantization of PyTorch models with NNCF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="113-image-classification-quantization-with-output.html">
   Quantization of Image Classification Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="114-quantization-simplified-mode-with-output.html">
   INT8 Quantization with Post-training Optimization Tool (POT) in Simplified Mode tutorial
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="201-vision-monodepth-with-output.html">
   Monodepth Estimation with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-image-with-output.html">
   Single Image Super Resolution with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-video-with-output.html">
   Video Super Resolution with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="205-vision-background-removal-with-output.html">
   Image Background Removal with U^2-Net and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="206-vision-paddlegan-anime-with-output.html">
   Photos to Anime with PaddleGAN and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="207-vision-paddlegan-superresolution-with-output.html">
   Super Resolution with PaddleGAN and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="208-optical-character-recognition-with-output.html">
   Optical Character Recognition (OCR) with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="209-handwritten-ocr-with-output.html">
   Handwritten Chinese and Japanese OCR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="210-ct-scan-live-inference-with-output.html">
   Live Inference and Benchmark CT-scan Data with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="211-speech-to-text-with-output.html">
   Speech to Text with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="212-onnx-style-transfer-with-output.html">
   Style Transfer on ONNX Models with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="213-question-answering-with-output.html">
   Interactive question answering with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="214-vision-paddle-classification-with-output.html">
   PaddlePaddle Image Classification with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="215-image-inpainting-with-output.html">
   Image In-painting with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="217-vision-deblur-with-output.html">
   Deblur Photos with DeblurGAN-v2 and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="218-vehicle-detection-and-recognition-with-output.html">
   Vehicle Detection And Recognition with OpenVINO
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-with-output.html">
   From Training to Deployment with TensorFlow and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-pot-with-output.html">
   Post-Training Quantization with TensorFlow Classification Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="302-pytorch-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using PyTorch framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="305-tensorflow-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using TensorFlow Framework
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="401-object-detection-with-output.html">
   Live Object Detection with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="402-pose-estimation-with-output.html">
   Live Human Pose Estimation with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="403-action-recognition-webcam-with-output.html">
   Human Action Recognition with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="405-paddle-ocr-webcam-with-output.html">
   PaddleOCR with OpenVINO
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-inference-engine-and-show-info">
   Load Inference Engine and Show Info
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-a-model">
   Loading a Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ir-model">
     IR Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#onnx-model">
     ONNX Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#getting-information-about-a-model">
   Getting Information about a Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-inputs">
     Model Inputs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-outputs">
     Model Outputs
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#doing-inference-on-a-model">
   Doing Inference on a Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reshaping-and-resizing">
   Reshaping and Resizing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#change-image-size">
     Change Image Size
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#change-batch-size">
     Change Batch Size
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#caching-a-model">
   Caching a Model
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <section id="openvino-api-tutorial">
<h1>OpenVINO API Tutorial<a class="headerlink" href="#openvino-api-tutorial" title="Permalink to this headline">¶</a></h1>
<p>This notebook explains the basics of the OpenVINO Inference Engine API.
It covers:</p>
<ul class="simple">
<li><p><a class="reference external" href="#Load-Inference-Engine-and-Show-Info">Load Inference Engine and Show
Info</a></p></li>
<li><p><a class="reference external" href="#Loading-a-Model">Loading a Model</a></p>
<ul>
<li><p><a class="reference external" href="#IR-Model">IR Model</a></p></li>
<li><p><a class="reference external" href="#ONNX-Model">ONNX Model</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#Getting-Information-about-a-Model">Getting Information about a
Model</a></p>
<ul>
<li><p><a class="reference external" href="#Model-Inputs">Model Inputs</a></p></li>
<li><p><a class="reference external" href="#Model-Outputs">Model Outputs</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#Doing-Inference-on-a-Model">Doing Inference on a Model</a></p></li>
<li><p><a class="reference external" href="#Reshaping-and-Resizing">Reshaping and Resizing</a></p>
<ul>
<li><p><a class="reference external" href="#Change-Image-Size">Change Image Size</a></p></li>
<li><p><a class="reference external" href="#Change-Batch-Size">Change Batch Size</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#Caching-a-Model">Caching a Model</a></p></li>
</ul>
<p>The notebook is divided into sections with headers. Each section is
standalone and does not depend on previous sections. A segmentation and
classification IR model and a segmentation ONNX model are provided as
examples. You can replace these model files with your own models. The
exact outputs will be different, but the process is the same.</p>
<section id="load-inference-engine-and-show-info">
<h2>Load Inference Engine and Show Info<a class="headerlink" href="#load-inference-engine-and-show-info" title="Permalink to this headline">¶</a></h2>
<p>Initialize Inference Engine with Core()</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
</pre></div>
</div>
<p>Inference Engine can load a network on a device. A device in this
context means a CPU, an Intel GPU, a Neural Compute Stick 2, etc. The
<code class="docutils literal notranslate"><span class="pre">available_devices</span></code> property shows the devices that are available on
your system. The “FULL_DEVICE_NAME” option to <code class="docutils literal notranslate"><span class="pre">ie.get_property()</span></code>
shows the name of the device.</p>
<p>In this notebook the CPU device is used. To use an integrated GPU, use
<code class="docutils literal notranslate"><span class="pre">device_name=&quot;GPU&quot;</span></code> instead. Note that loading a network on GPU will
be slower than loading a network on CPU, but inference will likely be
faster.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">devices</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">available_devices</span>

<span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">:</span>
    <span class="n">device_name</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">get_property</span><span class="p">(</span><span class="n">device_name</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;FULL_DEVICE_NAME&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">device_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CPU</span><span class="p">:</span> <span class="n">Intel</span><span class="p">(</span><span class="n">R</span><span class="p">)</span> <span class="n">Xeon</span><span class="p">(</span><span class="n">R</span><span class="p">)</span> <span class="n">Platinum</span> <span class="mi">8272</span><span class="n">CL</span> <span class="n">CPU</span> <span class="o">@</span> <span class="mf">2.60</span><span class="n">GHz</span>
</pre></div>
</div>
</section>
<section id="loading-a-model">
<h2>Loading a Model<a class="headerlink" href="#loading-a-model" title="Permalink to this headline">¶</a></h2>
<p>After initializing Inference Engine, first read the model file with
<code class="docutils literal notranslate"><span class="pre">read_model()</span></code>, then compile it to the specified device with
<code class="docutils literal notranslate"><span class="pre">compile_model()</span></code>.</p>
<section id="ir-model">
<h3>IR Model<a class="headerlink" href="#ir-model" title="Permalink to this headline">¶</a></h3>
<p>An IR (Intermediate Representation) model consists of an .xml file,
containing information about network topology, and a .bin file,
containing the weights and biases binary data. <code class="docutils literal notranslate"><span class="pre">read_model()</span></code> expects
the weights file to be located in the same directory as the xml file,
with the same filename, and the extension .bin:
<code class="docutils literal notranslate"><span class="pre">model_weights_file</span> <span class="pre">==</span> <span class="pre">Path(model_xml).with_suffix(&quot;.bin&quot;)</span></code>. If this
is the case, specifying the weights file is optional. If the weights
file has a different filename, it can be specified with the <code class="docutils literal notranslate"><span class="pre">weights</span></code>
parameter to <code class="docutils literal notranslate"><span class="pre">read_model()</span></code>.</p>
<p>See the
<a class="reference external" href="101-tensorflow-to-openvino-with-output.html">tensorflow-to-openvino</a>
and
<a class="reference external" href="102-pytorch-onnx-to-openvino-with-output.html">pytorch-onnx-to-openvino</a>
notebooks for information on how to convert your existing TensorFlow,
PyTorch or ONNX model to OpenVINO’s IR format with OpenVINO’s Model
Optimizer. For exporting ONNX models to IR with default settings, the
<code class="docutils literal notranslate"><span class="pre">.serialize()</span></code> method can also be used.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">classification_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/classification.xml&quot;</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">classification_model_xml</span><span class="p">)</span>
<span class="n">compiled_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="onnx-model">
<h3>ONNX Model<a class="headerlink" href="#onnx-model" title="Permalink to this headline">¶</a></h3>
<p>An ONNX model is a single file. Reading and loading an ONNX model works
the same way as reading and loading an IR model. The <code class="docutils literal notranslate"><span class="pre">model</span></code> argument
points to the ONNX filename.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">onnx_model_path</span> <span class="o">=</span> <span class="s2">&quot;model/segmentation.onnx&quot;</span>
<span class="n">model_onnx</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">onnx_model_path</span><span class="p">)</span>
<span class="n">compiled_model_onnx</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_onnx</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The ONNX model can be exported to IR with .serialize():</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.offline_transformations</span> <span class="kn">import</span> <span class="n">serialize</span>

<span class="n">serialize</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_onnx</span><span class="p">,</span> <span class="n">model_path</span><span class="o">=</span><span class="s2">&quot;model/exported_onnx_model.xml&quot;</span><span class="p">,</span> <span class="n">weights_path</span><span class="o">=</span><span class="s2">&quot;model/exported_onnx_model.bin&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="getting-information-about-a-model">
<h2>Getting Information about a Model<a class="headerlink" href="#getting-information-about-a-model" title="Permalink to this headline">¶</a></h2>
<p>The OpenVINO IENetwork instance stores information about the model.
Information about the inputs and outputs of the model are in
<code class="docutils literal notranslate"><span class="pre">model.inputs</span></code> and <code class="docutils literal notranslate"><span class="pre">model.outputs</span></code>. These are also properties of the
ExecutableNetwork instance. Where we use <code class="docutils literal notranslate"><span class="pre">model.inputs</span></code> and
<code class="docutils literal notranslate"><span class="pre">model.outputs</span></code> in the cells below, you can also use
<code class="docutils literal notranslate"><span class="pre">compiled_model.inputs</span></code> and <code class="docutils literal notranslate"><span class="pre">compiled_model.outputs</span></code>.</p>
<section id="model-inputs">
<h3>Model Inputs<a class="headerlink" href="#model-inputs" title="Permalink to this headline">¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">classification_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/classification.xml&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">classification_model_xml</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any_name</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;input&#39;</span>
</pre></div>
</div>
<p>The cell above shows that the model loaded expects one input, with the
name <em>input</em>. If you loaded a different model, you may see a different
input layer name, and you may see more inputs.</p>
<p>It is often useful to have a reference to the name of the first input
layer. For a model with one input, <code class="docutils literal notranslate"><span class="pre">model.input(0)</span></code> gets this name.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Information for this input layer is stored in <code class="docutils literal notranslate"><span class="pre">inputs</span></code>. The next cell
prints the input layout, precision and shape.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input precision: </span><span class="si">{</span><span class="n">input_layer</span><span class="o">.</span><span class="n">element_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input shape: </span><span class="si">{</span><span class="n">input_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="n">precision</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">Type</span><span class="p">:</span> <span class="s1">&#39;float32&#39;</span><span class="o">&gt;</span>
<span class="nb">input</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">}</span>
</pre></div>
</div>
<p>This cell output tells us that the model expects inputs with a shape of
[1,3,224,224], and that this is in NCHW layout. This means that the
model expects input data with a batch size (N) of 1, 3 channels (C), and
images of a height (H) and width (W) of 224. The input data is expected
to be of FP32 (floating point) precision.</p>
</section>
<section id="model-outputs">
<h3>Model Outputs<a class="headerlink" href="#model-outputs" title="Permalink to this headline">¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">classification_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/classification.xml&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">classification_model_xml</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any_name</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;MobilenetV3/Predictions/Softmax&#39;</span>
</pre></div>
</div>
<p>Model output info is stored in <code class="docutils literal notranslate"><span class="pre">model.outputs</span></code>. The cell above shows
that the model returns one output, with the name
<em>MobilenetV3/Predictions/Softmax</em>. If you loaded a different model, you
will probably see a different output layer name, and you may see more
outputs.</p>
<p>Since this model has one output, follow the same method as for the input
layer to get its name.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">output_layer</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">Output</span><span class="p">:</span> <span class="n">names</span><span class="p">[</span><span class="n">MobilenetV3</span><span class="o">/</span><span class="n">Predictions</span><span class="o">/</span><span class="n">Softmax</span><span class="p">]</span> <span class="n">shape</span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="mi">1001</span><span class="p">}</span> <span class="nb">type</span><span class="p">:</span> <span class="n">f32</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Getting the output precision and shape is similar to getting the input
precision and shape.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output precision: </span><span class="si">{</span><span class="n">output_layer</span><span class="o">.</span><span class="n">element_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output shape: </span><span class="si">{</span><span class="n">output_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="n">precision</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">Type</span><span class="p">:</span> <span class="s1">&#39;float32&#39;</span><span class="o">&gt;</span>
<span class="n">output</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1001</span><span class="p">}</span>
</pre></div>
</div>
<p>This cell output shows that the model returns outputs with a shape of
[1, 1001], where 1 is the batch size (N) and 1001 the number of classes
(C). The output is returned as 32-bit floating point.</p>
</section>
</section>
<section id="doing-inference-on-a-model">
<h2>Doing Inference on a Model<a class="headerlink" href="#doing-inference-on-a-model" title="Permalink to this headline">¶</a></h2>
<p>To do inference on a model, first you need to create inference request
by calling <code class="docutils literal notranslate"><span class="pre">create_infer_request()</span></code> being method of
<em>ExecutableNetwork</em>, the <code class="docutils literal notranslate"><span class="pre">exec_net</span></code> that we loaded with
<code class="docutils literal notranslate"><span class="pre">compile_model()</span></code>. Than you have to call <code class="docutils literal notranslate"><span class="pre">infer()</span></code>, being the method
of <code class="docutils literal notranslate"><span class="pre">_InferRequest_</span></code>, expects one argument: <em>inputs</em>. This is a
dictionary, mapping input layer names to input data.</p>
<p><strong>Preparation: load network</strong></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">classification_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/classification.xml&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">classification_model_xml</span><span class="p">)</span>
<span class="n">compiled_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">input_layer</span> <span class="o">=</span> <span class="n">compiled_model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">output_layer</span> <span class="o">=</span> <span class="n">compiled_model</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Preparation: load image and convert to input shape</strong></p>
<p>To propagate an image through the network, it needs to be loaded into an
array, resized to the shape that the network expects, and converted to
the network’s input layout.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">image_filename</span> <span class="o">=</span> <span class="s2">&quot;data/coco_hollywood.jpg&quot;</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">image_filename</span><span class="p">)</span>
<span class="n">image</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">663</span><span class="p">,</span> <span class="mi">994</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>The image has a shape of (663,994,3). It is 663 pixels in height, 994
pixels in width, and has 3 color channels. We get a reference to the
height and width that the network expects and resize the image to that
size.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># N,C,H,W = batch size, number of channels, height, width</span>
<span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">input_layer</span><span class="o">.</span><span class="n">shape</span>
<span class="c1"># OpenCV resize expects the destination size as (width, height)</span>
<span class="n">resized_image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">src</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">dsize</span><span class="o">=</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">))</span>
<span class="n">resized_image</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>Now the image has the width and height that the network expects. It is
still in H,W,C format. We change it to N,C,H,W format (where N=1) by
first calling <code class="docutils literal notranslate"><span class="pre">np.transpose()</span></code> to change to C,H,W and then adding the
N dimension by calling <code class="docutils literal notranslate"><span class="pre">np.expand_dims()</span></code>. Convert the data to FP32
with <code class="docutils literal notranslate"><span class="pre">np.astype()</span></code>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">resized_image</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">input_data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Do inference</strong></p>
<p>Now that the input data is in the right shape, do the inference.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">compiled_model</span><span class="p">([</span><span class="n">input_data</span><span class="p">])[</span><span class="n">output_layer</span><span class="p">]</span>
</pre></div>
</div>
<p>We can also create <code class="docutils literal notranslate"><span class="pre">InferRequest</span></code> and run <code class="docutils literal notranslate"><span class="pre">infer</span></code> method on request.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">request</span> <span class="o">=</span> <span class="n">compiled_model</span><span class="o">.</span><span class="n">create_infer_request</span><span class="p">()</span>
<span class="n">request</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">{</span><span class="n">input_layer</span><span class="o">.</span><span class="n">any_name</span><span class="p">:</span> <span class="n">input_data</span><span class="p">})</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">get_output_tensor</span><span class="p">(</span><span class="n">output_layer</span><span class="o">.</span><span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">data</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">.infer()</span></code> sets output tensor, that we can reach using
<code class="docutils literal notranslate"><span class="pre">get_output_tensor()</span></code>. Since we know this network returns one output,
and we stored the reference to the output layer in the
<code class="docutils literal notranslate"><span class="pre">output_layer.index</span></code> parameter, we can get the data with
<code class="docutils literal notranslate"><span class="pre">request.get_output_tensor(output_layer.index)</span></code>. To get numpy array
from output we need to take parameter <code class="docutils literal notranslate"><span class="pre">.data</span></code>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1001</span><span class="p">)</span>
</pre></div>
</div>
<p>The output shape is (1,1001), which we saw is the expected shape of the
output. This output shape indicates that the network returns
probabilities for 1001 classes. To transform this into meaningful
information, check out the <a class="reference external" href="001-hello-world-with-output.html">hello world
notebook</a>.</p>
</section>
<section id="reshaping-and-resizing">
<h2>Reshaping and Resizing<a class="headerlink" href="#reshaping-and-resizing" title="Permalink to this headline">¶</a></h2>
<section id="change-image-size">
<h3>Change Image Size<a class="headerlink" href="#change-image-size" title="Permalink to this headline">¶</a></h3>
<p>Instead of reshaping the image to fit the model, you can also reshape
the model to fit the image. Note that not all models support reshaping,
and models that do may not support all input shapes. The model accuracy
may also suffer if you reshape the model input shape.</p>
<p>We first check the input shape of the model, and then reshape to the new
input shape.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span><span class="p">,</span> <span class="n">PartialShape</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">segmentation_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/segmentation.xml&quot;</span>
<span class="n">segmentation_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">segmentation_model_xml</span><span class="p">)</span>
<span class="n">segmentation_input_layer</span> <span class="o">=</span> <span class="n">segmentation_model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">segmentation_output_layer</span> <span class="o">=</span> <span class="n">segmentation_model</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;~~~~ ORIGINAL MODEL ~~~~&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input shape: </span><span class="si">{</span><span class="n">segmentation_input_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output shape: </span><span class="si">{</span><span class="n">segmentation_output_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">new_shape</span> <span class="o">=</span> <span class="n">PartialShape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">])</span>
<span class="n">segmentation_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">({</span><span class="n">segmentation_input_layer</span><span class="o">.</span><span class="n">any_name</span><span class="p">:</span> <span class="n">new_shape</span><span class="p">})</span>
<span class="n">segmentation_compiled_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">segmentation_model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="c1"># help(segmentation_compiled_model)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;~~~~ RESHAPED MODEL ~~~~&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;model input shape: </span><span class="si">{</span><span class="n">segmentation_input_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;compiled_model input shape: &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">segmentation_compiled_model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;compiled_model output shape: </span><span class="si">{</span><span class="n">segmentation_output_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">~~~~</span> <span class="n">ORIGINAL</span> <span class="n">MODEL</span> <span class="o">~~~~</span>
<span class="nb">input</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">}</span>
<span class="n">output</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">}</span>
<span class="o">~~~~</span> <span class="n">RESHAPED</span> <span class="n">MODEL</span> <span class="o">~~~~</span>
<span class="n">model</span> <span class="nb">input</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">}</span>
<span class="n">compiled_model</span> <span class="nb">input</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">}</span>
<span class="n">compiled_model</span> <span class="n">output</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">}</span>
</pre></div>
</div>
<p>The input shape for the segmentation network is [1,3,512,512], with an
NCHW layout: the network expects 3-channel images with a width and
height of 512 and a batch size of 1. We reshape the network to make it
accept input images with a width and height of 544 with the
<code class="docutils literal notranslate"><span class="pre">.reshape()</span></code> method of <code class="docutils literal notranslate"><span class="pre">IENetwork</span></code>. This segmentation network always
returns arrays with the same width and height as the input width and
height, so setting the input dimensions to 544x544 also modifies the
output dimensions. After reshaping, compile the network once again.</p>
</section>
<section id="change-batch-size">
<h3>Change Batch Size<a class="headerlink" href="#change-batch-size" title="Permalink to this headline">¶</a></h3>
<p>We can also use <code class="docutils literal notranslate"><span class="pre">.reshape()</span></code> to set the batch size, by increasing the
first element of <em>new_shape</em>. For example, to set a batch size of two,
set <code class="docutils literal notranslate"><span class="pre">new_shape</span> <span class="pre">=</span> <span class="pre">(2,3,544,544)</span></code> in the cell above.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span><span class="p">,</span> <span class="n">PartialShape</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">segmentation_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/segmentation.xml&quot;</span>
<span class="n">segmentation_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">segmentation_model_xml</span><span class="p">)</span>
<span class="n">segmentation_input_layer</span> <span class="o">=</span> <span class="n">segmentation_model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">segmentation_output_layer</span> <span class="o">=</span> <span class="n">segmentation_model</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">new_shape</span> <span class="o">=</span> <span class="n">PartialShape</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">])</span>
<span class="n">segmentation_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">({</span><span class="n">segmentation_input_layer</span><span class="o">.</span><span class="n">any_name</span><span class="p">:</span> <span class="n">new_shape</span><span class="p">})</span>
<span class="n">segmentation_compiled_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">segmentation_model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input shape: </span><span class="si">{</span><span class="n">segmentation_input_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output shape: </span><span class="si">{</span><span class="n">segmentation_output_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">}</span>
<span class="n">output</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">}</span>
</pre></div>
</div>
<p>The output shows that by setting the batch size to 2, the first element
(N) of the input and output shape now has a value of 2. Let’s see what
happens if we propagate our input image through the network:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span><span class="p">,</span> <span class="n">PartialShape</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">segmentation_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/segmentation.xml&quot;</span>
<span class="n">segmentation_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">segmentation_model_xml</span><span class="p">)</span>
<span class="n">segmentation_input_layer</span> <span class="o">=</span> <span class="n">segmentation_model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">segmentation_output_layer</span> <span class="o">=</span> <span class="n">segmentation_model</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">new_shape</span> <span class="o">=</span> <span class="n">PartialShape</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">])</span>
<span class="n">segmentation_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">({</span><span class="n">segmentation_input_layer</span><span class="o">.</span><span class="n">any_name</span><span class="p">:</span> <span class="n">new_shape</span><span class="p">})</span>
<span class="n">segmentation_compiled_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">segmentation_model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">segmentation_compiled_model</span><span class="p">([</span><span class="n">input_data</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input data shape: </span><span class="si">{</span><span class="n">input_data</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;result data data shape: </span><span class="si">{</span><span class="n">segmentation_output_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="n">data</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">)</span>
<span class="n">result</span> <span class="n">data</span> <span class="n">data</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="caching-a-model">
<h2>Caching a Model<a class="headerlink" href="#caching-a-model" title="Permalink to this headline">¶</a></h2>
<p>For some devices, like GPU, loading a model can take some time. Model
Caching solves this issue by caching the model in a cache directory. If
<code class="docutils literal notranslate"><span class="pre">ie.compile_model(model=net,</span> <span class="pre">device_name=device_name,</span> <span class="pre">config=config_dict)</span></code>
is set, caching will be used. This option checks if a model exists in
the cache. If so, it loads it from the cache. If not, it loads the model
regularly, and stores it in the cache, so that the next time the model
is loaded when this option is set, the model will be loaded from the
cache.</p>
<p>In the cell below, we create a <em>model_cache</em> directory as a subdirectory
of <em>model</em>, where the model will be cached for the specified device. The
model will be loaded to the GPU. After running this cell once, the model
will be cached, so subsequent runs of this cell will load the model from
the cache.</p>
<p><em>Note: Model Caching is not available on CPU devices</em></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span><span class="p">,</span> <span class="n">PartialShape</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>

<span class="n">device_name</span> <span class="o">=</span> <span class="s2">&quot;GPU&quot;</span>  <span class="c1"># Model Caching is not available for CPU</span>

<span class="k">if</span> <span class="n">device_name</span> <span class="ow">in</span> <span class="n">ie</span><span class="o">.</span><span class="n">available_devices</span> <span class="ow">and</span> <span class="n">device_name</span> <span class="o">!=</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span>
    <span class="n">cache_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;model/model_cache&quot;</span><span class="p">)</span>
    <span class="n">cache_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Enable caching for Inference Engine. To disable caching set enable_caching = False</span>
    <span class="n">enable_caching</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">config_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;CACHE_DIR&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">cache_path</span><span class="p">)}</span> <span class="k">if</span> <span class="n">enable_caching</span> <span class="k">else</span> <span class="p">{}</span>

    <span class="n">classification_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/classification.xml&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">classification_model_xml</span><span class="p">)</span>

    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="n">device_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_dict</span><span class="p">)</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading the network to the </span><span class="si">{</span><span class="n">device_name</span><span class="si">}</span><span class="s2"> device took </span><span class="si">{</span><span class="n">end_time</span><span class="o">-</span><span class="n">start_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model caching is not available on CPU devices.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Model</span> <span class="n">caching</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">available</span> <span class="n">on</span> <span class="n">CPU</span> <span class="n">devices</span><span class="o">.</span>
</pre></div>
</div>
<p>After running the previous cell, we know the model exists in the cache
directory. We delete the compiled model and load it again. We measure
the time it takes now.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">device_name</span> <span class="ow">in</span> <span class="n">ie</span><span class="o">.</span><span class="n">available_devices</span> <span class="ow">and</span> <span class="n">device_name</span> <span class="o">!=</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span>
    <span class="k">del</span> <span class="n">compiled_model</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="n">device_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_dict</span><span class="p">)</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading the network to the </span><span class="si">{</span><span class="n">device_name</span><span class="si">}</span><span class="s2"> device took </span><span class="si">{</span><span class="n">end_time</span><span class="o">-</span><span class="n">start_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="001-hello-world-with-output.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="003-hello-segmentation-with-output.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>