
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Open Model Zoo Demos &#8212; OpenVINO™  documentation</title>
    
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/tabs.css" type="text/css" />
    <script src="../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/tabs.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/custom.js"></script>
    <script src="../_static/js/graphs.js"></script>
    <script src="../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/demos/README.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3D Segmentation Python* Demo" href="3d_segmentation_demo/python/README.html" />
    <link rel="prev" title="yolox-tiny" href="../models/public/yolox-tiny/README.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/resources.html">
  Resources
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/latest/demos/README.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/cn/latest/demos/README.html">Chinese</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Pre-Trained Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../models/intel/index.html">
   Overview of OpenVINO™ Toolkit Intel’s Pre-Trained Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/action-recognition-0001/README.html">
     action-recognition-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/age-gender-recognition-retail-0013/README.html">
     age-gender-recognition-retail-0013
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/asl-recognition-0004/README.html">
     asl-recognition-0004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/bert-large-uncased-whole-word-masking-squad-0001/README.html">
     bert-large-uncased-whole-word-masking-squad-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/bert-large-uncased-whole-word-masking-squad-emb-0001/README.html">
     bert-large-uncased-whole-word-masking-squad-emb-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/bert-large-uncased-whole-word-masking-squad-int8-0001/README.html">
     bert-large-uncased-whole-word-masking-squad-int8-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/bert-small-uncased-whole-word-masking-squad-0001/README.html">
     bert-small-uncased-whole-word-masking-squad-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/bert-small-uncased-whole-word-masking-squad-0002/README.html">
     bert-small-uncased-whole-word-masking-squad-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/bert-small-uncased-whole-word-masking-squad-emb-int8-0001/README.html">
     bert-small-uncased-whole-word-masking-squad-emb-int8-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/bert-small-uncased-whole-word-masking-squad-int8-0002/README.html">
     bert-small-uncased-whole-word-masking-squad-int8-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/common-sign-language-0002/README.html">
     common-sign-language-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/device_support.html">
     Intel’s Pre-Trained Models Device Support
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/driver-action-recognition-adas-0002/README.html">
     driver-action-recognition-adas-0002 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/emotions-recognition-retail-0003/README.html">
     emotions-recognition-retail-0003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/face-detection-0200/README.html">
     face-detection-0200
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/face-detection-0202/README.html">
     face-detection-0202
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/face-detection-0204/README.html">
     face-detection-0204
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/face-detection-0205/README.html">
     face-detection-0205
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/face-detection-0206/README.html">
     face-detection-0206
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/face-detection-adas-0001/README.html">
     face-detection-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/face-detection-retail-0004/README.html">
     face-detection-retail-0004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/face-detection-retail-0005/README.html">
     face-detection-retail-0005
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/face-reidentification-retail-0095/README.html">
     face-reidentification-retail-0095
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/facial-landmarks-35-adas-0002/README.html">
     facial-landmarks-35-adas-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/facial-landmarks-98-detection-0001/README.html">
     facial-landmarks-98-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/faster-rcnn-resnet101-coco-sparse-60-0001/README.html">
     faster-rcnn-resnet101-coco-sparse-60-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/formula-recognition-medium-scan-0001/README.html">
     formula-recognition-medium-scan-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/formula-recognition-polynomials-handwritten-0001/README.html">
     formula-recognition-polynomials-handwritten-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/gaze-estimation-adas-0002/README.html">
     gaze-estimation-adas-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/handwritten-english-recognition-0001/README.html">
     handwritten-english-recognition-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/handwritten-japanese-recognition-0001/README.html">
     handwritten-japanese-recognition-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/handwritten-score-recognition-0003/README.html">
     handwritten-score-recognition-0003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/handwritten-simplified-chinese-recognition-0001/README.html">
     handwritten-simplified-chinese-recognition-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/head-pose-estimation-adas-0001/README.html">
     head-pose-estimation-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/horizontal-text-detection-0001/README.html">
     horizontal-text-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/human-pose-estimation-0001/README.html">
     human-pose-estimation-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/human-pose-estimation-0005/README.html">
     human-pose-estimation-0005
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/human-pose-estimation-0006/README.html">
     human-pose-estimation-0006
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/human-pose-estimation-0007/README.html">
     human-pose-estimation-0007
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/icnet-camvid-ava-0001/README.html">
     icnet-camvid-ava-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/icnet-camvid-ava-sparse-30-0001/README.html">
     icnet-camvid-ava-sparse-30-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/icnet-camvid-ava-sparse-60-0001/README.html">
     icnet-camvid-ava-sparse-60-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/image-retrieval-0001/README.html">
     image-retrieval-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/instance-segmentation-person-0007/README.html">
     instance-segmentation-person-0007
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/instance-segmentation-security-0002/README.html">
     instance-segmentation-security-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/instance-segmentation-security-0091/README.html">
     instance-segmentation-security-0091
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/instance-segmentation-security-0228/README.html">
     instance-segmentation-security-0228
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/instance-segmentation-security-1039/README.html">
     instance-segmentation-security-1039
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/instance-segmentation-security-1040/README.html">
     instance-segmentation-security-1040
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/landmarks-regression-retail-0009/README.html">
     landmarks-regression-retail-0009
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/license-plate-recognition-barrier-0001/README.html">
     license-plate-recognition-barrier-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/machine-translation-nar-de-en-0002/README.html">
     machine-translation-nar-de-en-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/machine-translation-nar-en-de-0002/README.html">
     machine-translation-nar-en-de-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/machine-translation-nar-en-ru-0002/README.html">
     machine-translation-nar-en-ru-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/machine-translation-nar-ru-en-0002/README.html">
     machine-translation-nar-ru-en-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/noise-suppression-denseunet-ll-0001/README.html">
     noise-suppression-denseunet-ll-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/noise-suppression-poconetlike-0001/README.html">
     noise-suppression-poconetlike-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/pedestrian-and-vehicle-detector-adas-0001/README.html">
     pedestrian-and-vehicle-detector-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/pedestrian-detection-adas-0002/README.html">
     pedestrian-detection-adas-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-attributes-recognition-crossroad-0230/README.html">
     person-attributes-recognition-crossroad-0230
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-attributes-recognition-crossroad-0234/README.html">
     person-attributes-recognition-crossroad-0234
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-attributes-recognition-crossroad-0238/README.html">
     person-attributes-recognition-crossroad-0238
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-0106/README.html">
     person-detection-0106
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-0200/README.html">
     person-detection-0200
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-0201/README.html">
     person-detection-0201
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-0202/README.html">
     person-detection-0202
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-0203/README.html">
     person-detection-0203
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-0301/README.html">
     person-detection-0301
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-0302/README.html">
     person-detection-0302
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-0303/README.html">
     person-detection-0303
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-action-recognition-0005/README.html">
     person-detection-action-recognition-0005
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-action-recognition-0006/README.html">
     person-detection-action-recognition-0006
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-action-recognition-teacher-0002/README.html">
     person-detection-action-recognition-teacher-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-asl-0001/README.html">
     person-detection-asl-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-raisinghand-recognition-0001/README.html">
     person-detection-raisinghand-recognition-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-retail-0002/README.html">
     person-detection-retail-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-detection-retail-0013/README.html">
     person-detection-retail-0013
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-reidentification-retail-0277/README.html">
     person-reidentification-retail-0277
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-reidentification-retail-0286/README.html">
     person-reidentification-retail-0286
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-reidentification-retail-0287/README.html">
     person-reidentification-retail-0287
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-reidentification-retail-0288/README.html">
     person-reidentification-retail-0288
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-vehicle-bike-detection-2000/README.html">
     person-vehicle-bike-detection-2000
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-vehicle-bike-detection-2001/README.html">
     person-vehicle-bike-detection-2001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-vehicle-bike-detection-2002/README.html">
     person-vehicle-bike-detection-2002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-vehicle-bike-detection-2003/README.html">
     person-vehicle-bike-detection-2003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-vehicle-bike-detection-2004/README.html">
     person-vehicle-bike-detection-2004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-vehicle-bike-detection-crossroad-0078/README.html">
     person-vehicle-bike-detection-crossroad-0078
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-vehicle-bike-detection-crossroad-1016/README.html">
     person-vehicle-bike-detection-crossroad-1016
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/person-vehicle-bike-detection-crossroad-yolov3-1020/README.html">
     person-vehicle-bike-detection-crossroad-yolov3-1020
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/product-detection-0001/README.html">
     product-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/resnet18-xnor-binary-onnx-0001/README.html">
     resnet18-xnor-binary-onnx-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/resnet50-binary-0001/README.html">
     resnet50-binary-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/road-segmentation-adas-0001/README.html">
     road-segmentation-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/semantic-segmentation-adas-0001/README.html">
     semantic-segmentation-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/single-image-super-resolution-1032/README.html">
     single-image-super-resolution-1032
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/single-image-super-resolution-1033/README.html">
     single-image-super-resolution-1033
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/smartlab-object-detection-0001/README.html">
     smartlab-object-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/smartlab-object-detection-0002/README.html">
     smartlab-object-detection-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/smartlab-object-detection-0003/README.html">
     smartlab-object-detection-0003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/smartlab-object-detection-0004/README.html">
     smartlab-object-detection-0004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/smartlab-sequence-modelling-0001/README.html">
     smartlab-sequence-modelling-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/text-detection-0003/README.html">
     text-detection-0003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/text-detection-0004/README.html">
     text-detection-0004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/text-image-super-resolution-0001/README.html">
     text-image-super-resolution-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/text-recognition-0012/README.html">
     text-recognition-0012
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/text-recognition-0014/README.html">
     text-recognition-0014
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/text-recognition-0015/README.html">
     text-recognition-0015 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/text-recognition-0016/README.html">
     text-recognition-0016 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/text-spotting-0005/README.html">
     text-spotting-0005 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/text-to-speech-en-0001/README.html">
     text-to-speech-en-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/text-to-speech-en-multi-0001/README.html">
     text-to-speech-en-multi-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/time-series-forecasting-electricity-0001/README.html">
     time-series-forecasting-electricity-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/unet-camvid-onnx-0001/README.html">
     unet-camvid-onnx-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/vehicle-attributes-recognition-barrier-0039/README.html">
     vehicle-attributes-recognition-barrier-0039
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/vehicle-attributes-recognition-barrier-0042/README.html">
     vehicle-attributes-recognition-barrier-0042
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/vehicle-detection-0200/README.html">
     vehicle-detection-0200
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/vehicle-detection-0201/README.html">
     vehicle-detection-0201
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/vehicle-detection-0202/README.html">
     vehicle-detection-0202
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/vehicle-detection-adas-0002/README.html">
     vehicle-detection-adas-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/vehicle-license-plate-detection-barrier-0106/README.html">
     vehicle-license-plate-detection-barrier-0106
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/weld-porosity-detection-0001/README.html">
     weld-porosity-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/yolo-v2-ava-0001/README.html">
     yolo-v2-ava-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/yolo-v2-ava-sparse-35-0001/README.html">
     yolo-v2-ava-sparse-35-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/yolo-v2-ava-sparse-70-0001/README.html">
     yolo-v2-ava-sparse-70-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/yolo-v2-tiny-ava-0001/README.html">
     yolo-v2-tiny-ava-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/yolo-v2-tiny-ava-sparse-30-0001/README.html">
     yolo-v2-tiny-ava-sparse-30-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/yolo-v2-tiny-ava-sparse-60-0001/README.html">
     yolo-v2-tiny-ava-sparse-60-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/intel/yolo-v2-tiny-vehicle-detection-0001/README.html">
     yolo-v2-tiny-vehicle-detection-0001
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../models/public/index.html">
   Overview of OpenVINO™ Toolkit Public Pre-Trained Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/Sphereface/README.html">
     Sphereface
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/aclnet/README.html">
     aclnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/aclnet-int8/README.html">
     aclnet-int8
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/alexnet/README.html">
     alexnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/anti-spoof-mn3/README.html">
     anti-spoof-mn3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/background-matting-mobilenetv2/README.html">
     background-matting-mobilenetv2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/bert-base-ner/README.html">
     bert-base-ner
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/brain-tumor-segmentation-0001/README.html">
     brain-tumor-segmentation-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/brain-tumor-segmentation-0002/README.html">
     brain-tumor-segmentation-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/caffenet/README.html">
     caffenet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/cocosnet/README.html">
     cocosnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/colorization-siggraph/README.html">
     colorization-siggraph
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/colorization-v2/README.html">
     colorization-v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/common-sign-language-0001/README.html">
     common-sign-language-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/convnext-tiny/README.html">
     convnext-tiny
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/ctdet_coco_dlav0_512/README.html">
     ctdet_coco_dlav0_512
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/ctpn/README.html">
     ctpn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/deblurgan-v2/README.html">
     deblurgan-v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/deeplabv3/README.html">
     deeplabv3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/densenet-121/README.html">
     densenet-121
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/densenet-121-tf/README.html">
     densenet-121-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/detr-resnet50/README.html">
     detr-resnet50
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/device_support.html">
     Public Pre-Trained Models Device Support
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/dla-34/README.html">
     dla-34
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/drn-d-38/README.html">
     drn-d-38
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/efficientdet-d0-tf/README.html">
     efficientdet-d0-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/efficientdet-d1-tf/README.html">
     efficientdet-d1-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/efficientnet-b0/README.html">
     efficientnet-b0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/efficientnet-b0-pytorch/README.html">
     efficientnet-b0-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/efficientnet-v2-b0/README.html">
     efficientnet-v2-b0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/efficientnet-v2-s/README.html">
     efficientnet-v2-s
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/f3net/README.html">
     f3net
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/face-detection-retail-0044/README.html">
     face-detection-retail-0044
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/face-recognition-resnet100-arcface-onnx/README.html">
     face-recognition-resnet100-arcface-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/faceboxes-pytorch/README.html">
     faceboxes-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/facenet-20180408-102900/README.html">
     facenet-20180408-102900
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/fast-neural-style-mosaic-onnx/README.html">
     fast-neural-style-mosaic-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/faster_rcnn_inception_resnet_v2_atrous_coco/README.html">
     faster_rcnn_inception_resnet_v2_atrous_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/faster_rcnn_resnet50_coco/README.html">
     faster_rcnn_resnet50_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/fastseg-large/README.html">
     fastseg-large
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/fastseg-small/README.html">
     fastseg-small
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/fbcnn/README.html">
     fbcnn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/fcrn-dp-nyu-depth-v2-tf/README.html">
     fcrn-dp-nyu-depth-v2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/forward-tacotron/README.html">
     forward-tacotron (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/gmcnn-places2-tf/README.html">
     gmcnn-places2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/googlenet-v1/README.html">
     googlenet-v1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/googlenet-v1-tf/README.html">
     googlenet-v1-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/googlenet-v2/README.html">
     googlenet-v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/googlenet-v2-tf/README.html">
     googlenet-v2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/googlenet-v3/README.html">
     googlenet-v3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/googlenet-v3-pytorch/README.html">
     googlenet-v3-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/googlenet-v4-tf/README.html">
     googlenet-v4-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/gpt-2/README.html">
     gpt-2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/hbonet-0.25/README.html">
     hbonet-0.25
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/hbonet-1.0/README.html">
     hbonet-1.0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/higher-hrnet-w32-human-pose-estimation/README.html">
     higher-hrnet-w32-human-pose-estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/hrnet-v2-c1-segmentation/README.html">
     hrnet-v2-c1-segmentation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/human-pose-estimation-3d-0001/README.html">
     human-pose-estimation-3d-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/hybrid-cs-model-mri/README.html">
     hybrid-cs-model-mri
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/i3d-rgb-tf/README.html">
     i3d-rgb-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/inception-resnet-v2-tf/README.html">
     inception-resnet-v2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/levit-128s/README.html">
     levit-128s
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/license-plate-recognition-barrier-0007/README.html">
     license-plate-recognition-barrier-0007
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mask_rcnn_inception_resnet_v2_atrous_coco/README.html">
     mask_rcnn_inception_resnet_v2_atrous_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mask_rcnn_resnet50_atrous_coco/README.html">
     mask_rcnn_resnet50_atrous_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/midasnet/README.html">
     midasnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mixnet-l/README.html">
     mixnet-l
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mobilefacedet-v1-mxnet/README.html">
     mobilefacedet-v1-mxnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mobilenet-ssd/README.html">
     mobilenet-ssd
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mobilenet-v1-0.25-128/README.html">
     mobilenet-v1-0.25-128
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mobilenet-v1-1.0-224/README.html">
     mobilenet-v1-1.0-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mobilenet-v1-1.0-224-tf/README.html">
     mobilenet-v1-1.0-224-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mobilenet-v2/README.html">
     mobilenet-v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mobilenet-v2-1.0-224/README.html">
     mobilenet-v2-1.0-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mobilenet-v2-1.4-224/README.html">
     mobilenet-v2-1.4-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mobilenet-v2-pytorch/README.html">
     mobilenet-v2-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mobilenet-v3-large-1.0-224-paddle/README.html">
     mobilenet-v3-large-1.0-224-paddle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mobilenet-v3-large-1.0-224-tf/README.html">
     mobilenet-v3-large-1.0-224-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mobilenet-v3-small-1.0-224-paddle/README.html">
     mobilenet-v3-small-1.0-224-paddle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mobilenet-v3-small-1.0-224-tf/README.html">
     mobilenet-v3-small-1.0-224-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mobilenet-yolo-v4-syg/README.html">
     mobilenet-yolo-v4-syg
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/modnet-photographic-portrait-matting/README.html">
     modnet-photographic-portrait-matting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/modnet-webcam-portrait-matting/README.html">
     modnet-webcam-portrait-matting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mozilla-deepspeech-0.6.1/README.html">
     mozilla-deepspeech-0.6.1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mozilla-deepspeech-0.8.2/README.html">
     mozilla-deepspeech-0.8.2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/mtcnn/README.html">
     mtcnn (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/nanodet-m-1.5x-416/README.html">
     nanodet-m-1.5x-416
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/nanodet-plus-m-1.5x-416/README.html">
     nanodet-plus-m-1.5x-416
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/netvlad-tf/README.html">
     netvlad-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/nfnet-f0/README.html">
     nfnet-f0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/ocrnet-hrnet-w48-paddle/README.html">
     ocrnet-hrnet-w48-paddle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/octave-resnet-26-0.25/README.html">
     octave-resnet-26-0.25
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/open-closed-eye-0001/README.html">
     open-closed-eye-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/pelee-coco/README.html">
     pelee-coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/pspnet-pytorch/README.html">
     pspnet-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/quartznet-15x5-en/README.html">
     quartznet-15x5-en
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/regnetx-3.2gf/README.html">
     regnetx-3.2gf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/repvgg-a0/README.html">
     repvgg-a0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/repvgg-b1/README.html">
     repvgg-b1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/repvgg-b3/README.html">
     repvgg-b3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/resnest-50-pytorch/README.html">
     resnest-50-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/resnet-18-pytorch/README.html">
     resnet-18-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/resnet-34-pytorch/README.html">
     resnet-34-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/resnet-50-pytorch/README.html">
     resnet-50-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/resnet-50-tf/README.html">
     resnet-50-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/retinaface-resnet50-pytorch/README.html">
     retinaface-resnet50-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/retinanet-tf/README.html">
     retinanet-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/rexnet-v1-x1.0/README.html">
     rexnet-v1-x1.0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/rfcn-resnet101-coco-tf/README.html">
     rfcn-resnet101-coco-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/robust-video-matting-mobilenetv3/README.html">
     robust-video-matting-mobilenetv3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/se-inception/README.html">
     se-inception
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/se-resnet-50/README.html">
     se-resnet-50
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/se-resnext-50/README.html">
     se-resnext-50
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/shufflenet-v2-x0.5/README.html">
     shufflenet-v2-x0.5
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/shufflenet-v2-x1.0/README.html">
     shufflenet-v2-x1.0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/single-human-pose-estimation-0001/README.html">
     single-human-pose-estimation-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/squeezenet1.0/README.html">
     squeezenet1.0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/squeezenet1.1/README.html">
     squeezenet1.1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/ssd-resnet34-1200-onnx/README.html">
     ssd-resnet34-1200-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/ssd300/README.html">
     ssd300
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/ssd512/README.html">
     ssd512
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/ssd_mobilenet_v1_coco/README.html">
     ssd_mobilenet_v1_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/ssd_mobilenet_v1_fpn_coco/README.html">
     ssd_mobilenet_v1_fpn_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/ssdlite_mobilenet_v2/README.html">
     ssdlite_mobilenet_v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/swin-tiny-patch4-window7-224/README.html">
     swin-tiny-patch4-window7-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/t2t-vit-14/README.html">
     t2t-vit-14
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/text-recognition-resnet-fc/README.html">
     text-recognition-resnet-fc
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/ultra-lightweight-face-detection-rfb-320/README.html">
     ultra-lightweight-face-detection-rfb-320
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/ultra-lightweight-face-detection-slim-320/README.html">
     ultra-lightweight-face-detection-slim-320
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/vehicle-license-plate-detection-barrier-0123/README.html">
     vehicle-license-plate-detection-barrier-0123
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/vehicle-reid-0001/README.html">
     vehicle-reid-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/vgg16/README.html">
     vgg16
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/vgg19/README.html">
     vgg19
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/vitstr-small-patch16-224/README.html">
     vitstr-small-patch16-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/wav2vec2-base/README.html">
     wav2vec2-base
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/wavernn/README.html">
     wavernn (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/yolact-resnet50-fpn-pytorch/README.html">
     yolact-resnet50-fpn-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/yolo-v1-tiny-tf/README.html">
     yolo-v1-tiny-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/yolo-v2-tf/README.html">
     yolo-v2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/yolo-v2-tiny-tf/README.html">
     yolo-v2-tiny-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/yolo-v3-onnx/README.html">
     yolo-v3-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/yolo-v3-tf/README.html">
     yolo-v3-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/yolo-v3-tiny-onnx/README.html">
     yolo-v3-tiny-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/yolo-v3-tiny-tf/README.html">
     yolo-v3-tiny-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/yolo-v4-tf/README.html">
     yolo-v4-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/yolo-v4-tiny-tf/README.html">
     yolo-v4-tiny-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/yolof/README.html">
     yolof
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/public/yolox-tiny/README.html">
     yolox-tiny
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Demo Applications
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="current reference internal" href="#">
   Open Model Zoo Demos
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="3d_segmentation_demo/python/README.html">
     3D Segmentation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="action_recognition_demo/python/README.html">
     Action Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="background_subtraction_demo/cpp_gapi/README.html">
     G-API Background Subtraction Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="background_subtraction_demo/python/README.html">
     Background subtraction Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bert_named_entity_recognition_demo/python/README.html">
     BERT Named Entity Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bert_question_answering_demo/python/README.html">
     BERT Question Answering Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bert_question_answering_embedding_demo/python/README.html">
     BERT Question Answering Embedding Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_benchmark_demo/cpp/README.html">
     Classification Benchmark C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_demo/python/README.html">
     Classification Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="colorization_demo/python/README.html">
     Colorization Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="crossroad_camera_demo/cpp/README.html">
     Crossroad Camera C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="deblurring_demo/python/README.html">
     Image Deblurring Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="face_detection_mtcnn_demo/cpp_gapi/README.html">
     G-API Face Detection MTCNN Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="face_detection_mtcnn_demo/python/README.html">
     Face Detection MTCNN Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="face_recognition_demo/python/README.html">
     Face Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="formula_recognition_demo/python/README.html">
     Formula Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gaze_estimation_demo/cpp/README.html">
     Gaze Estimation Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gaze_estimation_demo/cpp_gapi/README.html">
     G-API Gaze Estimation Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gesture_recognition_demo/cpp_gapi/README.html">
     G-API Gesture Recognition Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gesture_recognition_demo/python/README.html">
     Gesture Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gpt2_text_prediction_demo/python/README.html">
     GPT-2 Text Prediction Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="handwritten_text_recognition_demo/python/README.html">
     Handwritten Text Recognition Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="human_pose_estimation_3d_demo/python/README.html">
     3D Human Pose Estimation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="human_pose_estimation_demo/cpp/README.html">
     Human Pose Estimation C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="human_pose_estimation_demo/python/README.html">
     Human Pose Estimation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="image_inpainting_demo/python/README.html">
     Image Inpainting Python Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="image_processing_demo/cpp/README.html">
     Image Processing C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="image_retrieval_demo/python/README.html">
     Image Retrieval Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="image_translation_demo/python/README.html">
     Image Translation Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="instance_segmentation_demo/python/README.html">
     Instance Segmentation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="interactive_face_detection_demo/cpp/README.html">
     Interactive Face Detection C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="interactive_face_detection_demo/cpp_gapi/README.html">
     G-API Interactive Face Detection Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="machine_translation_demo/python/README.html">
     Machine Translation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mask_rcnn_demo/cpp/README.html">
     TensorFlow* Object Detection Mask R-CNNs Segmentation C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="monodepth_demo/python/README.html">
     MonoDepth Python Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mri_reconstruction_demo/cpp/README.html">
     MRI Reconstruction C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mri_reconstruction_demo/python/README.html">
     MRI Reconstruction Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multi_camera_multi_target_tracking_demo/python/README.html">
     Multi Camera Multi Target Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multi_channel_face_detection_demo/cpp/README.html">
     Multi-Channel Face Detection C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multi_channel_human_pose_estimation_demo/cpp/README.html">
     Multi-Channel Human Pose Estimation C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multi_channel_object_detection_demo_yolov3/cpp/README.html">
     Multi-Channel Object Detection Yolov3 C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="noise_suppression_demo/cpp/README.html">
     Noise Suppression C++* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="noise_suppression_demo/python/README.html">
     Noise Suppression Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="object_detection_demo/cpp/README.html">
     Object Detection C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="object_detection_demo/python/README.html">
     Object Detection Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="pedestrian_tracker_demo/cpp/README.html">
     Pedestrian Tracker C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="place_recognition_demo/python/README.html">
     Place Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="security_barrier_camera_demo/cpp/README.html">
     Security Barrier Camera C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="segmentation_demo/cpp/README.html">
     Image Segmentation C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="segmentation_demo/python/README.html">
     Image Segmentation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="single_human_pose_estimation_demo/python/README.html">
     Single Human Pose Estimation Demo (top-down pipeline)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="smart_classroom_demo/cpp/README.html">
     Smart Classroom C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="smart_classroom_demo/cpp_gapi/README.html">
     Smart Classroom C++ G-API Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="smartlab_demo/python/README.html">
     Smartlab Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="social_distance_demo/cpp/README.html">
     Social Distance C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sound_classification_demo/python/README.html">
     Sound Classification Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="speech_recognition_deepspeech_demo/python/README.html">
     Speech Recognition DeepSpeech Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="speech_recognition_quartznet_demo/python/README.html">
     Speech Recognition QuartzNet Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="speech_recognition_wav2vec_demo/python/README.html">
     Speech Recognition Wav2Vec Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text_detection_demo/cpp/README.html">
     Text Detection C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text_spotting_demo/python/README.html">
     Text Spotting Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text_to_speech_demo/python/README.html">
     Text-to-speech Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="time_series_forecasting_demo/python/README.html">
     Time Series Forecasting Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="whiteboard_inpainting_demo/python/README.html">
     Whiteboard Inpainting Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="common/python/openvino/model_zoo/model_api/adapters/ovms_adapter.html">
     OpenVINO Model Server Adapter
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Model API
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="common/python/openvino/model_zoo/model_api/adapters/ovms_adapter.html">
   OpenVINO Model Server Adapter
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#media-files-available-for-demos">
   Media Files Available for Demos
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#demos-that-support-pre-trained-models">
   Demos that Support Pre-Trained Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#build-the-demo-applications">
   Build the Demo Applications
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-build-demos-linux-a-build-the-demo-applications-on-linux">
     <a name="build_demos_linux">
     </a>
     Build the Demo Applications on Linux*
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-build-demos-windows-a-build-the-demos-applications-on-microsoft-windows-os">
     <a name="build_demos_windows">
     </a>
     Build the Demos Applications on Microsoft Windows* OS
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-python-requirements-a-dependencies-for-python-demos">
     <a name="python_requirements">
     </a>
     Dependencies for Python* Demos
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-python-model-api-a-python-model-api-package">
     <a name="python_model_api">
     </a>
     Python* model API package
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-build-python-extensions-a-build-the-native-python-extension-modules">
     <a name="build_python_extensions">
     </a>
     Build the Native Python* Extension Modules
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-build-specific-demos-a-build-specific-demos">
     <a name="build_specific_demos">
     </a>
     Build Specific Demos
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#get-ready-for-running-the-demo-applications">
   Get Ready for Running the Demo Applications
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#get-ready-for-running-the-demo-applications-on-linux">
     Get Ready for Running the Demo Applications on Linux*
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#get-ready-for-running-the-demo-applications-on-windows">
     Get Ready for Running the Demo Applications on Windows*
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#see-also">
   See Also
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <section id="open-model-zoo-demos">
<h1>Open Model Zoo Demos<a class="headerlink" href="#open-model-zoo-demos" title="Permalink to this headline">¶</a></h1>
<!--
@sphinxdirective

.. toctree::
   :maxdepth: 1
   :hidden:

   omz_demos_human_pose_estimation_3d_demo_python
   omz_demos_3d_segmentation_demo_python
   omz_demos_action_recognition_demo_python
   omz_demos_background_subtraction_demo_cpp_gapi
   omz_demos_background_subtraction_demo_python
   omz_demos_bert_named_entity_recognition_demo_python
   omz_demos_bert_question_answering_embedding_demo_python
   omz_demos_bert_question_answering_demo_python
   omz_demos_classification_benchmark_demo_cpp
   omz_demos_classification_demo_python
   omz_demos_colorization_demo_python
   omz_demos_crossroad_camera_demo_cpp
   omz_demos_face_detection_mtcnn_demo_cpp_gapi
   omz_demos_face_detection_mtcnn_demo_python
   omz_demos_face_recognition_demo_python
   omz_demos_formula_recognition_demo_python
   omz_demos_gaze_estimation_demo_cpp_gapi
   omz_demos_interactive_face_detection_demo_cpp_gapi
   omz_demos_gaze_estimation_demo_cpp
   omz_demos_gesture_recognition_demo_cpp_gapi
   omz_demos_gesture_recognition_demo_python
   omz_demos_gpt2_text_prediction_demo_python
   omz_demos_handwritten_text_recognition_demo_python
   omz_demos_human_pose_estimation_demo_cpp
   omz_demos_human_pose_estimation_demo_python
   omz_demos_deblurring_demo_python
   omz_demos_image_inpainting_demo_python
   omz_demos_image_processing_demo_cpp
   omz_demos_image_retrieval_demo_python
   omz_demos_segmentation_demo_cpp
   omz_demos_segmentation_demo_python
   omz_demos_image_translation_demo_python
   omz_demos_instance_segmentation_demo_python
   omz_demos_interactive_face_detection_demo_cpp
   omz_demos_machine_translation_demo_python
   omz_demos_monodepth_demo_python
   omz_demos_mri_reconstruction_demo_cpp
   omz_demos_mri_reconstruction_demo_python
   omz_demos_multi_camera_multi_target_tracking_demo_python
   omz_demos_multi_channel_face_detection_demo_cpp
   omz_demos_multi_channel_human_pose_estimation_demo_cpp
   omz_demos_multi_channel_object_detection_demo_yolov3_cpp
   omz_demos_noise_suppression_demo_cpp
   omz_demos_noise_suppression_demo_python
   omz_demos_object_detection_demo_cpp
   omz_demos_object_detection_demo_python
   omz_demos_pedestrian_tracker_demo_cpp
   omz_demos_place_recognition_demo_python
   omz_demos_security_barrier_camera_demo_cpp
   omz_demos_single_human_pose_estimation_demo_python
   omz_demos_smartlab_demo_python
   omz_demos_smart_classroom_demo_cpp
   omz_demos_smart_classroom_demo_cpp_gapi
   omz_demos_social_distance_demo_cpp
   omz_demos_sound_classification_demo_python
   omz_demos_speech_recognition_deepspeech_demo_python
   omz_demos_speech_recognition_quartznet_demo_python
   omz_demos_speech_recognition_wav2vec_demo_python
   omz_demos_mask_rcnn_demo_cpp
   omz_demos_text_detection_demo_cpp
   omz_demos_text_spotting_demo_python
   omz_demos_text_to_speech_demo_python
   omz_demos_time_series_forecasting_demo_python
   omz_demos_whiteboard_inpainting_demo_python

@endsphinxdirective
-->
<p>The Open Model Zoo demo applications are console applications that provide robust application templates to help you implement specific deep learning scenarios. These applications involve increasingly complex processing pipelines that gather analysis data from several models that run inference simultaneously, such as detecting a person in a video stream along with detecting the person’s physical attributes, such as age, gender, and emotional state</p>
<p>Source code of the demos can be obtained from the Open Model Zoo <a class="reference external" href="https://github.com/openvinotoolkit/open_model_zoo/">GitHub repository</a>.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/openvinotoolkit/open_model_zoo.git
<span class="nb">cd</span> open_model_zoo
git submodule update --init --recursive
</pre></div>
</div>
<p>C++, C++ G-API and Python* versions are located in the <code class="docutils literal notranslate"><span class="pre">cpp</span></code>, <code class="docutils literal notranslate"><span class="pre">cpp_gapi</span></code> and <code class="docutils literal notranslate"><span class="pre">python</span></code> subdirectories respectively.</p>
<p>The Open Model Zoo includes the following demos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="human_pose_estimation_3d_demo/python/README.html"><span class="doc std std-doc">3D Human Pose Estimation Python* Demo</span></a> - 3D human pose estimation demo.</p></li>
<li><p><a class="reference internal" href="3d_segmentation_demo/python/README.html"><span class="doc std std-doc">3D Segmentation Python* Demo</span></a> - Segmentation demo segments 3D images using 3D convolutional networks.</p></li>
<li><p><a class="reference internal" href="action_recognition_demo/python/README.html"><span class="doc std std-doc">Action Recognition Python* Demo</span></a> - Demo application for Action Recognition algorithm, which classifies actions that are being performed on input video.</p></li>
<li><p><a class="reference internal" href="background_subtraction_demo/python/README.html"><span class="doc std std-doc">Background Subtraction Python* Demo</span></a> - Background subtraction using instance segmentation based models.</p></li>
<li><p><a class="reference internal" href="background_subtraction_demo/cpp_gapi/README.html"><span class="doc std std-doc">Background Subtraction C++ G-API* Demo</span></a> - Background subtraction G-API version.</p></li>
<li><p><a class="reference internal" href="bert_named_entity_recognition_demo/python/README.html"><span class="doc std std-doc">BERT Named Entity Recognition Python* Demo</span></a> - NER Demo application that uses a CONLL2003-tuned BERT model for inference.</p></li>
<li><p><a class="reference internal" href="bert_question_answering_demo/python/README.html"><span class="doc std std-doc">BERT Question Answering Python* Demo</span></a></p></li>
<li><p><a class="reference internal" href="bert_question_answering_embedding_demo/python/README.html"><span class="doc std std-doc">BERT Question Answering Embedding Python* Demo</span></a> - The demo demonstrates how to run BERT based models for question answering task.</p></li>
<li><p><a class="reference internal" href="classification_demo/python/README.html"><span class="doc std std-doc">Classification Python* Demo</span></a> - Shows an example of using neural networks for image classification.</p></li>
<li><p><a class="reference internal" href="classification_benchmark_demo/cpp/README.html"><span class="doc std std-doc">Classification Benchmark C++ Demo</span></a> - Visualizes OpenVINO performance on inference of neural networks for image classification.</p></li>
<li><p><a class="reference internal" href="colorization_demo/python/README.html"><span class="doc std std-doc">Colorization Python* Demo</span></a> - Colorization demo colorizes input frames.</p></li>
<li><p><a class="reference internal" href="crossroad_camera_demo/cpp/README.html"><span class="doc std std-doc">Crossroad Camera C++ Demo</span></a> - Person Detection followed by the Person Attributes Recognition and Person Reidentification Retail, supports images/video and camera inputs.</p></li>
<li><p><a class="reference internal" href="deblurring_demo/python/README.html"><span class="doc std std-doc">Deblurring Python* Demo</span></a> - Demo for deblurring the input images.</p></li>
<li><p><a class="reference internal" href="face_detection_mtcnn_demo/python/README.html"><span class="doc std std-doc">Face Detection MTCNN Python* Demo</span></a> - The demo demonstrates how to run MTCNN face detection model to detect faces on images.</p></li>
<li><p><a class="reference internal" href="face_detection_mtcnn_demo/cpp_gapi/README.html"><span class="doc std std-doc">Face Detection MTCNN C++ G-API* Demo</span></a> - The demo demonstrates how to run MTCNN face detection model to detect faces on images. G-API version.</p></li>
<li><p><a class="reference internal" href="face_recognition_demo/python/README.html"><span class="doc std std-doc">Face Recognition Python* Demo</span></a> - The interactive face recognition demo.</p></li>
<li><p><a class="reference internal" href="formula_recognition_demo/python/README.html"><span class="doc std std-doc">Formula Recognition Python* Demo</span></a> - The demo demonstrates how to run Im2latex formula recognition models and recognize latex formulas.</p></li>
<li><p><a class="reference internal" href="gaze_estimation_demo/cpp/README.html"><span class="doc std std-doc">Gaze Estimation C++ Demo</span></a> - Face detection followed by gaze estimation, head pose estimation and facial landmarks regression.</p></li>
<li><p><a class="reference internal" href="gaze_estimation_demo/cpp_gapi/README.html"><span class="doc std std-doc">Gaze Estimation C++ G-API* Demo</span></a> - Face detection followed by gaze estimation, head pose estimation and facial landmarks regression. G-API version.</p></li>
<li><p><a class="reference internal" href="gesture_recognition_demo/python/README.html"><span class="doc std std-doc">Gesture Recognition Python* Demo</span></a> - Demo application for Gesture Recognition algorithm (e.g. American Sign Language gestures), which classifies gesture actions that are being performed on input video.</p></li>
<li><p><a class="reference internal" href="gesture_recognition_demo/cpp_gapi/README.html"><span class="doc std std-doc">Gesture Recognition C++ G-API* Demo</span></a> - Demo application for Gesture Recognition algorithm (e.g. American Sign Language gestures), which classifies gesture actions that are being performed on input video. G-API version.</p></li>
<li><p><a class="reference internal" href="gpt2_text_prediction_demo/python/README.html"><span class="doc std std-doc">GPT-2 Text Prediction Python* Demo</span></a> - GPT-2 text prediction demo.</p></li>
<li><p><a class="reference internal" href="handwritten_text_recognition_demo/python/README.html"><span class="doc std std-doc">Handwritten Text Recognition Python* Demo</span></a> - The demo demonstrates how to run Handwritten Text Recognition models for Japanese, Simplified Chinese and English.</p></li>
<li><p><a class="reference internal" href="human_pose_estimation_demo/cpp/README.html"><span class="doc std std-doc">Human Pose Estimation C++ Demo</span></a> - Human pose estimation demo.</p></li>
<li><p><a class="reference internal" href="human_pose_estimation_demo/python/README.html"><span class="doc std std-doc">Human Pose Estimation Python* Demo</span></a> - Human pose estimation demo.</p></li>
<li><p><a class="reference internal" href="image_inpainting_demo/python/README.html"><span class="doc std std-doc">Image Inpainting Python* Demo</span></a> - Demo application for GMCNN inpainting network.</p></li>
<li><p><a class="reference internal" href="image_processing_demo/cpp/README.html"><span class="doc std std-doc">Image Processing C++ Demo</span></a> - Demo application for deblurring and enhancing the resolution of the input image.</p></li>
<li><p><a class="reference internal" href="image_retrieval_demo/python/README.html"><span class="doc std std-doc">Image Retrieval Python* Demo</span></a> - The demo demonstrates how to run Image Retrieval models using OpenVINO™.</p></li>
<li><p><a class="reference internal" href="segmentation_demo/cpp/README.html"><span class="doc std std-doc">Image Segmentation C++ Demo</span></a> - Inference of semantic segmentation networks (supports video and camera inputs).</p></li>
<li><p><a class="reference internal" href="segmentation_demo/python/README.html"><span class="doc std std-doc">Image Segmentation Python* Demo</span></a> - Inference of semantic segmentation networks (supports video and camera inputs).</p></li>
<li><p><a class="reference internal" href="image_translation_demo/python/README.html"><span class="doc std std-doc">Image Translation Python* Demo</span></a> - Demo application to synthesize a photo-realistic image based on exemplar image.</p></li>
<li><p><a class="reference internal" href="instance_segmentation_demo/python/README.html"><span class="doc std std-doc">Instance Segmentation Python* Demo</span></a> - Inference of instance segmentation networks trained in <code class="docutils literal notranslate"><span class="pre">Detectron</span></code> or <code class="docutils literal notranslate"><span class="pre">maskrcnn-benchmark</span></code>.</p></li>
<li><p><a class="reference internal" href="interactive_face_detection_demo/cpp/README.html"><span class="doc std std-doc">Interactive Face Detection C++ Demo</span></a> - Face Detection coupled with Age/Gender, Head-Pose, Emotion, and Facial Landmarks detectors. Supports video and camera inputs.</p></li>
<li><p><a class="reference internal" href="interactive_face_detection_demo/cpp_gapi/README.html"><span class="doc std std-doc">Interactive Face Detection G-API* Demo</span></a> - G-API based Face Detection coupled with Age/Gender, Head-Pose, Emotion, and Facial Landmarks detectors. Supports video and camera inputs.</p></li>
<li><p><a class="reference internal" href="machine_translation_demo/python/README.html"><span class="doc std std-doc">Machine Translation Python* Demo</span></a> - The demo demonstrates how to run non-autoregressive machine translation models.</p></li>
<li><p><a class="reference internal" href="mask_rcnn_demo/cpp/README.html"><span class="doc std std-doc">Mask R-CNN C++ Demo for TensorFlow* Object Detection API</span></a> - Inference of instance segmentation networks created with TensorFlow* Object Detection API.</p></li>
<li><p><a class="reference internal" href="monodepth_demo/python/README.html"><span class="doc std std-doc">Monodepth Python* Demo</span></a> - The demo demonstrates how to run monocular depth estimation models.</p></li>
<li><p><a class="reference internal" href="mri_reconstruction_demo/cpp/README.html"><span class="doc std std-doc">MRI Reconstruction C++ Demo</span></a> - Compressed Sensing demo for medical images</p></li>
<li><p><a class="reference internal" href="mri_reconstruction_demo/python/README.html"><span class="doc std std-doc">MRI Reconstruction Python* Demo</span></a> - Compressed Sensing demo for medical images</p></li>
<li><p><a class="reference internal" href="multi_camera_multi_target_tracking_demo/python/README.html"><span class="doc std std-doc">Multi-Camera Multi-Target Tracking Python* Demo</span></a> Demo application for multiple targets (persons or vehicles) tracking on multiple cameras.</p></li>
<li><p><a class="reference internal" href="multi_channel_face_detection_demo/cpp/README.html"><span class="doc std std-doc">Multi-Channel Face Detection C++ Demo</span></a> - The demo demonstrates an inference pipeline for multi-channel face detection scenario.</p></li>
<li><p><a class="reference internal" href="multi_channel_human_pose_estimation_demo/cpp/README.html"><span class="doc std std-doc">Multi-Channel Human Pose Estimation C++ Demo</span></a> - The demo demonstrates an inference pipeline for multi-channel human pose estimation scenario.</p></li>
<li><p><a class="reference internal" href="multi_channel_object_detection_demo_yolov3/cpp/README.html"><span class="doc std std-doc">Multi-Channel Object Detection Yolov3 C++ Demo</span></a> - The demo demonstrates an inference pipeline for multi-channel common object detection scenario.</p></li>
<li><p><a class="reference internal" href="noise_suppression_demo/python/README.html"><span class="doc std std-doc">Noise Suppression Python* Demo</span></a> - The demo shows how to use the OpenVINO™ toolkit to reduce noise in speech audio.</p></li>
<li><p><a class="reference internal" href="noise_suppression_demo/cpp/README.html"><span class="doc std std-doc">Noise Suppression C++* Demo</span></a> - The demo shows how to use the OpenVINO™ toolkit to reduce noise in speech audio.</p></li>
<li><p><a class="reference internal" href="object_detection_demo/python/README.html"><span class="doc std std-doc">Object Detection Python* Demo</span></a> - Demo application for several object detection model types (like SSD, Yolo, etc).</p></li>
<li><p><a class="reference internal" href="object_detection_demo/cpp/README.html"><span class="doc std std-doc">Object Detection C++ Demo</span></a> - Demo application for Object Detection networks (different models architectures are supported), async API showcase, simple OpenCV interoperability (supports video and camera inputs).</p></li>
<li><p><a class="reference internal" href="pedestrian_tracker_demo/cpp/README.html"><span class="doc std std-doc">Pedestrian Tracker C++ Demo</span></a> - Demo application for pedestrian tracking scenario.</p></li>
<li><p><a class="reference internal" href="place_recognition_demo/python/README.html"><span class="doc std std-doc">Place Recognition Python* Demo</span></a> - This demo demonstrates how to run Place Recognition models using OpenVINO™.</p></li>
<li><p><a class="reference internal" href="security_barrier_camera_demo/cpp/README.html"><span class="doc std std-doc">Security Barrier Camera C++ Demo</span></a> - Vehicle Detection followed by the Vehicle Attributes and License-Plate Recognition, supports images/video and camera inputs.</p></li>
<li><p><a class="reference internal" href="speech_recognition_deepspeech_demo/python/README.html"><span class="doc std std-doc">Speech Recognition DeepSpeech Python* Demo</span></a> - Speech recognition demo: accepts an audio file with an English phrase on input and converts it into text. This demo does streaming audio data processing and can optionally provide current transcription of the processed part.</p></li>
<li><p><a class="reference internal" href="speech_recognition_quartznet_demo/python/README.html"><span class="doc std std-doc">Speech Recognition QuartzNet Python* Demo</span></a> - Speech recognition demo for QuartzNet: takes a whole audio file with an English phrase on input and converts it into text.</p></li>
<li><p><a class="reference internal" href="speech_recognition_wav2vec_demo/python/README.html"><span class="doc std std-doc">Speech Recognition Wav2Vec Python* Demo</span></a> - Speech recognition demo for Wav2Vec: takes a whole audio file with an English phrase on input and converts it into text.</p></li>
<li><p><a class="reference internal" href="single_human_pose_estimation_demo/python/README.html"><span class="doc std std-doc">Single Human Pose Estimation Python* Demo</span></a> - 2D human pose estimation demo.</p></li>
<li><p><a class="reference internal" href="smart_classroom_demo/cpp/README.html"><span class="doc std std-doc">Smart Classroom C++ Demo</span></a> - Face recognition and action detection demo for classroom environment.</p></li>
<li><p><a class="reference internal" href="smart_classroom_demo/cpp_gapi/README.html"><span class="doc std std-doc">Smart Classroom C++ G-API Demo</span></a> - Face recognition and action detection demo for classroom environment. G-PI version.</p></li>
<li><p><a class="reference internal" href="smartlab_demo/python/README.html"><span class="doc std std-doc">Smartlab Python* Demo</span></a> - action recognition and object detection for smartlab.</p></li>
<li><p><a class="reference internal" href="social_distance_demo/cpp/README.html"><span class="doc std std-doc">Social Distance C++ Demo</span></a> - This demo showcases a retail social distance application that detects people and measures the distance between them.</p></li>
<li><p><a class="reference internal" href="sound_classification_demo/python/README.html"><span class="doc std std-doc">Sound Classification Python* Demo</span></a> - Demo application for sound classification algorithm.</p></li>
<li><p><a class="reference internal" href="text_detection_demo/cpp/README.html"><span class="doc std std-doc">Text Detection C++ Demo</span></a> - Text Detection demo. It detects and recognizes multi-oriented scene text on an input image and puts a bounding box around detected area.</p></li>
<li><p><a class="reference internal" href="text_spotting_demo/python/README.html"><span class="doc std std-doc">Text Spotting Python* Demo</span></a> - The demo demonstrates how to run Text Spotting models.</p></li>
<li><p><a class="reference internal" href="text_to_speech_demo/python/README.html"><span class="doc std std-doc">Text-to-speech Python* Demo</span></a> - Shows an example of using Forward Tacotron and WaveRNN neural networks for text to speech task.</p></li>
<li><p><a class="reference internal" href="time_series_forecasting_demo/python/README.html"><span class="doc std std-doc">Time Series Forecasting Python* Demo</span></a> - The demo shows how to use the OpenVINO™ toolkit to time series forecasting.</p></li>
<li><p><a class="reference internal" href="whiteboard_inpainting_demo/python/README.html"><span class="doc std std-doc">Whiteboard Inpainting Python* Demo</span></a> - The demo shows how to use the OpenVINO™ toolkit to detect and hide a person on a video so that all text on a whiteboard is visible.</p></li>
</ul>
<section id="media-files-available-for-demos">
<h2>Media Files Available for Demos<a class="headerlink" href="#media-files-available-for-demos" title="Permalink to this headline">¶</a></h2>
<p>To run the demo applications, you can use images and videos from the media files collection available at https://github.com/intel-iot-devkit/sample-videos.</p>
</section>
<section id="demos-that-support-pre-trained-models">
<h2>Demos that Support Pre-Trained Models<a class="headerlink" href="#demos-that-support-pre-trained-models" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p><strong>NOTE:</strong> OpenVINO™ Runtime HDDL plugin is available in <a class="reference external" href="https://software.intel.com/en-us/openvino-toolkit">proprietary</a> distribution only.</p>
</div></blockquote>
<p>You can download the <a class="reference internal" href="../models/intel/index.html"><span class="doc std std-doc">Intel pre-trained models</span></a> or <a class="reference internal" href="../models/public/index.html"><span class="doc std std-doc">public pre-trained models</span></a> using the OpenVINO <span class="xref myst">Model Downloader</span>.</p>
</section>
<section id="build-the-demo-applications">
<h2>Build the Demo Applications<a class="headerlink" href="#build-the-demo-applications" title="Permalink to this headline">¶</a></h2>
<p>To build the demos, you need to source OpenVINO™ and OpenCV environment. You can install the OpenVINO™ toolkit using the installation package for <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit-download.html">Intel® Distribution of OpenVINO™ toolkit</a> or build the open-source version available in the <a class="reference external" href="https://github.com/openvinotoolkit/openvino">OpenVINO GitHub repository</a> using the <a class="reference external" href="https://github.com/openvinotoolkit/openvino/wiki/BuildingCode">build instructions</a>.
For the Intel® Distribution of OpenVINO™ toolkit installed to the <code class="docutils literal notranslate"><span class="pre">&lt;INSTALL_DIR&gt;</span></code> directory on your machine, run the following commands to download prebuilt OpenCV and set environment variables before building the demos:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>&lt;INSTALL_DIR&gt;/extras/scripts/download_opencv.sh
<span class="nb">source</span> &lt;INSTALL_DIR&gt;/setupvars.sh
</pre></div>
</div>
<blockquote>
<div><p><strong>NOTE:</strong> If you plan to use Python* demos only, you can install the OpenVINO Python* package.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pip install openvino
</pre></div>
</div>
</div></blockquote>
<p>For the open-source version of OpenVINO, set the following variables:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">OpenVINO_DIR</span></code> pointing to a folder containing <code class="docutils literal notranslate"><span class="pre">OpenVINOConfig.cmake</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OpenCV_DIR</span></code> pointing to OpenCV. The same OpenCV version should be used both for OpenVINO and demos build.</p></li>
</ul>
<p>Alternatively, these values can be provided via command line while running <code class="docutils literal notranslate"><span class="pre">cmake</span></code>. See <a class="reference external" href="https://cmake.org/cmake/help/latest/command/find_package.html#search-procedure">CMake search procedure</a>.
Also add paths to the built OpenVINO™ Runtime libraries to the <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> (Linux) or <code class="docutils literal notranslate"><span class="pre">PATH</span></code> (Windows) variable before building the demos.</p>
<section id="a-name-build-demos-linux-a-build-the-demo-applications-on-linux">
<h3><a name="build_demos_linux"></a>Build the Demo Applications on Linux*<a class="headerlink" href="#a-name-build-demos-linux-a-build-the-demo-applications-on-linux" title="Permalink to this headline">¶</a></h3>
<p>The officially supported Linux* build environment is the following:</p>
<ul class="simple">
<li><p>Ubuntu* 18.04 LTS 64-bit or Ubuntu* 20.04 LTS 64-bit</p></li>
<li><p>GCC* 7.5.0 (for Ubuntu* 18.04) or GCC* 9.3.0 (for Ubuntu* 20.04)</p></li>
<li><p>CMake* version 3.10 or higher.</p></li>
</ul>
<p>To build the demo applications for Linux, go to the directory with the <code class="docutils literal notranslate"><span class="pre">build_demos.sh</span></code> script and
run it:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>build_demos.sh
</pre></div>
</div>
<p>You can also build the demo applications manually:</p>
<ol class="arabic simple">
<li><p>Navigate to a directory that you have write access to and create a demos build directory. This example uses a directory named <code class="docutils literal notranslate"><span class="pre">build</span></code>:</p></li>
</ol>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>mkdir build
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Go to the created directory:</p></li>
</ol>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> build
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Run CMake to generate the Make files for release or debug configuration:</p></li>
</ol>
<ul class="simple">
<li><p>For release configuration:</p></li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>cmake -DCMAKE_BUILD_TYPE<span class="o">=</span>Release &lt;open_model_zoo&gt;/demos
</pre></div>
</div>
<ul class="simple">
<li><p>For debug configuration:</p></li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>cmake -DCMAKE_BUILD_TYPE<span class="o">=</span>Debug &lt;open_model_zoo&gt;/demos
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">cmake</span> <span class="pre">--build</span></code> tool to build the demos:</p></li>
</ol>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>cmake --build .
</pre></div>
</div>
<p>For the release configuration, the demo application binaries are in <code class="docutils literal notranslate"><span class="pre">&lt;path_to_build_directory&gt;/intel64/Release/</span></code>;
for the debug configuration — in <code class="docutils literal notranslate"><span class="pre">&lt;path_to_build_directory&gt;/intel64/Debug/</span></code>.</p>
</section>
<section id="a-name-build-demos-windows-a-build-the-demos-applications-on-microsoft-windows-os">
<h3><a name="build_demos_windows"></a>Build the Demos Applications on Microsoft Windows* OS<a class="headerlink" href="#a-name-build-demos-windows-a-build-the-demos-applications-on-microsoft-windows-os" title="Permalink to this headline">¶</a></h3>
<p>The recommended Windows* build environment is the following:</p>
<ul class="simple">
<li><p>Microsoft Windows* 10</p></li>
<li><p>Microsoft Visual Studio* 2019</p></li>
<li><p>CMake* version 3.14 or higher</p></li>
</ul>
<p>To build the demo applications for Windows, go to the directory with the <code class="docutils literal notranslate"><span class="pre">build_demos_msvc.bat</span></code>
batch file and run it:</p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span>build_demos_msvc.bat
</pre></div>
</div>
<p>By default, the script automatically detects the highest Microsoft Visual Studio version installed on the machine and uses it to create and build
a solution for a demo code. Optionally, you can also specify the preferred Microsoft Visual Studio version to be used by the script. Supported
version is: <code class="docutils literal notranslate"><span class="pre">VS2019</span></code>. For example, to build the demos using the Microsoft Visual Studio 2019, use the following command:</p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span>build_demos_msvc.bat VS2019
</pre></div>
</div>
<p>By default, the demo applications binaries are build into the <code class="docutils literal notranslate"><span class="pre">C:\Users\&lt;username&gt;\Documents\Intel\OpenVINO\omz_demos_build\intel64\Release</span></code> directory.
The default build folder can be changed with <code class="docutils literal notranslate"><span class="pre">-b</span></code> option. For example, following command will build Open Model Zoo demos into <code class="docutils literal notranslate"><span class="pre">c:\temp\omz-demos-build</span></code> folder:</p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span>build_demos_msvc.bat -b c:\temp\omz-demos-build
</pre></div>
</div>
<p>You can also build a generated solution by yourself, for example, if you want to
build binaries in Debug configuration. Run the appropriate version of the
Microsoft Visual Studio and open the generated solution file from the <code class="docutils literal notranslate"><span class="pre">C:\Users\&lt;username&gt;\Documents\Intel\OpenVINO\omz_demos_build\Demos.sln</span></code>
directory.</p>
<p>You can also build the demo applications using <code class="docutils literal notranslate"><span class="pre">cmake</span> <span class="pre">--build</span></code> tool:</p>
<ol class="arabic simple">
<li><p>Navigate to a directory that you have write access to and create a demos build directory. This example uses a directory named <code class="docutils literal notranslate"><span class="pre">build</span></code>:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">md</span> <span class="n">build</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Go to the created directory:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">build</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Run CMake to generate project files:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cmake</span> <span class="o">-</span><span class="n">A</span> <span class="n">x64</span> <span class="o">&lt;</span><span class="n">open_model_zoo</span><span class="o">&gt;/</span><span class="n">demos</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">cmake</span> <span class="pre">--build</span></code> tool to  build the demos:</p></li>
</ol>
<ul class="simple">
<li><p>For release configuration</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cmake</span> <span class="o">--</span><span class="n">build</span> <span class="o">.</span> <span class="o">--</span><span class="n">config</span> <span class="n">Release</span>
</pre></div>
</div>
<ul class="simple">
<li><p>For debug configuration:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cmake</span> <span class="o">--</span><span class="n">build</span> <span class="o">.</span> <span class="o">--</span><span class="n">config</span> <span class="n">Debug</span>
</pre></div>
</div>
</section>
<section id="a-name-python-requirements-a-dependencies-for-python-demos">
<h3><a name="python_requirements"></a>Dependencies for Python* Demos<a class="headerlink" href="#a-name-python-requirements-a-dependencies-for-python-demos" title="Permalink to this headline">¶</a></h3>
<p>The dependencies for Python demos must be installed before running. It can be achieved with the following command:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python -mpip install --user -r &lt;omz_dir&gt;/demos/requirements.txt
</pre></div>
</div>
</section>
<section id="a-name-python-model-api-a-python-model-api-package">
<h3><a name="python_model_api"></a>Python* model API package<a class="headerlink" href="#a-name-python-model-api-a-python-model-api-package" title="Permalink to this headline">¶</a></h3>
<p>To run Python demo applications, you need to install the Python* Model API package. Refer to the <a class="reference external" href="https://github.com/openvinotoolkit/open_model_zoo/blob/master/demos/common/python/openvino/model_zoo/model_api/README.md#installing-python-model-api-package">Python Model API documentation</a>) to learn about its installation.</p>
</section>
<section id="a-name-build-python-extensions-a-build-the-native-python-extension-modules">
<h3><a name="build_python_extensions"></a>Build the Native Python* Extension Modules<a class="headerlink" href="#a-name-build-python-extensions-a-build-the-native-python-extension-modules" title="Permalink to this headline">¶</a></h3>
<p>Some of the Python demo applications require native Python extension modules to be built before they can be run.
This requires you to have Python development files (headers and import libraries) installed.
To build these modules, follow the instructions for building the demo applications above,
but add <code class="docutils literal notranslate"><span class="pre">-DENABLE_PYTHON=ON</span></code> to either the <code class="docutils literal notranslate"><span class="pre">cmake</span></code> or the <code class="docutils literal notranslate"><span class="pre">build_demos*</span></code> command, depending on which you use.
For example:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>cmake -DCMAKE_BUILD_TYPE<span class="o">=</span>Release -DENABLE_PYTHON<span class="o">=</span>ON &lt;open_model_zoo&gt;/demos
</pre></div>
</div>
<p>Once the modules are built, add the demo build folder to the <code class="docutils literal notranslate"><span class="pre">PYTHONPATH</span></code> environment variable.</p>
</section>
<section id="a-name-build-specific-demos-a-build-specific-demos">
<h3><a name="build_specific_demos"></a>Build Specific Demos<a class="headerlink" href="#a-name-build-specific-demos-a-build-specific-demos" title="Permalink to this headline">¶</a></h3>
<p>To build specific demos, follow the instructions for building the demo applications above,
but add <code class="docutils literal notranslate"><span class="pre">--target</span> <span class="pre">&lt;demo1&gt;</span> <span class="pre">&lt;demo2&gt;</span> <span class="pre">...</span></code> to the <code class="docutils literal notranslate"><span class="pre">cmake</span> <span class="pre">--build</span></code> command or <code class="docutils literal notranslate"><span class="pre">--target=&quot;&lt;demo1&gt;</span> <span class="pre">&lt;demo2&gt;</span> <span class="pre">...&quot;</span></code> to the <code class="docutils literal notranslate"><span class="pre">build_demos*</span></code> command.
Note, <code class="docutils literal notranslate"><span class="pre">cmake</span> <span class="pre">--build</span></code> tool supports multiple targets starting with version 3.15, with lower versions you can specify only one target.</p>
<p>For Linux*:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>cmake -DCMAKE_BUILD_TYPE<span class="o">=</span>Release &lt;open_model_zoo&gt;/demos
cmake --build . --target classification_demo segmentation_demo
</pre></div>
</div>
<p>or</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>build_demos.sh --target<span class="o">=</span><span class="s2">&quot;classification_demo segmentation_demo&quot;</span>
</pre></div>
</div>
<p>For Microsoft Windows* OS:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cmake</span> <span class="o">-</span><span class="n">A</span> <span class="n">x64</span> <span class="o">&lt;</span><span class="n">open_model_zoo</span><span class="o">&gt;/</span><span class="n">demos</span>
<span class="n">cmake</span> <span class="o">--</span><span class="n">build</span> <span class="o">.</span> <span class="o">--</span><span class="n">config</span> <span class="n">Release</span> <span class="o">--</span><span class="n">target</span> <span class="n">classification_demo</span> <span class="n">segmentation_demo</span>
</pre></div>
</div>
<p>or</p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span>build_demos_msvc.bat --target=<span class="s2">&quot;classification_demo segmentation_demo&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="get-ready-for-running-the-demo-applications">
<h2>Get Ready for Running the Demo Applications<a class="headerlink" href="#get-ready-for-running-the-demo-applications" title="Permalink to this headline">¶</a></h2>
<section id="get-ready-for-running-the-demo-applications-on-linux">
<h3>Get Ready for Running the Demo Applications on Linux*<a class="headerlink" href="#get-ready-for-running-the-demo-applications-on-linux" title="Permalink to this headline">¶</a></h3>
<p>Before running compiled binary files, make sure your application can find the OpenVINO™ and OpenCV libraries.
If you use a <a class="reference external" href="https://software.intel.com/en-us/openvino-toolkit">proprietary</a> distribution to build demos,
run the <code class="docutils literal notranslate"><span class="pre">setupvars</span></code> script to set all necessary environment variables:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span> &lt;INSTALL_DIR&gt;/setupvars.sh
</pre></div>
</div>
<p>If you use your own OpenVINO™ and OpenCV binaries to build the demos please make sure you have added them
to the <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> environment variable.</p>
<p><strong>(Optional)</strong>: The OpenVINO environment variables are removed when you close the
shell. As an option, you can permanently set the environment variables as follows:</p>
<ol class="arabic simple">
<li><p>Open the <code class="docutils literal notranslate"><span class="pre">.bashrc</span></code> file in <code class="docutils literal notranslate"><span class="pre">&lt;user_home_directory&gt;</span></code>:</p></li>
</ol>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>vi &lt;user_home_directory&gt;/.bashrc
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Add this line to the end of the file:</p></li>
</ol>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span> &lt;INSTALL_DIR&gt;/setupvars.sh
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Save and close the file: press the <strong>Esc</strong> key, type <code class="docutils literal notranslate"><span class="pre">:wq</span></code> and press the <strong>Enter</strong> key.</p></li>
<li><p>To test your change, open a new terminal. You will see <code class="docutils literal notranslate"><span class="pre">[setupvars.sh]</span> <span class="pre">OpenVINO</span> <span class="pre">environment</span> <span class="pre">initialized</span></code>.</p></li>
</ol>
<p>To run Python demo applications that require native Python extension modules, you must additionally
set up the <code class="docutils literal notranslate"><span class="pre">PYTHONPATH</span></code> environment variable as follows, where <code class="docutils literal notranslate"><span class="pre">&lt;bin_dir&gt;</span></code> is the directory with
the built demo applications:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">PYTHONPATH</span><span class="o">=</span><span class="s2">&quot;&lt;bin_dir&gt;:</span><span class="nv">$PYTHONPATH</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>You are ready to run the demo applications. To learn about how to run a particular
demo, read the demo documentation by clicking the demo name in the demo
list above.</p>
</section>
<section id="get-ready-for-running-the-demo-applications-on-windows">
<h3>Get Ready for Running the Demo Applications on Windows*<a class="headerlink" href="#get-ready-for-running-the-demo-applications-on-windows" title="Permalink to this headline">¶</a></h3>
<p>Before running compiled binary files, make sure your application can find the OpenVINO™ and OpenCV libraries.
Optionally, download the OpenCV community FFmpeg plugin using the downloader script in the OpenVINO package: <code class="docutils literal notranslate"><span class="pre">&lt;INSTALL_DIR&gt;\extras\opencv\ffmpeg-download.ps1</span></code>.
If you use the <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html">Intel® Distribution of OpenVINO™ toolkit</a> distribution to build demos,
run the <code class="docutils literal notranslate"><span class="pre">setupvars</span></code> script to set all necessary environment variables:</p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="p">&lt;</span>INSTALL_DIR<span class="p">&gt;</span>\setupvars.bat
</pre></div>
</div>
<p>If you use your own OpenVINO™ and OpenCV binaries to build the demos please make sure you have added
to the <code class="docutils literal notranslate"><span class="pre">PATH</span></code> environment variable.</p>
<p>To run Python demo applications that require native Python extension modules, you must additionally
set up the <code class="docutils literal notranslate"><span class="pre">PYTHONPATH</span></code> environment variable as follows, where <code class="docutils literal notranslate"><span class="pre">&lt;bin_dir&gt;</span></code> is the directory with
the built demo applications:</p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">set</span> <span class="nv">PYTHONPATH</span><span class="p">=&lt;</span>bin_dir<span class="p">&gt;</span>;<span class="nv">%PYTHONPATH%</span>
</pre></div>
</div>
<p>To debug or run the demos on Windows in Microsoft Visual Studio, make sure you
have properly configured <strong>Debugging</strong> environment settings for the <strong>Debug</strong>
and <strong>Release</strong> configurations. Set correct paths to the OpenCV libraries, and
debug and release versions of the OpenVINO™ libraries.
For example, for the <strong>Debug</strong> configuration, go to the project’s
<strong>Configuration Properties</strong> to the <strong>Debugging</strong> category and set the <code class="docutils literal notranslate"><span class="pre">PATH</span></code>
variable in the <strong>Environment</strong> field to the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PATH</span><span class="o">=&lt;</span><span class="n">INSTALL_DIR</span><span class="o">&gt;</span>\<span class="n">runtime</span>\<span class="nb">bin</span>\<span class="n">intel64</span>\<span class="n">Debug</span><span class="p">;</span><span class="o">&lt;</span><span class="n">INSTALL_DIR</span><span class="o">&gt;</span>\<span class="n">extras</span>\<span class="n">opencv</span>\<span class="nb">bin</span><span class="p">;</span><span class="o">%</span><span class="n">PATH</span><span class="o">%</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">&lt;INSTALL_DIR&gt;</span></code> is the directory in which the OpenVINO toolkit is installed.</p>
<p>You are ready to run the demo applications. To learn about how to run a particular
demo, read the demo documentation by clicking the demo name in the demos
list above.</p>
</section>
</section>
<section id="see-also">
<h2>See Also<a class="headerlink" href="#see-also" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.openvino.ai/latest/documentation.html">Intel OpenVINO Documentation</a></p></li>
<li><p><a class="reference internal" href="../models/intel/index.html"><span class="doc std std-doc">Overview of OpenVINO™ Toolkit Intel’s Pre-Trained Models</span></a></p></li>
<li><p><a class="reference internal" href="../models/public/index.html"><span class="doc std std-doc">Overview of OpenVINO™ Toolkit Public Pre-Trained Models</span></a></p></li>
</ul>
<hr class="docutils" />
<p>* Other names and brands may be claimed as the property of others.</p>
<div class="toctree-wrapper compound">
</div>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="../models/public/yolox-tiny/README.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="3d_segmentation_demo/python/README.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>