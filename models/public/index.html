
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Overview of OpenVINO™ Toolkit Public Pre-Trained Models &#8212; OpenVINO™  documentation</title>
    
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/tabs.css" type="text/css" />
    <script src="../../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/js/custom.js"></script>
    <script src="../../_static/js/graphs.js"></script>
    <script src="../../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/models/public/index.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Sphereface" href="Sphereface/README.html" />
    <link rel="prev" title="yolo-v2-tiny-vehicle-detection-0001" href="../intel/yolo-v2-tiny-vehicle-detection-0001/README.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../index.html">
  <img src="../../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../pages/documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../pages/resources.html">
  Resources
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/latest/models/public/index.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/cn/latest/models/public/index.html">Chinese</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Pre-Trained Models
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../intel/index.html">
   Overview of OpenVINO™ Toolkit Intel’s Pre-Trained Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/action-recognition-0001/README.html">
     action-recognition-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/age-gender-recognition-retail-0013/README.html">
     age-gender-recognition-retail-0013
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/asl-recognition-0004/README.html">
     asl-recognition-0004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/bert-large-uncased-whole-word-masking-squad-0001/README.html">
     bert-large-uncased-whole-word-masking-squad-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/bert-large-uncased-whole-word-masking-squad-emb-0001/README.html">
     bert-large-uncased-whole-word-masking-squad-emb-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/bert-large-uncased-whole-word-masking-squad-int8-0001/README.html">
     bert-large-uncased-whole-word-masking-squad-int8-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/bert-small-uncased-whole-word-masking-squad-0001/README.html">
     bert-small-uncased-whole-word-masking-squad-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/bert-small-uncased-whole-word-masking-squad-0002/README.html">
     bert-small-uncased-whole-word-masking-squad-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/bert-small-uncased-whole-word-masking-squad-emb-int8-0001/README.html">
     bert-small-uncased-whole-word-masking-squad-emb-int8-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/bert-small-uncased-whole-word-masking-squad-int8-0002/README.html">
     bert-small-uncased-whole-word-masking-squad-int8-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/common-sign-language-0002/README.html">
     common-sign-language-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/device_support.html">
     Intel’s Pre-Trained Models Device Support
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/driver-action-recognition-adas-0002/README.html">
     driver-action-recognition-adas-0002 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/emotions-recognition-retail-0003/README.html">
     emotions-recognition-retail-0003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/face-detection-0200/README.html">
     face-detection-0200
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/face-detection-0202/README.html">
     face-detection-0202
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/face-detection-0204/README.html">
     face-detection-0204
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/face-detection-0205/README.html">
     face-detection-0205
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/face-detection-0206/README.html">
     face-detection-0206
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/face-detection-adas-0001/README.html">
     face-detection-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/face-detection-retail-0004/README.html">
     face-detection-retail-0004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/face-detection-retail-0005/README.html">
     face-detection-retail-0005
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/face-reidentification-retail-0095/README.html">
     face-reidentification-retail-0095
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/facial-landmarks-35-adas-0002/README.html">
     facial-landmarks-35-adas-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/facial-landmarks-98-detection-0001/README.html">
     facial-landmarks-98-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/faster-rcnn-resnet101-coco-sparse-60-0001/README.html">
     faster-rcnn-resnet101-coco-sparse-60-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/formula-recognition-medium-scan-0001/README.html">
     formula-recognition-medium-scan-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/formula-recognition-polynomials-handwritten-0001/README.html">
     formula-recognition-polynomials-handwritten-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/gaze-estimation-adas-0002/README.html">
     gaze-estimation-adas-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/handwritten-english-recognition-0001/README.html">
     handwritten-english-recognition-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/handwritten-japanese-recognition-0001/README.html">
     handwritten-japanese-recognition-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/handwritten-score-recognition-0003/README.html">
     handwritten-score-recognition-0003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/handwritten-simplified-chinese-recognition-0001/README.html">
     handwritten-simplified-chinese-recognition-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/head-pose-estimation-adas-0001/README.html">
     head-pose-estimation-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/horizontal-text-detection-0001/README.html">
     horizontal-text-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/human-pose-estimation-0001/README.html">
     human-pose-estimation-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/human-pose-estimation-0005/README.html">
     human-pose-estimation-0005
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/human-pose-estimation-0006/README.html">
     human-pose-estimation-0006
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/human-pose-estimation-0007/README.html">
     human-pose-estimation-0007
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/icnet-camvid-ava-0001/README.html">
     icnet-camvid-ava-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/icnet-camvid-ava-sparse-30-0001/README.html">
     icnet-camvid-ava-sparse-30-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/icnet-camvid-ava-sparse-60-0001/README.html">
     icnet-camvid-ava-sparse-60-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/image-retrieval-0001/README.html">
     image-retrieval-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/instance-segmentation-person-0007/README.html">
     instance-segmentation-person-0007
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/instance-segmentation-security-0002/README.html">
     instance-segmentation-security-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/instance-segmentation-security-0091/README.html">
     instance-segmentation-security-0091
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/instance-segmentation-security-0228/README.html">
     instance-segmentation-security-0228
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/instance-segmentation-security-1039/README.html">
     instance-segmentation-security-1039
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/instance-segmentation-security-1040/README.html">
     instance-segmentation-security-1040
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/landmarks-regression-retail-0009/README.html">
     landmarks-regression-retail-0009
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/license-plate-recognition-barrier-0001/README.html">
     license-plate-recognition-barrier-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/machine-translation-nar-de-en-0002/README.html">
     machine-translation-nar-de-en-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/machine-translation-nar-en-de-0002/README.html">
     machine-translation-nar-en-de-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/machine-translation-nar-en-ru-0002/README.html">
     machine-translation-nar-en-ru-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/machine-translation-nar-ru-en-0002/README.html">
     machine-translation-nar-ru-en-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/noise-suppression-denseunet-ll-0001/README.html">
     noise-suppression-denseunet-ll-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/noise-suppression-poconetlike-0001/README.html">
     noise-suppression-poconetlike-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/pedestrian-and-vehicle-detector-adas-0001/README.html">
     pedestrian-and-vehicle-detector-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/pedestrian-detection-adas-0002/README.html">
     pedestrian-detection-adas-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-attributes-recognition-crossroad-0230/README.html">
     person-attributes-recognition-crossroad-0230
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-attributes-recognition-crossroad-0234/README.html">
     person-attributes-recognition-crossroad-0234
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-attributes-recognition-crossroad-0238/README.html">
     person-attributes-recognition-crossroad-0238
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-0106/README.html">
     person-detection-0106
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-0200/README.html">
     person-detection-0200
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-0201/README.html">
     person-detection-0201
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-0202/README.html">
     person-detection-0202
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-0203/README.html">
     person-detection-0203
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-0301/README.html">
     person-detection-0301
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-0302/README.html">
     person-detection-0302
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-0303/README.html">
     person-detection-0303
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-action-recognition-0005/README.html">
     person-detection-action-recognition-0005
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-action-recognition-0006/README.html">
     person-detection-action-recognition-0006
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-action-recognition-teacher-0002/README.html">
     person-detection-action-recognition-teacher-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-asl-0001/README.html">
     person-detection-asl-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-raisinghand-recognition-0001/README.html">
     person-detection-raisinghand-recognition-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-retail-0002/README.html">
     person-detection-retail-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-detection-retail-0013/README.html">
     person-detection-retail-0013
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-reidentification-retail-0277/README.html">
     person-reidentification-retail-0277
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-reidentification-retail-0286/README.html">
     person-reidentification-retail-0286
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-reidentification-retail-0287/README.html">
     person-reidentification-retail-0287
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-reidentification-retail-0288/README.html">
     person-reidentification-retail-0288
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-vehicle-bike-detection-2000/README.html">
     person-vehicle-bike-detection-2000
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-vehicle-bike-detection-2001/README.html">
     person-vehicle-bike-detection-2001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-vehicle-bike-detection-2002/README.html">
     person-vehicle-bike-detection-2002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-vehicle-bike-detection-2003/README.html">
     person-vehicle-bike-detection-2003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-vehicle-bike-detection-2004/README.html">
     person-vehicle-bike-detection-2004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-vehicle-bike-detection-crossroad-0078/README.html">
     person-vehicle-bike-detection-crossroad-0078
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-vehicle-bike-detection-crossroad-1016/README.html">
     person-vehicle-bike-detection-crossroad-1016
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/person-vehicle-bike-detection-crossroad-yolov3-1020/README.html">
     person-vehicle-bike-detection-crossroad-yolov3-1020
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/product-detection-0001/README.html">
     product-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/resnet18-xnor-binary-onnx-0001/README.html">
     resnet18-xnor-binary-onnx-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/resnet50-binary-0001/README.html">
     resnet50-binary-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/road-segmentation-adas-0001/README.html">
     road-segmentation-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/semantic-segmentation-adas-0001/README.html">
     semantic-segmentation-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/single-image-super-resolution-1032/README.html">
     single-image-super-resolution-1032
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/single-image-super-resolution-1033/README.html">
     single-image-super-resolution-1033
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/smartlab-object-detection-0001/README.html">
     smartlab-object-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/smartlab-object-detection-0002/README.html">
     smartlab-object-detection-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/smartlab-object-detection-0003/README.html">
     smartlab-object-detection-0003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/smartlab-object-detection-0004/README.html">
     smartlab-object-detection-0004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/smartlab-sequence-modelling-0001/README.html">
     smartlab-sequence-modelling-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/text-detection-0003/README.html">
     text-detection-0003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/text-detection-0004/README.html">
     text-detection-0004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/text-image-super-resolution-0001/README.html">
     text-image-super-resolution-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/text-recognition-0012/README.html">
     text-recognition-0012
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/text-recognition-0014/README.html">
     text-recognition-0014
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/text-recognition-0015/README.html">
     text-recognition-0015 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/text-recognition-0016/README.html">
     text-recognition-0016 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/text-spotting-0005/README.html">
     text-spotting-0005 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/text-to-speech-en-0001/README.html">
     text-to-speech-en-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/text-to-speech-en-multi-0001/README.html">
     text-to-speech-en-multi-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/time-series-forecasting-electricity-0001/README.html">
     time-series-forecasting-electricity-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/unet-camvid-onnx-0001/README.html">
     unet-camvid-onnx-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/vehicle-attributes-recognition-barrier-0039/README.html">
     vehicle-attributes-recognition-barrier-0039
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/vehicle-attributes-recognition-barrier-0042/README.html">
     vehicle-attributes-recognition-barrier-0042
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/vehicle-detection-0200/README.html">
     vehicle-detection-0200
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/vehicle-detection-0201/README.html">
     vehicle-detection-0201
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/vehicle-detection-0202/README.html">
     vehicle-detection-0202
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/vehicle-detection-adas-0002/README.html">
     vehicle-detection-adas-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/vehicle-license-plate-detection-barrier-0106/README.html">
     vehicle-license-plate-detection-barrier-0106
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/weld-porosity-detection-0001/README.html">
     weld-porosity-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/yolo-v2-ava-0001/README.html">
     yolo-v2-ava-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/yolo-v2-ava-sparse-35-0001/README.html">
     yolo-v2-ava-sparse-35-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/yolo-v2-ava-sparse-70-0001/README.html">
     yolo-v2-ava-sparse-70-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/yolo-v2-tiny-ava-0001/README.html">
     yolo-v2-tiny-ava-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/yolo-v2-tiny-ava-sparse-30-0001/README.html">
     yolo-v2-tiny-ava-sparse-30-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/yolo-v2-tiny-ava-sparse-60-0001/README.html">
     yolo-v2-tiny-ava-sparse-60-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../intel/yolo-v2-tiny-vehicle-detection-0001/README.html">
     yolo-v2-tiny-vehicle-detection-0001
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="current reference internal" href="#">
   Overview of OpenVINO™ Toolkit Public Pre-Trained Models
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Sphereface/README.html">
     Sphereface
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="aclnet/README.html">
     aclnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="aclnet-int8/README.html">
     aclnet-int8
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="alexnet/README.html">
     alexnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="anti-spoof-mn3/README.html">
     anti-spoof-mn3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="background-matting-mobilenetv2/README.html">
     background-matting-mobilenetv2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bert-base-ner/README.html">
     bert-base-ner
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="brain-tumor-segmentation-0001/README.html">
     brain-tumor-segmentation-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="brain-tumor-segmentation-0002/README.html">
     brain-tumor-segmentation-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="caffenet/README.html">
     caffenet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="cocosnet/README.html">
     cocosnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="colorization-siggraph/README.html">
     colorization-siggraph
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="colorization-v2/README.html">
     colorization-v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="common-sign-language-0001/README.html">
     common-sign-language-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="convnext-tiny/README.html">
     convnext-tiny
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ctdet_coco_dlav0_512/README.html">
     ctdet_coco_dlav0_512
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ctpn/README.html">
     ctpn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="deblurgan-v2/README.html">
     deblurgan-v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="deeplabv3/README.html">
     deeplabv3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="densenet-121/README.html">
     densenet-121
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="densenet-121-tf/README.html">
     densenet-121-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="detr-resnet50/README.html">
     detr-resnet50
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="device_support.html">
     Public Pre-Trained Models Device Support
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dla-34/README.html">
     dla-34
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="drn-d-38/README.html">
     drn-d-38
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="efficientdet-d0-tf/README.html">
     efficientdet-d0-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="efficientdet-d1-tf/README.html">
     efficientdet-d1-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="efficientnet-b0/README.html">
     efficientnet-b0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="efficientnet-b0-pytorch/README.html">
     efficientnet-b0-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="efficientnet-v2-b0/README.html">
     efficientnet-v2-b0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="efficientnet-v2-s/README.html">
     efficientnet-v2-s
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="f3net/README.html">
     f3net
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="face-detection-retail-0044/README.html">
     face-detection-retail-0044
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="face-recognition-resnet100-arcface-onnx/README.html">
     face-recognition-resnet100-arcface-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="faceboxes-pytorch/README.html">
     faceboxes-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="facenet-20180408-102900/README.html">
     facenet-20180408-102900
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fast-neural-style-mosaic-onnx/README.html">
     fast-neural-style-mosaic-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="faster_rcnn_inception_resnet_v2_atrous_coco/README.html">
     faster_rcnn_inception_resnet_v2_atrous_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="faster_rcnn_resnet50_coco/README.html">
     faster_rcnn_resnet50_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fastseg-large/README.html">
     fastseg-large
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fastseg-small/README.html">
     fastseg-small
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fbcnn/README.html">
     fbcnn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fcrn-dp-nyu-depth-v2-tf/README.html">
     fcrn-dp-nyu-depth-v2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="forward-tacotron/README.html">
     forward-tacotron (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gmcnn-places2-tf/README.html">
     gmcnn-places2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="googlenet-v1/README.html">
     googlenet-v1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="googlenet-v1-tf/README.html">
     googlenet-v1-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="googlenet-v2/README.html">
     googlenet-v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="googlenet-v2-tf/README.html">
     googlenet-v2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="googlenet-v3/README.html">
     googlenet-v3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="googlenet-v3-pytorch/README.html">
     googlenet-v3-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="googlenet-v4-tf/README.html">
     googlenet-v4-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gpt-2/README.html">
     gpt-2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="hbonet-0.25/README.html">
     hbonet-0.25
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="hbonet-1.0/README.html">
     hbonet-1.0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="higher-hrnet-w32-human-pose-estimation/README.html">
     higher-hrnet-w32-human-pose-estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="hrnet-v2-c1-segmentation/README.html">
     hrnet-v2-c1-segmentation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="human-pose-estimation-3d-0001/README.html">
     human-pose-estimation-3d-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="hybrid-cs-model-mri/README.html">
     hybrid-cs-model-mri
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="i3d-rgb-tf/README.html">
     i3d-rgb-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="inception-resnet-v2-tf/README.html">
     inception-resnet-v2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="levit-128s/README.html">
     levit-128s
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="license-plate-recognition-barrier-0007/README.html">
     license-plate-recognition-barrier-0007
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mask_rcnn_inception_resnet_v2_atrous_coco/README.html">
     mask_rcnn_inception_resnet_v2_atrous_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mask_rcnn_resnet50_atrous_coco/README.html">
     mask_rcnn_resnet50_atrous_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="midasnet/README.html">
     midasnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mixnet-l/README.html">
     mixnet-l
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mobilefacedet-v1-mxnet/README.html">
     mobilefacedet-v1-mxnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mobilenet-ssd/README.html">
     mobilenet-ssd
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mobilenet-v1-0.25-128/README.html">
     mobilenet-v1-0.25-128
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mobilenet-v1-1.0-224/README.html">
     mobilenet-v1-1.0-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mobilenet-v1-1.0-224-tf/README.html">
     mobilenet-v1-1.0-224-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mobilenet-v2/README.html">
     mobilenet-v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mobilenet-v2-1.0-224/README.html">
     mobilenet-v2-1.0-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mobilenet-v2-1.4-224/README.html">
     mobilenet-v2-1.4-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mobilenet-v2-pytorch/README.html">
     mobilenet-v2-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mobilenet-v3-large-1.0-224-paddle/README.html">
     mobilenet-v3-large-1.0-224-paddle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mobilenet-v3-large-1.0-224-tf/README.html">
     mobilenet-v3-large-1.0-224-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mobilenet-v3-small-1.0-224-paddle/README.html">
     mobilenet-v3-small-1.0-224-paddle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mobilenet-v3-small-1.0-224-tf/README.html">
     mobilenet-v3-small-1.0-224-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mobilenet-yolo-v4-syg/README.html">
     mobilenet-yolo-v4-syg
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="modnet-photographic-portrait-matting/README.html">
     modnet-photographic-portrait-matting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="modnet-webcam-portrait-matting/README.html">
     modnet-webcam-portrait-matting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mozilla-deepspeech-0.6.1/README.html">
     mozilla-deepspeech-0.6.1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mozilla-deepspeech-0.8.2/README.html">
     mozilla-deepspeech-0.8.2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mtcnn/README.html">
     mtcnn (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="nanodet-m-1.5x-416/README.html">
     nanodet-m-1.5x-416
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="nanodet-plus-m-1.5x-416/README.html">
     nanodet-plus-m-1.5x-416
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="netvlad-tf/README.html">
     netvlad-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="nfnet-f0/README.html">
     nfnet-f0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ocrnet-hrnet-w48-paddle/README.html">
     ocrnet-hrnet-w48-paddle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="octave-resnet-26-0.25/README.html">
     octave-resnet-26-0.25
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="open-closed-eye-0001/README.html">
     open-closed-eye-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="pelee-coco/README.html">
     pelee-coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="pspnet-pytorch/README.html">
     pspnet-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="quartznet-15x5-en/README.html">
     quartznet-15x5-en
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regnetx-3.2gf/README.html">
     regnetx-3.2gf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="repvgg-a0/README.html">
     repvgg-a0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="repvgg-b1/README.html">
     repvgg-b1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="repvgg-b3/README.html">
     repvgg-b3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="resnest-50-pytorch/README.html">
     resnest-50-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="resnet-18-pytorch/README.html">
     resnet-18-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="resnet-34-pytorch/README.html">
     resnet-34-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="resnet-50-pytorch/README.html">
     resnet-50-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="resnet-50-tf/README.html">
     resnet-50-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="retinaface-resnet50-pytorch/README.html">
     retinaface-resnet50-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="retinanet-tf/README.html">
     retinanet-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rexnet-v1-x1.0/README.html">
     rexnet-v1-x1.0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rfcn-resnet101-coco-tf/README.html">
     rfcn-resnet101-coco-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="robust-video-matting-mobilenetv3/README.html">
     robust-video-matting-mobilenetv3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="se-inception/README.html">
     se-inception
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="se-resnet-50/README.html">
     se-resnet-50
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="se-resnext-50/README.html">
     se-resnext-50
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="shufflenet-v2-x0.5/README.html">
     shufflenet-v2-x0.5
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="shufflenet-v2-x1.0/README.html">
     shufflenet-v2-x1.0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="single-human-pose-estimation-0001/README.html">
     single-human-pose-estimation-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="squeezenet1.0/README.html">
     squeezenet1.0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="squeezenet1.1/README.html">
     squeezenet1.1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ssd-resnet34-1200-onnx/README.html">
     ssd-resnet34-1200-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ssd300/README.html">
     ssd300
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ssd512/README.html">
     ssd512
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ssd_mobilenet_v1_coco/README.html">
     ssd_mobilenet_v1_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ssd_mobilenet_v1_fpn_coco/README.html">
     ssd_mobilenet_v1_fpn_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ssdlite_mobilenet_v2/README.html">
     ssdlite_mobilenet_v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="swin-tiny-patch4-window7-224/README.html">
     swin-tiny-patch4-window7-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="t2t-vit-14/README.html">
     t2t-vit-14
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-recognition-resnet-fc/README.html">
     text-recognition-resnet-fc
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ultra-lightweight-face-detection-rfb-320/README.html">
     ultra-lightweight-face-detection-rfb-320
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ultra-lightweight-face-detection-slim-320/README.html">
     ultra-lightweight-face-detection-slim-320
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="vehicle-license-plate-detection-barrier-0123/README.html">
     vehicle-license-plate-detection-barrier-0123
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="vehicle-reid-0001/README.html">
     vehicle-reid-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="vgg16/README.html">
     vgg16
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="vgg19/README.html">
     vgg19
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="vitstr-small-patch16-224/README.html">
     vitstr-small-patch16-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="wav2vec2-base/README.html">
     wav2vec2-base
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="wavernn/README.html">
     wavernn (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolact-resnet50-fpn-pytorch/README.html">
     yolact-resnet50-fpn-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v1-tiny-tf/README.html">
     yolo-v1-tiny-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v2-tf/README.html">
     yolo-v2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v2-tiny-tf/README.html">
     yolo-v2-tiny-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v3-onnx/README.html">
     yolo-v3-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v3-tf/README.html">
     yolo-v3-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v3-tiny-onnx/README.html">
     yolo-v3-tiny-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v3-tiny-tf/README.html">
     yolo-v3-tiny-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v4-tf/README.html">
     yolo-v4-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v4-tiny-tf/README.html">
     yolo-v4-tiny-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolof/README.html">
     yolof
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolox-tiny/README.html">
     yolox-tiny
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Demo Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../demos/README.html">
   Open Model Zoo Demos
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/3d_segmentation_demo/python/README.html">
     3D Segmentation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/action_recognition_demo/python/README.html">
     Action Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/background_subtraction_demo/cpp_gapi/README.html">
     G-API Background Subtraction Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/background_subtraction_demo/python/README.html">
     Background subtraction Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/bert_named_entity_recognition_demo/python/README.html">
     BERT Named Entity Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/bert_question_answering_demo/python/README.html">
     BERT Question Answering Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/bert_question_answering_embedding_demo/python/README.html">
     BERT Question Answering Embedding Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/classification_benchmark_demo/cpp/README.html">
     Classification Benchmark C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/classification_demo/python/README.html">
     Classification Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/colorization_demo/python/README.html">
     Colorization Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/crossroad_camera_demo/cpp/README.html">
     Crossroad Camera C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/deblurring_demo/python/README.html">
     Image Deblurring Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/face_detection_mtcnn_demo/cpp_gapi/README.html">
     G-API Face Detection MTCNN Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/face_detection_mtcnn_demo/python/README.html">
     Face Detection MTCNN Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/face_recognition_demo/python/README.html">
     Face Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/formula_recognition_demo/python/README.html">
     Formula Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/gaze_estimation_demo/cpp/README.html">
     Gaze Estimation Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/gaze_estimation_demo/cpp_gapi/README.html">
     G-API Gaze Estimation Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/gesture_recognition_demo/cpp_gapi/README.html">
     G-API Gesture Recognition Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/gesture_recognition_demo/python/README.html">
     Gesture Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/gpt2_text_prediction_demo/python/README.html">
     GPT-2 Text Prediction Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/handwritten_text_recognition_demo/python/README.html">
     Handwritten Text Recognition Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/human_pose_estimation_3d_demo/python/README.html">
     3D Human Pose Estimation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/human_pose_estimation_demo/cpp/README.html">
     Human Pose Estimation C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/human_pose_estimation_demo/python/README.html">
     Human Pose Estimation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/image_inpainting_demo/python/README.html">
     Image Inpainting Python Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/image_processing_demo/cpp/README.html">
     Image Processing C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/image_retrieval_demo/python/README.html">
     Image Retrieval Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/image_translation_demo/python/README.html">
     Image Translation Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/instance_segmentation_demo/python/README.html">
     Instance Segmentation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/interactive_face_detection_demo/cpp/README.html">
     Interactive Face Detection C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/interactive_face_detection_demo/cpp_gapi/README.html">
     G-API Interactive Face Detection Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/machine_translation_demo/python/README.html">
     Machine Translation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/mask_rcnn_demo/cpp/README.html">
     TensorFlow* Object Detection Mask R-CNNs Segmentation C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/monodepth_demo/python/README.html">
     MonoDepth Python Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/mri_reconstruction_demo/cpp/README.html">
     MRI Reconstruction C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/mri_reconstruction_demo/python/README.html">
     MRI Reconstruction Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/multi_camera_multi_target_tracking_demo/python/README.html">
     Multi Camera Multi Target Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/multi_channel_face_detection_demo/cpp/README.html">
     Multi-Channel Face Detection C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/multi_channel_human_pose_estimation_demo/cpp/README.html">
     Multi-Channel Human Pose Estimation C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/multi_channel_object_detection_demo_yolov3/cpp/README.html">
     Multi-Channel Object Detection Yolov3 C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/noise_suppression_demo/cpp/README.html">
     Noise Suppression C++* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/noise_suppression_demo/python/README.html">
     Noise Suppression Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/object_detection_demo/cpp/README.html">
     Object Detection C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/object_detection_demo/python/README.html">
     Object Detection Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/pedestrian_tracker_demo/cpp/README.html">
     Pedestrian Tracker C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/place_recognition_demo/python/README.html">
     Place Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/security_barrier_camera_demo/cpp/README.html">
     Security Barrier Camera C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/segmentation_demo/cpp/README.html">
     Image Segmentation C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/segmentation_demo/python/README.html">
     Image Segmentation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/single_human_pose_estimation_demo/python/README.html">
     Single Human Pose Estimation Demo (top-down pipeline)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/smart_classroom_demo/cpp/README.html">
     Smart Classroom C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/smart_classroom_demo/cpp_gapi/README.html">
     Smart Classroom C++ G-API Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/smartlab_demo/python/README.html">
     Smartlab Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/social_distance_demo/cpp/README.html">
     Social Distance C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/sound_classification_demo/python/README.html">
     Sound Classification Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/speech_recognition_deepspeech_demo/python/README.html">
     Speech Recognition DeepSpeech Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/speech_recognition_quartznet_demo/python/README.html">
     Speech Recognition QuartzNet Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/speech_recognition_wav2vec_demo/python/README.html">
     Speech Recognition Wav2Vec Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/text_detection_demo/cpp/README.html">
     Text Detection C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/text_spotting_demo/python/README.html">
     Text Spotting Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/text_to_speech_demo/python/README.html">
     Text-to-speech Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/time_series_forecasting_demo/python/README.html">
     Time Series Forecasting Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/whiteboard_inpainting_demo/python/README.html">
     Whiteboard Inpainting Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/common/python/openvino/model_zoo/model_api/adapters/ovms_adapter.html">
     OpenVINO Model Server Adapter
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Model API
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../demos/common/python/openvino/model_zoo/model_api/adapters/ovms_adapter.html">
   OpenVINO Model Server Adapter
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classification-models">
   Classification Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#segmentation-models">
   Segmentation Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#semantic-segmentation-models">
     Semantic Segmentation Models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#instance-segmentation-models">
     Instance Segmentation Models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-semantic-segmentation-models">
     3D Semantic Segmentation Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#object-detection-models">
   Object Detection Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#face-recognition-models">
   Face Recognition Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#human-pose-estimation-models">
   Human Pose Estimation Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#monocular-depth-estimation-models">
   Monocular Depth Estimation Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-inpainting-models">
   Image Inpainting Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#style-transfer-models">
   Style Transfer Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#action-recognition-models">
   Action Recognition Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#colorization-models">
   Colorization Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sound-classification-models">
   Sound Classification Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#speech-recognition-models">
   Speech Recognition Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-translation-models">
   Image Translation Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optical-character-recognition-models">
   Optical Character Recognition Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#place-recognition-models">
   Place Recognition Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deblurring-models">
   Deblurring Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#jpeg-artifacts-removal-models">
   JPEG Artifacts Removal Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#salient-object-detection-models">
   Salient Object Detection Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#text-prediction-models">
   Text Prediction Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#text-recognition-models">
   Text Recognition Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#text-to-speech-models">
   Text to Speech Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#named-entity-recognition-models">
   Named Entity Recognition Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vehicle-reidentification-models">
   Vehicle Reidentification Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background-matting-models">
   Background Matting Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#see-also">
   See Also
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#legal-information">
   Legal Information
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <section id="overview-of-openvino-toolkit-public-pre-trained-models">
<h1>Overview of OpenVINO™ Toolkit Public Pre-Trained Models<a class="headerlink" href="#overview-of-openvino-toolkit-public-pre-trained-models" title="Permalink to this headline">¶</a></h1>
<!--
@sphinxdirective

.. toctree::
   :maxdepth: 1
   :hidden:
   :caption: Device Support

   omz_models_public_device_support
   omz_models_model_aclnet
   omz_models_model_aclnet_int8
   omz_models_model_alexnet
   omz_models_model_anti_spoof_mn3
   omz_models_model_background_matting_mobilenetv2
   omz_models_model_bert_base_ner
   omz_models_model_brain_tumor_segmentation_0001
   omz_models_model_brain_tumor_segmentation_0002
   omz_models_model_caffenet
   omz_models_model_cocosnet
   omz_models_model_colorization_siggraph
   omz_models_model_colorization_v2
   omz_models_model_common_sign_language_0001
   omz_models_model_convnext_tiny
   omz_models_model_ctdet_coco_dlav0_512
   omz_models_model_ctpn
   omz_models_model_deblurgan_v2
   omz_models_model_deeplabv3
   omz_models_model_densenet_121
   omz_models_model_densenet_121_tf
   omz_models_model_detr_resnet50
   omz_models_model_dla_34
   omz_models_model_drn_d_38
   omz_models_model_efficientdet_d0_tf
   omz_models_model_efficientdet_d1_tf
   omz_models_model_efficientnet_b0
   omz_models_model_efficientnet_b0_pytorch
   omz_models_model_efficientnet_v2_b0
   omz_models_model_efficientnet_v2_s
   omz_models_model_f3net
   omz_models_model_face_detection_retail_0044
   omz_models_model_face_recognition_resnet100_arcface_onnx
   omz_models_model_faceboxes_pytorch
   omz_models_model_facenet_20180408_102900
   omz_models_model_fast_neural_style_mosaic_onnx
   omz_models_model_faster_rcnn_inception_resnet_v2_atrous_coco
   omz_models_model_faster_rcnn_resnet50_coco
   omz_models_model_fastseg_large
   omz_models_model_fastseg_small
   omz_models_model_fbcnn
   omz_models_model_fcrn_dp_nyu_depth_v2_tf
   omz_models_model_forward_tacotron
   omz_models_model_gmcnn_places2_tf
   omz_models_model_googlenet_v1
   omz_models_model_googlenet_v1_tf
   omz_models_model_googlenet_v2
   omz_models_model_googlenet_v2_tf
   omz_models_model_googlenet_v3
   omz_models_model_googlenet_v3_pytorch
   omz_models_model_googlenet_v4_tf
   omz_models_model_gpt_2
   omz_models_model_hbonet_0_25
   omz_models_model_hbonet_1_0
   omz_models_model_higher_hrnet_w32_human_pose_estimation
   omz_models_model_hrnet_v2_c1_segmentation
   omz_models_model_human_pose_estimation_3d_0001
   omz_models_model_hybrid_cs_model_mri
   omz_models_model_i3d_rgb_tf
   omz_models_model_inception_resnet_v2_tf
   omz_models_model_levit_128s
   omz_models_model_license_plate_recognition_barrier_0007
   omz_models_model_mask_rcnn_inception_resnet_v2_atrous_coco
   omz_models_model_mask_rcnn_resnet50_atrous_coco
   omz_models_model_midasnet
   omz_models_model_mixnet_l
   omz_models_model_mobilefacedet_v1_mxnet
   omz_models_model_mobilenet_ssd
   omz_models_model_mobilenet_v1_0_25_128
   omz_models_model_mobilenet_v1_1_0_224
   omz_models_model_mobilenet_v1_1_0_224_tf
   omz_models_model_mobilenet_v2
   omz_models_model_mobilenet_v2_1_0_224
   omz_models_model_mobilenet_v2_1_4_224
   omz_models_model_mobilenet_v2_pytorch
   omz_models_model_mobilenet_v3_large_1_0_224_paddle
   omz_models_model_mobilenet_v3_large_1_0_224_tf
   omz_models_model_mobilenet_v3_small_1_0_224_paddle
   omz_models_model_mobilenet_v3_small_1_0_224_tf
   omz_models_model_mobilenet_yolo_v4_syg
   omz_models_model_modnet_photographic_portrait_matting
   omz_models_model_modnet_webcam_portrait_matting
   omz_models_model_mozilla_deepspeech_0_6_1
   omz_models_model_mozilla_deepspeech_0_8_2
   omz_models_model_mtcnn
   omz_models_model_nanodet_m_1_5x_416
   omz_models_model_nanodet_plus_m_1_5x_416
   omz_models_model_netvlad_tf
   omz_models_model_nfnet_f0
   omz_models_model_ocrnet_hrnet_w48_paddle
   omz_models_model_octave_resnet_26_0_25
   omz_models_model_open_closed_eye_0001
   omz_models_model_pelee_coco
   omz_models_model_pspnet_pytorch
   omz_models_model_quartznet_15x5_en
   omz_models_model_regnetx_3_2gf
   omz_models_model_repvgg_a0
   omz_models_model_repvgg_b1
   omz_models_model_repvgg_b3
   omz_models_model_resnest_50_pytorch
   omz_models_model_resnet_18_pytorch
   omz_models_model_resnet_34_pytorch
   omz_models_model_resnet_50_pytorch
   omz_models_model_resnet_50_tf
   omz_models_model_retinaface_resnet50_pytorch
   omz_models_model_retinanet_tf
   omz_models_model_rexnet_v1_x1_0
   omz_models_model_rfcn_resnet101_coco_tf
   omz_models_model_robust_video_matting_mobilenetv3
   omz_models_model_se_inception
   omz_models_model_se_resnet_50
   omz_models_model_se_resnext_50
   omz_models_model_shufflenet_v2_x0_5
   omz_models_model_shufflenet_v2_x1_0
   omz_models_model_single_human_pose_estimation_0001
   omz_models_model_Sphereface
   omz_models_model_squeezenet1_0
   omz_models_model_squeezenet1_1
   omz_models_model_ssd_mobilenet_v1_coco
   omz_models_model_ssd_mobilenet_v1_fpn_coco
   omz_models_model_ssd_resnet34_1200_onnx
   omz_models_model_ssd300
   omz_models_model_ssd512
   omz_models_model_ssdlite_mobilenet_v2
   omz_models_model_swin_tiny_patch4_window7_224
   omz_models_model_t2t_vit_14
   omz_models_model_text_recognition_resnet_fc
   omz_models_model_ultra_lightweight_face_detection_rfb_320
   omz_models_model_ultra_lightweight_face_detection_slim_320
   omz_models_model_vehicle_license_plate_detection_barrier_0123
   omz_models_model_vehicle_reid_0001
   omz_models_model_vgg16
   omz_models_model_vgg19
   omz_models_model_vitstr_small_patch16_224
   omz_models_model_wav2vec2_base
   omz_models_model_wavernn
   omz_models_model_yolact_resnet50_fpn_pytorch
   omz_models_model_yolo_v1_tiny_tf
   omz_models_model_yolo_v2_tf
   omz_models_model_yolo_v2_tiny_tf
   omz_models_model_yolo_v3_onnx
   omz_models_model_yolo_v3_tf
   omz_models_model_yolo_v3_tiny_onnx
   omz_models_model_yolo_v3_tiny_tf
   omz_models_model_yolo_v4_tf
   omz_models_model_yolo_v4_tiny_tf
   omz_models_model_yolof
   omz_models_model_yolox_tiny


.. raw:: html

   <script>
      window.TABLE_SORT = true;
   </script>

@endsphinxdirective
-->
<p>OpenVINO™ toolkit provides a set of public pre-trained models
that you can use for learning and demo purposes or for developing deep learning
software. Most recent version is available in the <a class="reference external" href="https://github.com/openvinotoolkit/open_model_zoo">repo on Github</a>.
The table <a class="reference internal" href="device_support.html"><span class="doc std std-doc">Public Pre-Trained Models Device Support</span></a> summarizes devices supported by each model.</p>
<p>You can download models and convert them into OpenVINO™ IR format (*.xml + *.bin) using the OpenVINO™ <span class="xref myst">Model Downloader</span> and other automation tools.</p>
<section id="classification-models">
<h2>Classification Models<a class="headerlink" href="#classification-models" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>AlexNet</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="alexnet/README.html"><span class="doc std std-doc">alexnet</span></a></p></td>
<td><p>56.598%/79.812%</p></td>
<td><p>1.5</p></td>
<td><p>60.965</p></td>
</tr>
<tr class="row-odd"><td><p>AntiSpoofNet</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="anti-spoof-mn3/README.html"><span class="doc std std-doc">anti-spoof-mn3</span></a></p></td>
<td><p>3.81%</p></td>
<td><p>0.15</p></td>
<td><p>3.02</p></td>
</tr>
<tr class="row-even"><td><p>CaffeNet</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="caffenet/README.html"><span class="doc std std-doc">caffenet</span></a></p></td>
<td><p>56.714%/79.916%</p></td>
<td><p>1.5</p></td>
<td><p>60.965</p></td>
</tr>
<tr class="row-odd"><td><p>ConvNeXt Tiny</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="convnext-tiny/README.html"><span class="doc std std-doc">convnext-tiny</span></a></p></td>
<td><p>82.05%/95.86%</p></td>
<td><p>8.9419</p></td>
<td><p>28.5892</p></td>
</tr>
<tr class="row-even"><td><p>DenseNet 121</p></td>
<td><p>Caffe*<br>TensorFlow*</p></td>
<td><p><a class="reference internal" href="densenet-121/README.html"><span class="doc std std-doc">densenet-121</span></a><br><a class="reference internal" href="densenet-121-tf/README.html"><span class="doc std std-doc">densenet-121-tf</span></a></p></td>
<td><p>74.42%/92.136%<br>74.46%/92.13%</p></td>
<td><p>5.723~5.7287</p></td>
<td><p>7.971</p></td>
</tr>
<tr class="row-odd"><td><p>DLA 34</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="dla-34/README.html"><span class="doc std std-doc">dla-34</span></a></p></td>
<td><p>74.64%/92.06%</p></td>
<td><p>6.1368</p></td>
<td><p>15.7344</p></td>
</tr>
<tr class="row-even"><td><p>EfficientNet B0</p></td>
<td><p>TensorFlow*<br>PyTorch*</p></td>
<td><p><a class="reference internal" href="efficientnet-b0/README.html"><span class="doc std std-doc">efficientnet-b0</span></a><br><a class="reference internal" href="efficientnet-b0-pytorch/README.html"><span class="doc std std-doc">efficientnet-b0-pytorch</span></a></p></td>
<td><p>75.70%/92.76%<br>77.70%/93.52%</p></td>
<td><p>0.819</p></td>
<td><p>5.268</p></td>
</tr>
<tr class="row-odd"><td><p>EfficientNet V2 B0</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="efficientnet-v2-b0/README.html"><span class="doc std std-doc">efficientnet-v2-b0</span></a></p></td>
<td><p>78.36%/94.02%</p></td>
<td><p>1.4641</p></td>
<td><p>7.1094</p></td>
</tr>
<tr class="row-even"><td><p>EfficientNet V2 Small</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="efficientnet-v2-s/README.html"><span class="doc std std-doc">efficientnet-v2-s</span></a></p></td>
<td><p>84.29%/97.26%</p></td>
<td><p>16.9406</p></td>
<td><p>21.3816</p></td>
</tr>
<tr class="row-odd"><td><p>HBONet 1.0</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="hbonet-1.0/README.html"><span class="doc std std-doc">hbonet-1.0</span></a></p></td>
<td><p>73.1%/91.0%</p></td>
<td><p>0.6208</p></td>
<td><p>4.5443</p></td>
</tr>
<tr class="row-even"><td><p>HBONet 0.25</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="hbonet-0.25/README.html"><span class="doc std std-doc">hbonet-0.25</span></a></p></td>
<td><p>57.3%/79.8%</p></td>
<td><p>0.0758</p></td>
<td><p>1.9299</p></td>
</tr>
<tr class="row-odd"><td><p>Inception (GoogleNet) V1</p></td>
<td><p>Caffe*<br>TensorFlow*</p></td>
<td><p><a class="reference internal" href="googlenet-v1/README.html"><span class="doc std std-doc">googlenet-v1</span></a><br><a class="reference internal" href="googlenet-v1-tf/README.html"><span class="doc std std-doc">googlenet-v1-tf</span></a></p></td>
<td><p>68.928%/89.144%<br>69.814%/89.6%</p></td>
<td><p>3.016~3.266</p></td>
<td><p>6.619~6.999</p></td>
</tr>
<tr class="row-even"><td><p>Inception (GoogleNet) V2</p></td>
<td><p>Caffe*<br>TensorFlow*</p></td>
<td><p><a class="reference internal" href="googlenet-v2/README.html"><span class="doc std std-doc">googlenet-v2</span></a><br><a class="reference internal" href="googlenet-v2-tf/README.html"><span class="doc std std-doc">googlenet-v2-tf</span></a></p></td>
<td><p>72.024%/90.844%<br>74.084%/91.798%</p></td>
<td><p>4.058</p></td>
<td><p>11.185</p></td>
</tr>
<tr class="row-odd"><td><p>Inception (GoogleNet) V3</p></td>
<td><p>TensorFlow*<br>PyTorch*</p></td>
<td><p><a class="reference internal" href="googlenet-v3/README.html"><span class="doc std std-doc">googlenet-v3</span></a> <br> <a class="reference internal" href="googlenet-v3-pytorch/README.html"><span class="doc std std-doc">googlenet-v3-pytorch</span></a></p></td>
<td><p>77.904%/93.808%<br>77.69%/93.7%</p></td>
<td><p>11.469</p></td>
<td><p>23.817</p></td>
</tr>
<tr class="row-even"><td><p>Inception (GoogleNet) V4</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="googlenet-v4-tf/README.html"><span class="doc std std-doc">googlenet-v4-tf</span></a></p></td>
<td><p>80.204%/95.21%</p></td>
<td><p>24.584</p></td>
<td><p>42.648</p></td>
</tr>
<tr class="row-odd"><td><p>Inception-ResNet V2</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="inception-resnet-v2-tf/README.html"><span class="doc std std-doc">inception-resnet-v2-tf</span></a></p></td>
<td><p>77.82%/94.03%</p></td>
<td><p>22.227</p></td>
<td><p>30.223</p></td>
</tr>
<tr class="row-even"><td><p>LeViT 128S</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="levit-128s/README.html"><span class="doc std std-doc">levit-128s</span></a></p></td>
<td><p>76.54%/92.85%</p></td>
<td><p>0.6177</p></td>
<td><p>8.2199</p></td>
</tr>
<tr class="row-odd"><td><p>MixNet L</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="mixnet-l/README.html"><span class="doc std std-doc">mixnet-l</span></a></p></td>
<td><p>78.30%/93.91%</p></td>
<td><p>0.565</p></td>
<td><p>7.3</p></td>
</tr>
<tr class="row-even"><td><p>MobileNet V1 0.25 128</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="mobilenet-v1-0.25-128/README.html"><span class="doc std std-doc">mobilenet-v1-0.25-128</span></a></p></td>
<td><p>40.54%/65%</p></td>
<td><p>0.028</p></td>
<td><p>0.468</p></td>
</tr>
<tr class="row-odd"><td><p>MobileNet V1 1.0 224</p></td>
<td><p>Caffe*<br>TensorFlow*</p></td>
<td><p><a class="reference internal" href="mobilenet-v1-1.0-224/README.html"><span class="doc std std-doc">mobilenet-v1-1.0-224</span></a><br><a class="reference internal" href="mobilenet-v1-1.0-224-tf/README.html"><span class="doc std std-doc">mobilenet-v1-1.0-224-tf</span></a></p></td>
<td><p>69.496%/89.224%<br>71.03%/89.94%</p></td>
<td><p>1.148</p></td>
<td><p>4.221</p></td>
</tr>
<tr class="row-even"><td><p>MobileNet V2 1.0 224</p></td>
<td><p>Caffe*<br>TensorFlow*<br>PyTorch*</p></td>
<td><p><a class="reference internal" href="mobilenet-v2/README.html"><span class="doc std std-doc">mobilenet-v2</span></a> <br><a class="reference internal" href="mobilenet-v2-1.0-224/README.html"><span class="doc std std-doc">mobilenet-v2-1.0-224</span></a><br><a class="reference internal" href="mobilenet-v2-pytorch/README.html"><span class="doc std std-doc">mobilenet-v2-pytorch</span></a></p></td>
<td><p>71.218%/90.178%<br>71.85%/90.69%<br>71.81%/90.396%</p></td>
<td><p>0.615~0.876</p></td>
<td><p>3.489</p></td>
</tr>
<tr class="row-odd"><td><p>MobileNet V2 1.4 224</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="mobilenet-v2-1.4-224/README.html"><span class="doc std std-doc">mobilenet-v2-1.4-224</span></a></p></td>
<td><p>74.09%/91.97%</p></td>
<td><p>1.183</p></td>
<td><p>6.087</p></td>
</tr>
<tr class="row-even"><td><p>MobileNet V3 Small 1.0</p></td>
<td><p>TensorFlow*<br>Paddle*</p></td>
<td><p><a class="reference internal" href="mobilenet-v3-small-1.0-224-tf/README.html"><span class="doc std std-doc">mobilenet-v3-small-1.0-224-tf</span></a> <br> <a class="reference internal" href="mobilenet-v3-small-1.0-224-paddle/README.html"><span class="doc std std-doc">mobilenet-v3-small-1.0-paddle</span></a></p></td>
<td><p>67.36%/87.44%<br>68.21%/88.04%</p></td>
<td><p>0.1168<br>0.1269</p></td>
<td><p>2.537<br>2.9339</p></td>
</tr>
<tr class="row-odd"><td><p>MobileNet V3 Large 1.0</p></td>
<td><p>TensorFlow*<br>Paddle*</p></td>
<td><p><a class="reference internal" href="mobilenet-v3-large-1.0-224-tf/README.html"><span class="doc std std-doc">mobilenet-v3-large-1.0-224-tf</span></a><br><a class="reference internal" href="mobilenet-v3-large-1.0-224-paddle/README.html"><span class="doc std std-doc">mobilenet-v3-large-1.0-224-paddle</span></a></p></td>
<td><p>75.30%/92.62%<br>75.248%/92.32%</p></td>
<td><p>0.4450<br>0.4565</p></td>
<td><p>5.4721<br>5.468</p></td>
</tr>
<tr class="row-even"><td><p>NFNet F0</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="nfnet-f0/README.html"><span class="doc std std-doc">nfnet-f0</span></a></p></td>
<td><p>83.34%/96.56%</p></td>
<td><p>24.8053</p></td>
<td><p>71.4444</p></td>
</tr>
<tr class="row-odd"><td><p>RegNetX-3.2GF</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="regnetx-3.2gf/README.html"><span class="doc std std-doc">regnetx-3.2gf</span></a></p></td>
<td><p>78.17%/94.08%</p></td>
<td><p>6.3893</p></td>
<td><p>15.2653</p></td>
</tr>
<tr class="row-even"><td><p>ResNet 26, alpha=0.25</p></td>
<td><p>MXNet*</p></td>
<td><p><a class="reference internal" href="octave-resnet-26-0.25/README.html"><span class="doc std std-doc">octave-resnet-26-0.25</span></a></p></td>
<td><p>76.076%/92.584%</p></td>
<td><p>3.768</p></td>
<td><p>15.99</p></td>
</tr>
<tr class="row-odd"><td><p>open-closed-eye-0001</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="open-closed-eye-0001/README.html"><span class="doc std std-doc">open-closed-eye-0001</span></a></p></td>
<td><p>95.84%</p></td>
<td><p>0.0014</p></td>
<td><p>0.0113</p></td>
</tr>
<tr class="row-even"><td><p>RepVGG A0</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="repvgg-a0/README.html"><span class="doc std std-doc">repvgg-a0</span></a></p></td>
<td><p>72.40%/90.49%</p></td>
<td><p>2.7286</p></td>
<td><p>8.3094</p></td>
</tr>
<tr class="row-odd"><td><p>RepVGG B1</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="repvgg-b1/README.html"><span class="doc std std-doc">repvgg-b1</span></a></p></td>
<td><p>78.37%/94.09%</p></td>
<td><p>23.6472</p></td>
<td><p>51.8295</p></td>
</tr>
<tr class="row-even"><td><p>RepVGG B3</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="repvgg-b3/README.html"><span class="doc std std-doc">repvgg-b3</span></a></p></td>
<td><p>80.50%/95.25%</p></td>
<td><p>52.4407</p></td>
<td><p>110.9609</p></td>
</tr>
<tr class="row-odd"><td><p>ResNeSt 50</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="resnest-50-pytorch/README.html"><span class="doc std std-doc">resnest-50-pytorch</span></a></p></td>
<td><p>81.11%/95.36%</p></td>
<td><p>10.8148</p></td>
<td><p>27.4493</p></td>
</tr>
<tr class="row-even"><td><p>ResNet 18</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="resnet-18-pytorch/README.html"><span class="doc std std-doc">resnet-18-pytorch</span></a></p></td>
<td><p>69.754%/89.088%</p></td>
<td><p>3.637</p></td>
<td><p>11.68</p></td>
</tr>
<tr class="row-odd"><td><p>ResNet 34</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="resnet-34-pytorch/README.html"><span class="doc std std-doc">resnet-34-pytorch</span></a></p></td>
<td><p>73.30%/91.42%</p></td>
<td><p>7.3409</p></td>
<td><p>21.7892</p></td>
</tr>
<tr class="row-even"><td><p>ResNet 50</p></td>
<td><p>PyTorch*<br>TensorFlow*</p></td>
<td><p><a class="reference internal" href="resnet-50-pytorch/README.html"><span class="doc std std-doc">resnet-50-pytorch</span></a><a class="reference internal" href="resnet-50-tf/README.html"><span class="doc std std-doc">resnet-50-tf</span></a></p></td>
<td><p>75.168%/92.212%<br>76.38%/93.188%<br>76.17%/92.98%</p></td>
<td><p>6.996~8.216</p></td>
<td><p>25.53</p></td>
</tr>
<tr class="row-odd"><td><p>ReXNet V1 x1.0</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="rexnet-v1-x1.0/README.html"><span class="doc std std-doc">rexnet-v1-x1.0</span></a></p></td>
<td><p>77.86%/93.87%</p></td>
<td><p>0.8325</p></td>
<td><p>4.7779</p></td>
</tr>
<tr class="row-even"><td><p>SE-Inception</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="se-inception/README.html"><span class="doc std std-doc">se-inception</span></a></p></td>
<td><p>75.996%/92.964%</p></td>
<td><p>4.091</p></td>
<td><p>11.922</p></td>
</tr>
<tr class="row-odd"><td><p>SE-ResNet 50</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="se-resnet-50/README.html"><span class="doc std std-doc">se-resnet-50</span></a></p></td>
<td><p>77.596%/93.85%</p></td>
<td><p>7.775</p></td>
<td><p>28.061</p></td>
</tr>
<tr class="row-even"><td><p>SE-ResNeXt 50</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="se-resnext-50/README.html"><span class="doc std std-doc">se-resnext-50</span></a></p></td>
<td><p>78.968%/94.63%</p></td>
<td><p>8.533</p></td>
<td><p>27.526</p></td>
</tr>
<tr class="row-odd"><td><p>Shufflenet V2 x0.5</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="shufflenet-v2-x0.5/README.html"><span class="doc std std-doc">shufflenet-v2-x0.5</span></a></p></td>
<td><p>58.80%/81.13%</p></td>
<td><p>0.08465</p></td>
<td><p>1.363</p></td>
</tr>
<tr class="row-even"><td><p>Shufflenet V2 x1.0</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="shufflenet-v2-x1.0/README.html"><span class="doc std std-doc">shufflenet-v2-x1.0</span></a></p></td>
<td><p>69.36%/88.32%</p></td>
<td><p>0.2957</p></td>
<td><p>2.2705</p></td>
</tr>
<tr class="row-odd"><td><p>SqueezeNet v1.0</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="squeezenet1.0/README.html"><span class="doc std std-doc">squeezenet1.0</span></a></p></td>
<td><p>57.684%/80.38%</p></td>
<td><p>1.737</p></td>
<td><p>1.248</p></td>
</tr>
<tr class="row-even"><td><p>SqueezeNet v1.1</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="squeezenet1.1/README.html"><span class="doc std std-doc">squeezenet1.1</span></a></p></td>
<td><p>58.382%/81%</p></td>
<td><p>0.785</p></td>
<td><p>1.236</p></td>
</tr>
<tr class="row-odd"><td><p>Swin Transformer Tiny, window size=7</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="swin-tiny-patch4-window7-224/README.html"><span class="doc std std-doc">swin-tiny-patch4-window7-224</span></a></p></td>
<td><p>81.38%/95.51%</p></td>
<td><p>9.0280</p></td>
<td><p>28.8173</p></td>
</tr>
<tr class="row-even"><td><p>T2T-ViT, transformer layers number=14</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="t2t-vit-14/README.html"><span class="doc std std-doc">t2t-vit-14</span></a></p></td>
<td><p>81.44%/95.66%</p></td>
<td><p>9.5451</p></td>
<td><p>21.5498</p></td>
</tr>
<tr class="row-odd"><td><p>VGG 16</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="vgg16/README.html"><span class="doc std std-doc">vgg16</span></a></p></td>
<td><p>70.968%/89.878%</p></td>
<td><p>30.974</p></td>
<td><p>138.358</p></td>
</tr>
<tr class="row-even"><td><p>VGG 19</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="vgg19/README.html"><span class="doc std std-doc">vgg19</span></a></p></td>
<td><p>71.062%/89.832%</p></td>
<td><p>39.3</p></td>
<td><p>143.667</p></td>
</tr>
</tbody>
</table>
</section>
<section id="segmentation-models">
<h2>Segmentation Models<a class="headerlink" href="#segmentation-models" title="Permalink to this headline">¶</a></h2>
<p>Semantic segmentation is an extension of object detection problem. Instead of
returning bounding boxes, semantic segmentation models return a “painted”
version of the input image, where the “color” of each pixel represents a certain
class. These networks are much bigger than respective object detection networks,
but they provide a better (pixel-level) localization of objects and they can
detect areas with complex shape.</p>
<section id="semantic-segmentation-models">
<h3>Semantic Segmentation Models<a class="headerlink" href="#semantic-segmentation-models" title="Permalink to this headline">¶</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>DeepLab V3</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="deeplabv3/README.html"><span class="doc std std-doc">deeplabv3</span></a></p></td>
<td><p>68.41%</p></td>
<td><p>11.469</p></td>
<td><p>23.819</p></td>
</tr>
<tr class="row-odd"><td><p>DRN-D-38</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="drn-d-38/README.html"><span class="doc std std-doc">drn-d-38</span></a></p></td>
<td><p>71.31%</p></td>
<td><p>1768.3276</p></td>
<td><p>25.9939</p></td>
</tr>
<tr class="row-even"><td><p>HRNet V2 C1 Segmentation</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="hrnet-v2-c1-segmentation/README.html"><span class="doc std std-doc">hrnet-v2-c1-segmentation</span></a></p></td>
<td><p>77.69%</p></td>
<td><p>81.993</p></td>
<td><p>66.4768</p></td>
</tr>
<tr class="row-odd"><td><p>Fastseg MobileV3Large LR-ASPP, F=128</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="fastseg-large/README.html"><span class="doc std std-doc">fastseg-large</span></a></p></td>
<td><p>72.67%</p></td>
<td><p>140.9611</p></td>
<td><p>3.2</p></td>
</tr>
<tr class="row-even"><td><p>Fastseg MobileV3Small LR-ASPP, F=128</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="fastseg-small/README.html"><span class="doc std std-doc">fastseg-small</span></a></p></td>
<td><p>67.15%</p></td>
<td><p>69.2204</p></td>
<td><p>1.1</p></td>
</tr>
<tr class="row-odd"><td><p>PSPNet R-50-D8</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="pspnet-pytorch/README.html"><span class="doc std std-doc">pspnet-pytorch</span></a></p></td>
<td><p>70.6%</p></td>
<td><p>357.1719</p></td>
<td><p>46.5827</p></td>
</tr>
<tr class="row-even"><td><p>OCRNet HRNet_w48</p></td>
<td><p>Paddle*</p></td>
<td><p><a class="reference internal" href="ocrnet-hrnet-w48-paddle/README.html"><span class="doc std std-doc">ocrnet-hrnet-w48-paddle</span></a></p></td>
<td><p>82.15%</p></td>
<td><p>324.66</p></td>
<td><p>70.47</p></td>
</tr>
</tbody>
</table>
</section>
<section id="instance-segmentation-models">
<h3>Instance Segmentation Models<a class="headerlink" href="#instance-segmentation-models" title="Permalink to this headline">¶</a></h3>
<p>Instance segmentation is an extension of object detection and semantic
segmentation problems. Instead of predicting a bounding box around each object
instance instance segmentation model outputs pixel-wise masks for all instances.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Mask R-CNN Inception ResNet V2</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="mask_rcnn_inception_resnet_v2_atrous_coco/README.html"><span class="doc std std-doc">mask_rcnn_inception_resnet_v2_atrous_coco</span></a></p></td>
<td><p>39.86%/35.36%</p></td>
<td><p>675.314</p></td>
<td><p>92.368</p></td>
</tr>
<tr class="row-odd"><td><p>Mask R-CNN ResNet 50</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="mask_rcnn_resnet50_atrous_coco/README.html"><span class="doc std std-doc">mask_rcnn_resnet50_atrous_coco</span></a></p></td>
<td><p>29.75%/27.46%</p></td>
<td><p>294.738</p></td>
<td><p>50.222</p></td>
</tr>
<tr class="row-even"><td><p>YOLACT ResNet 50 FPN</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="yolact-resnet50-fpn-pytorch/README.html"><span class="doc std std-doc">yolact-resnet50-fpn-pytorch</span></a></p></td>
<td><p>28.0%/30.69%</p></td>
<td><p>118.575</p></td>
<td><p>36.829</p></td>
</tr>
</tbody>
</table>
</section>
<section id="d-semantic-segmentation-models">
<h3>3D Semantic Segmentation Models<a class="headerlink" href="#d-semantic-segmentation-models" title="Permalink to this headline">¶</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Brain Tumor Segmentation</p></td>
<td><p>MXNet*</p></td>
<td><p><a class="reference internal" href="brain-tumor-segmentation-0001/README.html"><span class="doc std std-doc">brain-tumor-segmentation-0001</span></a></p></td>
<td><p>92.4003%</p></td>
<td><p>409.996</p></td>
<td><p>38.192</p></td>
</tr>
<tr class="row-odd"><td><p>Brain Tumor Segmentation 2</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="brain-tumor-segmentation-0002/README.html"><span class="doc std std-doc">brain-tumor-segmentation-0002</span></a></p></td>
<td><p>91.4826%</p></td>
<td><p>300.801</p></td>
<td><p>4.51</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="object-detection-models">
<h2>Object Detection Models<a class="headerlink" href="#object-detection-models" title="Permalink to this headline">¶</a></h2>
<p>Several detection models can be used to detect a set of the most popular
objects - for example, faces, people, vehicles. Most of the networks are
SSD-based and provide reasonable accuracy/performance trade-offs.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CTPN</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="ctpn/README.html"><span class="doc std std-doc">ctpn</span></a></p></td>
<td><p>73.67%</p></td>
<td><p>55.813</p></td>
<td><p>17.237</p></td>
</tr>
<tr class="row-odd"><td><p>CenterNet (CTDET with DLAV0) 512x512</p></td>
<td><p>ONNX*</p></td>
<td><p><a class="reference internal" href="ctdet_coco_dlav0_512/README.html"><span class="doc std std-doc">ctdet_coco_dlav0_512</span></a></p></td>
<td><p>44.2756%</p></td>
<td><p>62.211</p></td>
<td><p>17.911</p></td>
</tr>
<tr class="row-even"><td><p>DETR-ResNet50</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="detr-resnet50/README.html"><span class="doc std std-doc">detr-resnet50</span></a></p></td>
<td><p>39.27% / 42.36%</p></td>
<td><p>174.4708</p></td>
<td><p>41.3293</p></td>
</tr>
<tr class="row-odd"><td><p>EfficientDet-D0</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="efficientdet-d0-tf/README.html"><span class="doc std std-doc">efficientdet-d0-tf</span></a></p></td>
<td><p>31.95%</p></td>
<td><p>2.54</p></td>
<td><p>3.9</p></td>
</tr>
<tr class="row-even"><td><p>EfficientDet-D1</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="efficientdet-d1-tf/README.html"><span class="doc std std-doc">efficientdet-d1-tf</span></a></p></td>
<td><p>37.54%</p></td>
<td><p>6.1</p></td>
<td><p>6.6</p></td>
</tr>
<tr class="row-odd"><td><p>FaceBoxes</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="faceboxes-pytorch/README.html"><span class="doc std std-doc">faceboxes-pytorch</span></a></p></td>
<td><p>83.565%</p></td>
<td><p>1.8975</p></td>
<td><p>1.0059</p></td>
</tr>
<tr class="row-even"><td><p>Face Detection Retail</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="face-detection-retail-0044/README.html"><span class="doc std std-doc">face-detection-retail-0044</span></a></p></td>
<td><p>83.00%</p></td>
<td><p>1.067</p></td>
<td><p>0.588</p></td>
</tr>
<tr class="row-odd"><td><p>Faster R-CNN with Inception-ResNet v2</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="faster_rcnn_inception_resnet_v2_atrous_coco/README.html"><span class="doc std std-doc">faster_rcnn_inception_resnet_v2_atrous_coco</span></a></p></td>
<td><p>40.69%</p></td>
<td><p>30.687</p></td>
<td><p>13.307</p></td>
</tr>
<tr class="row-even"><td><p>Faster R-CNN with ResNet 50</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="faster_rcnn_resnet50_coco/README.html"><span class="doc std std-doc">faster_rcnn_resnet50_coco</span></a></p></td>
<td><p>31.09%</p></td>
<td><p>57.203</p></td>
<td><p>29.162</p></td>
</tr>
<tr class="row-odd"><td><p>MobileFace Detection V1</p></td>
<td><p>MXNet*</p></td>
<td><p><a class="reference internal" href="mobilefacedet-v1-mxnet/README.html"><span class="doc std std-doc">mobilefacedet-v1-mxnet</span></a></p></td>
<td><p>78.7488%</p></td>
<td><p>3.5456</p></td>
<td><p>7.6828</p></td>
</tr>
<tr class="row-even"><td><p>Mobilenet-yolo-v4-syg</p></td>
<td><p>Keras*</p></td>
<td><p><a class="reference internal" href="mobilenet-yolo-v4-syg/README.html"><span class="doc std std-doc">mobilenet-yolo-v4-syg</span></a></p></td>
<td><p>86.35%</p></td>
<td><p>65.981</p></td>
<td><p>61.922</p></td>
</tr>
<tr class="row-odd"><td><p>MTCNN</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="mtcnn/README.html"><span class="doc std std-doc">mtcnn</span></a>:<br>mtcnn-p <br>mtcnn-r <br>mtcnn-o</p></td>
<td><p>48.1308%/62.2625%</p></td>
<td><p><br>3.3715<br>0.0031<br>0.0263</p></td>
<td><p><br>0.0066<br>0.1002<br>0.3890</p></td>
</tr>
<tr class="row-even"><td><p>NanoDet with ShuffleNetV2 1.5x, size=416</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="nanodet-m-1.5x-416/README.html"><span class="doc std std-doc">nanodet-m-1.5x-416</span></a></p></td>
<td><p>27.38%/26.63%</p></td>
<td><p>2.3895</p></td>
<td><p>2.0534</p></td>
</tr>
<tr class="row-odd"><td><p>NanoDet Plus with ShuffleNetV2 1.5x, size=416</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="nanodet-plus-m-1.5x-416/README.html"><span class="doc std std-doc">nanodet-plus-m-1.5x-416</span></a></p></td>
<td><p>34.53%/33.77%</p></td>
<td><p>3.0147</p></td>
<td><p>2.4614</p></td>
</tr>
<tr class="row-even"><td><p>Pelee</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="pelee-coco/README.html"><span class="doc std std-doc">pelee-coco</span></a></p></td>
<td><p>21.9761%</p></td>
<td><p>1.290</p></td>
<td><p>5.98</p></td>
</tr>
<tr class="row-odd"><td><p>RetinaFace with ResNet 50</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="retinaface-resnet50-pytorch/README.html"><span class="doc std std-doc">retinaface-resnet50-pytorch</span></a></p></td>
<td><p>91.78%</p></td>
<td><p>88.8627</p></td>
<td><p>27.2646</p></td>
</tr>
<tr class="row-even"><td><p>RetinaNet with Resnet 50</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="retinanet-tf/README.html"><span class="doc std std-doc">retinanet-tf</span></a></p></td>
<td><p>33.15%</p></td>
<td><p>238.9469</p></td>
<td><p>64.9706</p></td>
</tr>
<tr class="row-odd"><td><p>R-FCN with Resnet-101</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="rfcn-resnet101-coco-tf/README.html"><span class="doc std std-doc">rfcn-resnet101-coco-tf</span></a></p></td>
<td><p>28.40%/45.02%</p></td>
<td><p>53.462</p></td>
<td><p>171.85</p></td>
</tr>
<tr class="row-even"><td><p>SSD 300</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="ssd300/README.html"><span class="doc std std-doc">ssd300</span></a></p></td>
<td><p>87.09%</p></td>
<td><p>62.815</p></td>
<td><p>26.285</p></td>
</tr>
<tr class="row-odd"><td><p>SSD 512</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="ssd512/README.html"><span class="doc std std-doc">ssd512</span></a></p></td>
<td><p>91.07%</p></td>
<td><p>180.611</p></td>
<td><p>27.189</p></td>
</tr>
<tr class="row-even"><td><p>SSD with MobileNet</p></td>
<td><p>Caffe* <br>TensorFlow*</p></td>
<td><p><a class="reference internal" href="mobilenet-ssd/README.html"><span class="doc std std-doc">mobilenet-ssd</span></a> <br><a class="reference internal" href="ssd_mobilenet_v1_coco/README.html"><span class="doc std std-doc">ssd_mobilenet_v1_coco</span></a></p></td>
<td><p>67.00%<br>23.32%</p></td>
<td><p>2.316~2.494</p></td>
<td><p>5.783~6.807</p></td>
</tr>
<tr class="row-odd"><td><p>SSD with MobileNet FPN</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="ssd_mobilenet_v1_fpn_coco/README.html"><span class="doc std std-doc">ssd_mobilenet_v1_fpn_coco</span></a></p></td>
<td><p>35.5453%</p></td>
<td><p>123.309</p></td>
<td><p>36.188</p></td>
</tr>
<tr class="row-even"><td><p>SSD lite with MobileNet V2</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="ssdlite_mobilenet_v2/README.html"><span class="doc std std-doc">ssdlite_mobilenet_v2</span></a></p></td>
<td><p>24.2946%</p></td>
<td><p>1.525</p></td>
<td><p>4.475</p></td>
</tr>
<tr class="row-odd"><td><p>SSD with ResNet 34 1200x1200</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="ssd-resnet34-1200-onnx/README.html"><span class="doc std std-doc">ssd-resnet34-1200-onnx</span></a></p></td>
<td><p>20.7198%/39.2752%</p></td>
<td><p>433.411</p></td>
<td><p>20.058</p></td>
</tr>
<tr class="row-even"><td><p>Ultra Lightweight Face Detection RFB 320</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="ultra-lightweight-face-detection-rfb-320/README.html"><span class="doc std std-doc">ultra-lightweight-face-detection-rfb-320</span></a></p></td>
<td><p>84.78%</p></td>
<td><p>0.2106</p></td>
<td><p>0.3004</p></td>
</tr>
<tr class="row-odd"><td><p>Ultra Lightweight Face Detection slim 320</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="ultra-lightweight-face-detection-slim-320/README.html"><span class="doc std std-doc">ultra-lightweight-face-detection-slim-320</span></a></p></td>
<td><p>83.32%</p></td>
<td><p>0.1724</p></td>
<td><p>0.2844</p></td>
</tr>
<tr class="row-even"><td><p>Vehicle License Plate Detection Barrier</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="vehicle-license-plate-detection-barrier-0123/README.html"><span class="doc std std-doc">vehicle-license-plate-detection-barrier-0123</span></a></p></td>
<td><p>99.52%</p></td>
<td><p>0.271</p></td>
<td><p>0.547</p></td>
</tr>
<tr class="row-odd"><td><p>YOLO v1 Tiny</p></td>
<td><p>TensorFlow.js*</p></td>
<td><p><a class="reference internal" href="yolo-v1-tiny-tf/README.html"><span class="doc std std-doc">yolo-v1-tiny-tf</span></a></p></td>
<td><p>54.79%</p></td>
<td><p>6.9883</p></td>
<td><p>15.8587</p></td>
</tr>
<tr class="row-even"><td><p>YOLO v2 Tiny</p></td>
<td><p>Keras*</p></td>
<td><p><a class="reference internal" href="yolo-v2-tiny-tf/README.html"><span class="doc std std-doc">yolo-v2-tiny-tf</span></a></p></td>
<td><p>27.3443%/29.1184%</p></td>
<td><p>5.4236</p></td>
<td><p>11.2295</p></td>
</tr>
<tr class="row-odd"><td><p>YOLO v2</p></td>
<td><p>Keras*</p></td>
<td><p><a class="reference internal" href="yolo-v2-tf/README.html"><span class="doc std std-doc">yolo-v2-tf</span></a></p></td>
<td><p>53.1453%/56.483%</p></td>
<td><p>63.0301</p></td>
<td><p>50.9526</p></td>
</tr>
<tr class="row-even"><td><p>YOLO v3</p></td>
<td><p>Keras* <br>ONNX*</p></td>
<td><p><a class="reference internal" href="yolo-v3-tf/README.html"><span class="doc std std-doc">yolo-v3-tf</span></a> <br><a class="reference internal" href="yolo-v3-onnx/README.html"><span class="doc std std-doc">yolo-v3-onnx</span></a></p></td>
<td><p>62.2759%/67.7221% <br> 48.30%/47.07%</p></td>
<td><p>65.9843~65.998</p></td>
<td><p>61.9221~61.930</p></td>
</tr>
<tr class="row-odd"><td><p>YOLO v3 Tiny</p></td>
<td><p>Keras* <br>ONNX*</p></td>
<td><p><a class="reference internal" href="yolo-v3-tiny-tf/README.html"><span class="doc std std-doc">yolo-v3-tiny-tf</span></a> <br><a class="reference internal" href="yolo-v3-tiny-onnx/README.html"><span class="doc std std-doc">yolo-v3-tiny-onnx</span></a></p></td>
<td><p>35.9%/39.7% <br> 17.07%/13.64%</p></td>
<td><p>5.582</p></td>
<td><p>8.848~8.8509</p></td>
</tr>
<tr class="row-even"><td><p>YOLO v4</p></td>
<td><p>Keras*</p></td>
<td><p><a class="reference internal" href="yolo-v4-tf/README.html"><span class="doc std std-doc">yolo-v4-tf</span></a></p></td>
<td><p>71.23%/77.40%/50.26%</p></td>
<td><p>129.5567</p></td>
<td><p>64.33</p></td>
</tr>
<tr class="row-odd"><td><p>YOLO v4 Tiny</p></td>
<td><p>Keras*</p></td>
<td><p><a class="reference internal" href="yolo-v4-tiny-tf/README.html"><span class="doc std std-doc">yolo-v4-tiny-tf</span></a></p></td>
<td><p></p></td>
<td><p>6.9289</p></td>
<td><p>6.0535</p></td>
</tr>
<tr class="row-even"><td><p>YOLOF</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="yolof/README.html"><span class="doc std std-doc">yolof</span></a></p></td>
<td><p>60.69%/66.23%/43.63%</p></td>
<td><p>175.37942</p></td>
<td><p>48.228</p></td>
</tr>
<tr class="row-odd"><td><p>YOLOX Tiny</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="yolox-tiny/README.html"><span class="doc std std-doc">yolox-tiny</span></a></p></td>
<td><p>47.85%/52.56%/31.82%</p></td>
<td><p>6.4813</p></td>
<td><p>5.0472</p></td>
</tr>
</tbody>
</table>
</section>
<section id="face-recognition-models">
<h2>Face Recognition Models<a class="headerlink" href="#face-recognition-models" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FaceNet</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="facenet-20180408-102900/README.html"><span class="doc std std-doc">facenet-20180408-102900</span></a></p></td>
<td><p>99.14%</p></td>
<td><p>2.846</p></td>
<td><p>23.469</p></td>
</tr>
<tr class="row-odd"><td><p>LResNet100E-IR,ArcFace&#64;ms1m-refine-v2</p></td>
<td><p>MXNet*</p></td>
<td><p><a class="reference internal" href="face-recognition-resnet100-arcface-onnx/README.html"><span class="doc std std-doc">face-recognition-resnet100-arcface-onnx</span></a></p></td>
<td><p>99.68%</p></td>
<td><p>24.2115</p></td>
<td><p>65.1320</p></td>
</tr>
<tr class="row-even"><td><p>SphereFace</p></td>
<td><p>Caffe*</p></td>
<td><p><a class="reference internal" href="Sphereface/README.html"><span class="doc std std-doc">Sphereface</span></a></p></td>
<td><p>98.8321%</p></td>
<td><p>3.504</p></td>
<td><p>22.671</p></td>
</tr>
</tbody>
</table>
</section>
<section id="human-pose-estimation-models">
<h2>Human Pose Estimation Models<a class="headerlink" href="#human-pose-estimation-models" title="Permalink to this headline">¶</a></h2>
<p>Human pose estimation task is to predict a pose: body skeleton, which consists
of keypoints and connections between them, for every person in an input image or
video. Keypoints are body joints, i.e. ears, eyes, nose, shoulders, knees, etc.
There are two major groups of such methods: top-down and bottom-up.  The first
detects persons in a given frame, crops or rescales detections, then runs pose
estimation network for every detection. These methods are very accurate. The
second finds all keypoints in a given frame, then groups them by person
instances, thus faster than previous, because network runs once.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>human-pose-estimation-3d-0001</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="human-pose-estimation-3d-0001/README.html"><span class="doc std std-doc">human-pose-estimation-3d-0001</span></a></p></td>
<td><p>100.44437mm</p></td>
<td><p>18.998</p></td>
<td><p>5.074</p></td>
</tr>
<tr class="row-odd"><td><p>single-human-pose-estimation-0001</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="single-human-pose-estimation-0001/README.html"><span class="doc std std-doc">single-human-pose-estimation-0001</span></a></p></td>
<td><p>69.0491%</p></td>
<td><p>60.125</p></td>
<td><p>33.165</p></td>
</tr>
<tr class="row-even"><td><p>higher-hrnet-w32-human-pose-estimation</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="higher-hrnet-w32-human-pose-estimation/README.html"><span class="doc std std-doc">higher-hrnet-w32-human-pose-estimation</span></a></p></td>
<td><p>64.64%</p></td>
<td><p>92.8364</p></td>
<td><p>28.6180</p></td>
</tr>
</tbody>
</table>
</section>
<section id="monocular-depth-estimation-models">
<h2>Monocular Depth Estimation Models<a class="headerlink" href="#monocular-depth-estimation-models" title="Permalink to this headline">¶</a></h2>
<p>The task of monocular depth estimation is to predict a depth (or inverse depth) map based on a single input image.
Since this task contains - in the general setting - some ambiguity, the resulting depth maps are often only defined up to an unknown scaling factor.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>midasnet</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="midasnet/README.html"><span class="doc std std-doc">midasnet</span></a></p></td>
<td><p>0.07071</p></td>
<td><p>207.25144</p></td>
<td><p>104.081</p></td>
</tr>
<tr class="row-odd"><td><p>FCRN ResNet50-Upproj</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="fcrn-dp-nyu-depth-v2-tf/README.html"><span class="doc std std-doc">fcrn-dp-nyu-depth-v2-tf</span></a></p></td>
<td><p>0.573</p></td>
<td><p>63.5421</p></td>
<td><p>34.5255</p></td>
</tr>
</tbody>
</table>
</section>
<section id="image-inpainting-models">
<h2>Image Inpainting Models<a class="headerlink" href="#image-inpainting-models" title="Permalink to this headline">¶</a></h2>
<p>Image inpainting task is to estimate suitable pixel information to fill holes in images.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>GMCNN Inpainting</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="gmcnn-places2-tf/README.html"><span class="doc std std-doc">gmcnn-places2-tf</span></a></p></td>
<td><p>33.47Db</p></td>
<td><p>691.1589</p></td>
<td><p>12.7773</p></td>
</tr>
<tr class="row-odd"><td><p>Hybrid-CS-Model-MRI</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="hybrid-cs-model-mri/README.html"><span class="doc std std-doc">hybrid-cs-model-mri</span></a></p></td>
<td><p>34.27Db</p></td>
<td><p>146.6037</p></td>
<td><p>11.3313</p></td>
</tr>
</tbody>
</table>
</section>
<section id="style-transfer-models">
<h2>Style Transfer Models<a class="headerlink" href="#style-transfer-models" title="Permalink to this headline">¶</a></h2>
<p>Style transfer task is to transfer the style of one image to another.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>fast-neural-style-mosaic-onnx</p></td>
<td><p>ONNX*</p></td>
<td><p><a class="reference internal" href="fast-neural-style-mosaic-onnx/README.html"><span class="doc std std-doc">fast-neural-style-mosaic-onnx</span></a></p></td>
<td><p>12.04dB</p></td>
<td><p>15.518</p></td>
<td><p>1.679</p></td>
</tr>
</tbody>
</table>
</section>
<section id="action-recognition-models">
<h2>Action Recognition Models<a class="headerlink" href="#action-recognition-models" title="Permalink to this headline">¶</a></h2>
<p>The task of action recognition is to predict action that is being performed on a short video clip
(tensor formed by stacking sampled frames from input video).</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>RGB-I3D, pretrained on ImageNet*</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="i3d-rgb-tf/README.html"><span class="doc std std-doc">i3d-rgb-tf</span></a></p></td>
<td><p>64.83%/84.58%</p></td>
<td><p>278.9815</p></td>
<td><p>12.6900</p></td>
</tr>
<tr class="row-odd"><td><p>common-sign-language-0001</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="common-sign-language-0001/README.html"><span class="doc std std-doc">common-sign-language-0001</span></a></p></td>
<td><p>93.58%</p></td>
<td><p>4.2269</p></td>
<td><p>4.1128</p></td>
</tr>
</tbody>
</table>
</section>
<section id="colorization-models">
<h2>Colorization Models<a class="headerlink" href="#colorization-models" title="Permalink to this headline">¶</a></h2>
<p>Colorization task is to predict colors of scene from grayscale image.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>colorization-v2</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="colorization-v2/README.html"><span class="doc std std-doc">colorization-v2</span></a></p></td>
<td><p>26.99dB</p></td>
<td><p>83.6045</p></td>
<td><p>32.2360</p></td>
</tr>
<tr class="row-odd"><td><p>colorization-siggraph</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="colorization-siggraph/README.html"><span class="doc std std-doc">colorization-siggraph</span></a></p></td>
<td><p>27.73dB</p></td>
<td><p>150.5441</p></td>
<td><p>34.0511</p></td>
</tr>
</tbody>
</table>
</section>
<section id="sound-classification-models">
<h2>Sound Classification Models<a class="headerlink" href="#sound-classification-models" title="Permalink to this headline">¶</a></h2>
<p>The task of sound classification is to predict what sounds are in an audio fragment.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ACLNet</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="aclnet/README.html"><span class="doc std std-doc">aclnet</span></a></p></td>
<td><p>86%/92%</p></td>
<td><p>1.4</p></td>
<td><p>2.7</p></td>
</tr>
<tr class="row-odd"><td><p>ACLNet-int8</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="aclnet-int8/README.html"><span class="doc std std-doc">aclnet-int8</span></a></p></td>
<td><p>87%/93%</p></td>
<td><p>1.41</p></td>
<td><p>2.71</p></td>
</tr>
</tbody>
</table>
</section>
<section id="speech-recognition-models">
<h2>Speech Recognition Models<a class="headerlink" href="#speech-recognition-models" title="Permalink to this headline">¶</a></h2>
<p>The task of speech recognition is to recognize and translate spoken language into text.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>DeepSpeech V0.6.1</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="mozilla-deepspeech-0.6.1/README.html"><span class="doc std std-doc">mozilla-deepspeech-0.6.1</span></a></p></td>
<td><p>7.55%</p></td>
<td><p>0.0472</p></td>
<td><p>47.2</p></td>
</tr>
<tr class="row-odd"><td><p>DeepSpeech V0.8.2</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="mozilla-deepspeech-0.8.2/README.html"><span class="doc std std-doc">mozilla-deepspeech-0.8.2</span></a></p></td>
<td><p>6.13%</p></td>
<td><p>0.0472</p></td>
<td><p>47.2</p></td>
</tr>
<tr class="row-even"><td><p>QuartzNet</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="quartznet-15x5-en/README.html"><span class="doc std std-doc">quartznet-15x5-en</span></a></p></td>
<td><p>3.86%</p></td>
<td><p>2.4195</p></td>
<td><p>18.8857</p></td>
</tr>
<tr class="row-odd"><td><p>Wav2Vec 2.0 Base</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="wav2vec2-base/README.html"><span class="doc std std-doc">wav2vec2-base</span></a></p></td>
<td><p>3.39%</p></td>
<td><p>26.843</p></td>
<td><p>94.3965</p></td>
</tr>
</tbody>
</table>
</section>
<section id="image-translation-models">
<h2>Image Translation Models<a class="headerlink" href="#image-translation-models" title="Permalink to this headline">¶</a></h2>
<p>The task of image translation is to generate the output based on exemplar.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CoCosNet</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="cocosnet/README.html"><span class="doc std std-doc">cocosnet</span></a></p></td>
<td><p>12.93dB</p></td>
<td><p>1080.7032</p></td>
<td><p>167.9141</p></td>
</tr>
</tbody>
</table>
</section>
<section id="optical-character-recognition-models">
<h2>Optical Character Recognition Models<a class="headerlink" href="#optical-character-recognition-models" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>license-plate-recognition-barrier-0007</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="license-plate-recognition-barrier-0007/README.html"><span class="doc std std-doc">license-plate-recognition-barrier-0007</span></a></p></td>
<td><p>98%</p></td>
<td><p>0.347</p></td>
<td><p>1.435</p></td>
</tr>
</tbody>
</table>
</section>
<section id="place-recognition-models">
<h2>Place Recognition Models<a class="headerlink" href="#place-recognition-models" title="Permalink to this headline">¶</a></h2>
<p>The task of place recognition is to quickly and accurately recognize the location of a given query photograph.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>NetVLAD</p></td>
<td><p>TensorFlow*</p></td>
<td><p><a class="reference internal" href="netvlad-tf/README.html"><span class="doc std std-doc">netvlad-tf</span></a></p></td>
<td><p>82.0321%</p></td>
<td><p>36.6374</p></td>
<td><p>149.0021</p></td>
</tr>
</tbody>
</table>
</section>
<section id="deblurring-models">
<h2>Deblurring Models<a class="headerlink" href="#deblurring-models" title="Permalink to this headline">¶</a></h2>
<p>The task of image deblurring.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>DeblurGAN-v2</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="deblurgan-v2/README.html"><span class="doc std std-doc">deblurgan-v2</span></a></p></td>
<td><p>28.25Db</p></td>
<td><p>80.8919</p></td>
<td><p>2.1083</p></td>
</tr>
</tbody>
</table>
</section>
<section id="jpeg-artifacts-removal-models">
<h2>JPEG Artifacts Removal Models<a class="headerlink" href="#jpeg-artifacts-removal-models" title="Permalink to this headline">¶</a></h2>
<p>The task of restoration images from jpeg format.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FBCNN</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="fbcnn/README.html"><span class="doc std std-doc">fbcnn</span></a></p></td>
<td><p>34.34Db</p></td>
<td><p>1420.78235</p></td>
<td><p>71.922</p></td>
</tr>
</tbody>
</table>
</section>
<section id="salient-object-detection-models">
<h2>Salient Object Detection Models<a class="headerlink" href="#salient-object-detection-models" title="Permalink to this headline">¶</a></h2>
<p>Salient object detection is a task-based on a visual attention mechanism,
in which algorithms aim to explore objects or regions more attentive than the surrounding areas on the scene or images.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>F3Net</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="f3net/README.html"><span class="doc std std-doc">f3net</span></a></p></td>
<td><p>84.21%</p></td>
<td><p>31.2883</p></td>
<td><p>25.2791</p></td>
</tr>
</tbody>
</table>
</section>
<section id="text-prediction-models">
<h2>Text Prediction Models<a class="headerlink" href="#text-prediction-models" title="Permalink to this headline">¶</a></h2>
<p>Text prediction is a task to predict the next word, given all of the previous words within some text.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>GPT-2</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="gpt-2/README.html"><span class="doc std std-doc">gpt-2</span></a></p></td>
<td><p>29.00%</p></td>
<td><p>293.0489</p></td>
<td><p>175.6203</p></td>
</tr>
</tbody>
</table>
</section>
<section id="text-recognition-models">
<h2>Text Recognition Models<a class="headerlink" href="#text-recognition-models" title="Permalink to this headline">¶</a></h2>
<p>Scene text recognition is a task to recognize text on a given image.
Researchers compete on creating algorithms which are able to recognize text of different shapes, fonts and background.
See details about datasets in <a class="reference internal" href="text-recognition-resnet-fc/README.html"><span class="doc std std-doc">here</span></a>
The reported metric is collected over the alphanumeric subset of ICDAR13 (1015 images) in case-insensitive mode.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Resnet-FC</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="text-recognition-resnet-fc/README.html"><span class="doc std std-doc">text-recognition-resnet-fc</span></a></p></td>
<td><p>90.94%</p></td>
<td><p>40.3704</p></td>
<td><p>177.9668</p></td>
</tr>
<tr class="row-odd"><td><p>ViTSTR Small patch=16, size=224</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="vitstr-small-patch16-224/README.html"><span class="doc std std-doc">vitstr-small-patch16-224</span></a></p></td>
<td><p>90.34%</p></td>
<td><p>9.1544</p></td>
<td><p>21.5061</p></td>
</tr>
</tbody>
</table>
</section>
<section id="text-to-speech-models">
<h2>Text to Speech Models<a class="headerlink" href="#text-to-speech-models" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ForwardTacotron</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="forward-tacotron/README.html"><span class="doc std std-doc">forward-tacotron</span></a>:<br>forward-tacotron-duration-prediction <br>forward-tacotron-regression</p></td>
<td><p></p></td>
<td><p><br>6.66 <br>4.91</p></td>
<td><p><br>13.81 <br>3.05</p></td>
</tr>
<tr class="row-odd"><td><p>WaveRNN</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="wavernn/README.html"><span class="doc std std-doc">wavernn</span></a>:<br>wavernn-upsampler <br>wavernn-rnn</p></td>
<td><p></p></td>
<td><p><br>0.37 <br>0.06</p></td>
<td><p><br>0.4 <br>3.83</p></td>
</tr>
</tbody>
</table>
</section>
<section id="named-entity-recognition-models">
<h2>Named Entity Recognition Models<a class="headerlink" href="#named-entity-recognition-models" title="Permalink to this headline">¶</a></h2>
<p>Named entity recognition (NER) is the task of tagging entities in text with their corresponding type.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>bert-base-NER</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="bert-base-ner/README.html"><span class="doc std std-doc">bert-base-ner</span></a></p></td>
<td><p>94.45%</p></td>
<td><p>22.3874</p></td>
<td><p>107.4319</p></td>
</tr>
</tbody>
</table>
</section>
<section id="vehicle-reidentification-models">
<h2>Vehicle Reidentification Models<a class="headerlink" href="#vehicle-reidentification-models" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>vehicle-reid-0001</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="vehicle-reid-0001/README.html"><span class="doc std std-doc">vehicle-reid-0001</span></a></p></td>
<td><p>96.31%/85.15 %</p></td>
<td><p>2.643</p></td>
<td><p>2.183</p></td>
</tr>
</tbody>
</table>
</section>
<section id="background-matting-models">
<h2>Background Matting Models<a class="headerlink" href="#background-matting-models" title="Permalink to this headline">¶</a></h2>
<p>Background matting is a method of separating a foreground from a background in an image or video,
wherein some pixels may belong to foreground as well as background, such pixels are called partial
or mixed pixels. This distinguishes background matting from segmentation approaches where the result is a binary mask.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Implementation</p></th>
<th class="head"><p>OMZ Model Name</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>GFlops</p></th>
<th class="head"><p>mParams</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>background-matting-mobilenetv2</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="background-matting-mobilenetv2/README.html"><span class="doc std std-doc">background-matting-mobilenetv2</span></a></p></td>
<td><p>4.32/1.0/2.48/2.7</p></td>
<td><p>6.7419</p></td>
<td><p>5.052</p></td>
</tr>
<tr class="row-odd"><td><p>modnet-photographic-portrait-matting</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="modnet-photographic-portrait-matting/README.html"><span class="doc std std-doc">modnet-photographic-portrait-matting</span></a></p></td>
<td><p>5.21/727.95</p></td>
<td><p>31.1564</p></td>
<td><p>6.4597</p></td>
</tr>
<tr class="row-even"><td><p>modnet-webcam-portrait-matting</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="modnet-webcam-portrait-matting/README.html"><span class="doc std std-doc">modnet-webcam-portrait-matting</span></a></p></td>
<td><p>5.66/762.52</p></td>
<td><p>31.1564</p></td>
<td><p>6.4597</p></td>
</tr>
<tr class="row-odd"><td><p>robust-video-matting-mobilenetv3</p></td>
<td><p>PyTorch*</p></td>
<td><p><a class="reference internal" href="robust-video-matting-mobilenetv3/README.html"><span class="doc std std-doc">robust-video-matting-mobilenetv3</span></a></p></td>
<td><p>20.8/15.1/4.42/4.05</p></td>
<td><p>9.3892</p></td>
<td><p>3.7363</p></td>
</tr>
</tbody>
</table>
</section>
<section id="see-also">
<h2>See Also<a class="headerlink" href="#see-also" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="../../demos/README.html"><span class="doc std std-doc">Open Model Zoo Demos</span></a></p></li>
<li><p><span class="xref myst">Model Downloader</span></p></li>
<li><p><a class="reference internal" href="../intel/index.html"><span class="doc std std-doc">Overview of OpenVINO™ Toolkit Intel’s Pre-Trained Models</span></a></p></li>
</ul>
</section>
<section id="legal-information">
<h2>Legal Information<a class="headerlink" href="#legal-information" title="Permalink to this headline">¶</a></h2>
<p>Caffe, Caffe2, Keras, MXNet, PyTorch, and TensorFlow are trademarks or brand names of their respective owners.
All company, product and service names used in this website are for identification purposes only.
Use of these names,trademarks and brands does not imply endorsement.</p>
<div class="toctree-wrapper compound">
</div>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="../intel/yolo-v2-tiny-vehicle-detection-0001/README.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="Sphereface/README.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>