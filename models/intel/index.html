
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Overview of OpenVINO™ Toolkit Intel’s Pre-Trained Models &#8212; OpenVINO™  documentation</title>
    
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/tabs.css" type="text/css" />
    <script src="../../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/js/custom.js"></script>
    <script src="../../_static/js/graphs.js"></script>
    <script src="../../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/models/intel/index.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="action-recognition-0001 (composite)" href="action-recognition-0001/README.html" />
    <link rel="prev" title="Model Zoo" href="../../model_zoo.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../index.html">
  <img src="../../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../pages/documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../pages/resources.html">
  Resources
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/latest/models/intel/index.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/cn/latest/models/intel/index.html">Chinese</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Pre-Trained Models
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="current reference internal" href="#">
   Overview of OpenVINO™ Toolkit Intel’s Pre-Trained Models
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="action-recognition-0001/README.html">
     action-recognition-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="age-gender-recognition-retail-0013/README.html">
     age-gender-recognition-retail-0013
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="asl-recognition-0004/README.html">
     asl-recognition-0004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bert-large-uncased-whole-word-masking-squad-0001/README.html">
     bert-large-uncased-whole-word-masking-squad-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bert-large-uncased-whole-word-masking-squad-emb-0001/README.html">
     bert-large-uncased-whole-word-masking-squad-emb-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bert-large-uncased-whole-word-masking-squad-int8-0001/README.html">
     bert-large-uncased-whole-word-masking-squad-int8-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bert-small-uncased-whole-word-masking-squad-0001/README.html">
     bert-small-uncased-whole-word-masking-squad-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bert-small-uncased-whole-word-masking-squad-0002/README.html">
     bert-small-uncased-whole-word-masking-squad-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bert-small-uncased-whole-word-masking-squad-emb-int8-0001/README.html">
     bert-small-uncased-whole-word-masking-squad-emb-int8-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bert-small-uncased-whole-word-masking-squad-int8-0002/README.html">
     bert-small-uncased-whole-word-masking-squad-int8-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="common-sign-language-0002/README.html">
     common-sign-language-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="device_support.html">
     Intel’s Pre-Trained Models Device Support
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="driver-action-recognition-adas-0002/README.html">
     driver-action-recognition-adas-0002 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="emotions-recognition-retail-0003/README.html">
     emotions-recognition-retail-0003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="face-detection-0200/README.html">
     face-detection-0200
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="face-detection-0202/README.html">
     face-detection-0202
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="face-detection-0204/README.html">
     face-detection-0204
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="face-detection-0205/README.html">
     face-detection-0205
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="face-detection-0206/README.html">
     face-detection-0206
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="face-detection-adas-0001/README.html">
     face-detection-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="face-detection-retail-0004/README.html">
     face-detection-retail-0004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="face-detection-retail-0005/README.html">
     face-detection-retail-0005
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="face-reidentification-retail-0095/README.html">
     face-reidentification-retail-0095
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="facial-landmarks-35-adas-0002/README.html">
     facial-landmarks-35-adas-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="facial-landmarks-98-detection-0001/README.html">
     facial-landmarks-98-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="faster-rcnn-resnet101-coco-sparse-60-0001/README.html">
     faster-rcnn-resnet101-coco-sparse-60-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="formula-recognition-medium-scan-0001/README.html">
     formula-recognition-medium-scan-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="formula-recognition-polynomials-handwritten-0001/README.html">
     formula-recognition-polynomials-handwritten-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gaze-estimation-adas-0002/README.html">
     gaze-estimation-adas-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="handwritten-english-recognition-0001/README.html">
     handwritten-english-recognition-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="handwritten-japanese-recognition-0001/README.html">
     handwritten-japanese-recognition-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="handwritten-score-recognition-0003/README.html">
     handwritten-score-recognition-0003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="handwritten-simplified-chinese-recognition-0001/README.html">
     handwritten-simplified-chinese-recognition-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="head-pose-estimation-adas-0001/README.html">
     head-pose-estimation-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="horizontal-text-detection-0001/README.html">
     horizontal-text-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="human-pose-estimation-0001/README.html">
     human-pose-estimation-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="human-pose-estimation-0005/README.html">
     human-pose-estimation-0005
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="human-pose-estimation-0006/README.html">
     human-pose-estimation-0006
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="human-pose-estimation-0007/README.html">
     human-pose-estimation-0007
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="icnet-camvid-ava-0001/README.html">
     icnet-camvid-ava-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="icnet-camvid-ava-sparse-30-0001/README.html">
     icnet-camvid-ava-sparse-30-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="icnet-camvid-ava-sparse-60-0001/README.html">
     icnet-camvid-ava-sparse-60-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="image-retrieval-0001/README.html">
     image-retrieval-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="instance-segmentation-person-0007/README.html">
     instance-segmentation-person-0007
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="instance-segmentation-security-0002/README.html">
     instance-segmentation-security-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="instance-segmentation-security-0091/README.html">
     instance-segmentation-security-0091
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="instance-segmentation-security-0228/README.html">
     instance-segmentation-security-0228
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="instance-segmentation-security-1039/README.html">
     instance-segmentation-security-1039
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="instance-segmentation-security-1040/README.html">
     instance-segmentation-security-1040
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="landmarks-regression-retail-0009/README.html">
     landmarks-regression-retail-0009
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="license-plate-recognition-barrier-0001/README.html">
     license-plate-recognition-barrier-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="machine-translation-nar-de-en-0002/README.html">
     machine-translation-nar-de-en-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="machine-translation-nar-en-de-0002/README.html">
     machine-translation-nar-en-de-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="machine-translation-nar-en-ru-0002/README.html">
     machine-translation-nar-en-ru-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="machine-translation-nar-ru-en-0002/README.html">
     machine-translation-nar-ru-en-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="noise-suppression-denseunet-ll-0001/README.html">
     noise-suppression-denseunet-ll-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="noise-suppression-poconetlike-0001/README.html">
     noise-suppression-poconetlike-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="pedestrian-and-vehicle-detector-adas-0001/README.html">
     pedestrian-and-vehicle-detector-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="pedestrian-detection-adas-0002/README.html">
     pedestrian-detection-adas-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-attributes-recognition-crossroad-0230/README.html">
     person-attributes-recognition-crossroad-0230
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-attributes-recognition-crossroad-0234/README.html">
     person-attributes-recognition-crossroad-0234
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-attributes-recognition-crossroad-0238/README.html">
     person-attributes-recognition-crossroad-0238
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-0106/README.html">
     person-detection-0106
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-0200/README.html">
     person-detection-0200
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-0201/README.html">
     person-detection-0201
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-0202/README.html">
     person-detection-0202
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-0203/README.html">
     person-detection-0203
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-0301/README.html">
     person-detection-0301
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-0302/README.html">
     person-detection-0302
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-0303/README.html">
     person-detection-0303
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-action-recognition-0005/README.html">
     person-detection-action-recognition-0005
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-action-recognition-0006/README.html">
     person-detection-action-recognition-0006
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-action-recognition-teacher-0002/README.html">
     person-detection-action-recognition-teacher-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-asl-0001/README.html">
     person-detection-asl-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-raisinghand-recognition-0001/README.html">
     person-detection-raisinghand-recognition-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-retail-0002/README.html">
     person-detection-retail-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-detection-retail-0013/README.html">
     person-detection-retail-0013
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-reidentification-retail-0277/README.html">
     person-reidentification-retail-0277
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-reidentification-retail-0286/README.html">
     person-reidentification-retail-0286
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-reidentification-retail-0287/README.html">
     person-reidentification-retail-0287
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-reidentification-retail-0288/README.html">
     person-reidentification-retail-0288
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-vehicle-bike-detection-2000/README.html">
     person-vehicle-bike-detection-2000
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-vehicle-bike-detection-2001/README.html">
     person-vehicle-bike-detection-2001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-vehicle-bike-detection-2002/README.html">
     person-vehicle-bike-detection-2002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-vehicle-bike-detection-2003/README.html">
     person-vehicle-bike-detection-2003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-vehicle-bike-detection-2004/README.html">
     person-vehicle-bike-detection-2004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-vehicle-bike-detection-crossroad-0078/README.html">
     person-vehicle-bike-detection-crossroad-0078
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-vehicle-bike-detection-crossroad-1016/README.html">
     person-vehicle-bike-detection-crossroad-1016
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="person-vehicle-bike-detection-crossroad-yolov3-1020/README.html">
     person-vehicle-bike-detection-crossroad-yolov3-1020
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="product-detection-0001/README.html">
     product-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="resnet18-xnor-binary-onnx-0001/README.html">
     resnet18-xnor-binary-onnx-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="resnet50-binary-0001/README.html">
     resnet50-binary-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="road-segmentation-adas-0001/README.html">
     road-segmentation-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="semantic-segmentation-adas-0001/README.html">
     semantic-segmentation-adas-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="single-image-super-resolution-1032/README.html">
     single-image-super-resolution-1032
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="single-image-super-resolution-1033/README.html">
     single-image-super-resolution-1033
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="smartlab-object-detection-0001/README.html">
     smartlab-object-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="smartlab-object-detection-0002/README.html">
     smartlab-object-detection-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="smartlab-object-detection-0003/README.html">
     smartlab-object-detection-0003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="smartlab-object-detection-0004/README.html">
     smartlab-object-detection-0004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="smartlab-sequence-modelling-0001/README.html">
     smartlab-sequence-modelling-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-detection-0003/README.html">
     text-detection-0003
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-detection-0004/README.html">
     text-detection-0004
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-image-super-resolution-0001/README.html">
     text-image-super-resolution-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-recognition-0012/README.html">
     text-recognition-0012
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-recognition-0014/README.html">
     text-recognition-0014
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-recognition-0015/README.html">
     text-recognition-0015 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-recognition-0016/README.html">
     text-recognition-0016 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-spotting-0005/README.html">
     text-spotting-0005 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-to-speech-en-0001/README.html">
     text-to-speech-en-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-to-speech-en-multi-0001/README.html">
     text-to-speech-en-multi-0001 (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="time-series-forecasting-electricity-0001/README.html">
     time-series-forecasting-electricity-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="unet-camvid-onnx-0001/README.html">
     unet-camvid-onnx-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="vehicle-attributes-recognition-barrier-0039/README.html">
     vehicle-attributes-recognition-barrier-0039
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="vehicle-attributes-recognition-barrier-0042/README.html">
     vehicle-attributes-recognition-barrier-0042
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="vehicle-detection-0200/README.html">
     vehicle-detection-0200
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="vehicle-detection-0201/README.html">
     vehicle-detection-0201
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="vehicle-detection-0202/README.html">
     vehicle-detection-0202
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="vehicle-detection-adas-0002/README.html">
     vehicle-detection-adas-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="vehicle-license-plate-detection-barrier-0106/README.html">
     vehicle-license-plate-detection-barrier-0106
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="weld-porosity-detection-0001/README.html">
     weld-porosity-detection-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v2-ava-0001/README.html">
     yolo-v2-ava-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v2-ava-sparse-35-0001/README.html">
     yolo-v2-ava-sparse-35-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v2-ava-sparse-70-0001/README.html">
     yolo-v2-ava-sparse-70-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v2-tiny-ava-0001/README.html">
     yolo-v2-tiny-ava-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v2-tiny-ava-sparse-30-0001/README.html">
     yolo-v2-tiny-ava-sparse-30-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v2-tiny-ava-sparse-60-0001/README.html">
     yolo-v2-tiny-ava-sparse-60-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="yolo-v2-tiny-vehicle-detection-0001/README.html">
     yolo-v2-tiny-vehicle-detection-0001
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../public/index.html">
   Overview of OpenVINO™ Toolkit Public Pre-Trained Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/Sphereface/README.html">
     Sphereface
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/aclnet/README.html">
     aclnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/aclnet-int8/README.html">
     aclnet-int8
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/alexnet/README.html">
     alexnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/anti-spoof-mn3/README.html">
     anti-spoof-mn3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/background-matting-mobilenetv2/README.html">
     background-matting-mobilenetv2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/bert-base-ner/README.html">
     bert-base-ner
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/brain-tumor-segmentation-0001/README.html">
     brain-tumor-segmentation-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/brain-tumor-segmentation-0002/README.html">
     brain-tumor-segmentation-0002
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/caffenet/README.html">
     caffenet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/cocosnet/README.html">
     cocosnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/colorization-siggraph/README.html">
     colorization-siggraph
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/colorization-v2/README.html">
     colorization-v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/common-sign-language-0001/README.html">
     common-sign-language-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/convnext-tiny/README.html">
     convnext-tiny
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/ctdet_coco_dlav0_512/README.html">
     ctdet_coco_dlav0_512
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/ctpn/README.html">
     ctpn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/deblurgan-v2/README.html">
     deblurgan-v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/deeplabv3/README.html">
     deeplabv3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/densenet-121/README.html">
     densenet-121
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/densenet-121-tf/README.html">
     densenet-121-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/detr-resnet50/README.html">
     detr-resnet50
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/device_support.html">
     Public Pre-Trained Models Device Support
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/dla-34/README.html">
     dla-34
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/drn-d-38/README.html">
     drn-d-38
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/efficientdet-d0-tf/README.html">
     efficientdet-d0-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/efficientdet-d1-tf/README.html">
     efficientdet-d1-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/efficientnet-b0/README.html">
     efficientnet-b0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/efficientnet-b0-pytorch/README.html">
     efficientnet-b0-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/efficientnet-v2-b0/README.html">
     efficientnet-v2-b0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/efficientnet-v2-s/README.html">
     efficientnet-v2-s
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/f3net/README.html">
     f3net
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/face-detection-retail-0044/README.html">
     face-detection-retail-0044
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/face-recognition-resnet100-arcface-onnx/README.html">
     face-recognition-resnet100-arcface-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/faceboxes-pytorch/README.html">
     faceboxes-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/facenet-20180408-102900/README.html">
     facenet-20180408-102900
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/fast-neural-style-mosaic-onnx/README.html">
     fast-neural-style-mosaic-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/faster_rcnn_inception_resnet_v2_atrous_coco/README.html">
     faster_rcnn_inception_resnet_v2_atrous_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/faster_rcnn_resnet50_coco/README.html">
     faster_rcnn_resnet50_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/fastseg-large/README.html">
     fastseg-large
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/fastseg-small/README.html">
     fastseg-small
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/fbcnn/README.html">
     fbcnn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/fcrn-dp-nyu-depth-v2-tf/README.html">
     fcrn-dp-nyu-depth-v2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/forward-tacotron/README.html">
     forward-tacotron (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/gmcnn-places2-tf/README.html">
     gmcnn-places2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/googlenet-v1/README.html">
     googlenet-v1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/googlenet-v1-tf/README.html">
     googlenet-v1-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/googlenet-v2/README.html">
     googlenet-v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/googlenet-v2-tf/README.html">
     googlenet-v2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/googlenet-v3/README.html">
     googlenet-v3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/googlenet-v3-pytorch/README.html">
     googlenet-v3-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/googlenet-v4-tf/README.html">
     googlenet-v4-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/gpt-2/README.html">
     gpt-2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/hbonet-0.25/README.html">
     hbonet-0.25
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/hbonet-1.0/README.html">
     hbonet-1.0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/higher-hrnet-w32-human-pose-estimation/README.html">
     higher-hrnet-w32-human-pose-estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/hrnet-v2-c1-segmentation/README.html">
     hrnet-v2-c1-segmentation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/human-pose-estimation-3d-0001/README.html">
     human-pose-estimation-3d-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/hybrid-cs-model-mri/README.html">
     hybrid-cs-model-mri
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/i3d-rgb-tf/README.html">
     i3d-rgb-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/inception-resnet-v2-tf/README.html">
     inception-resnet-v2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/levit-128s/README.html">
     levit-128s
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/license-plate-recognition-barrier-0007/README.html">
     license-plate-recognition-barrier-0007
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mask_rcnn_inception_resnet_v2_atrous_coco/README.html">
     mask_rcnn_inception_resnet_v2_atrous_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mask_rcnn_resnet50_atrous_coco/README.html">
     mask_rcnn_resnet50_atrous_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/midasnet/README.html">
     midasnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mixnet-l/README.html">
     mixnet-l
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mobilefacedet-v1-mxnet/README.html">
     mobilefacedet-v1-mxnet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mobilenet-ssd/README.html">
     mobilenet-ssd
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mobilenet-v1-0.25-128/README.html">
     mobilenet-v1-0.25-128
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mobilenet-v1-1.0-224/README.html">
     mobilenet-v1-1.0-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mobilenet-v1-1.0-224-tf/README.html">
     mobilenet-v1-1.0-224-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mobilenet-v2/README.html">
     mobilenet-v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mobilenet-v2-1.0-224/README.html">
     mobilenet-v2-1.0-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mobilenet-v2-1.4-224/README.html">
     mobilenet-v2-1.4-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mobilenet-v2-pytorch/README.html">
     mobilenet-v2-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mobilenet-v3-large-1.0-224-paddle/README.html">
     mobilenet-v3-large-1.0-224-paddle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mobilenet-v3-large-1.0-224-tf/README.html">
     mobilenet-v3-large-1.0-224-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mobilenet-v3-small-1.0-224-paddle/README.html">
     mobilenet-v3-small-1.0-224-paddle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mobilenet-v3-small-1.0-224-tf/README.html">
     mobilenet-v3-small-1.0-224-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mobilenet-yolo-v4-syg/README.html">
     mobilenet-yolo-v4-syg
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/modnet-photographic-portrait-matting/README.html">
     modnet-photographic-portrait-matting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/modnet-webcam-portrait-matting/README.html">
     modnet-webcam-portrait-matting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mozilla-deepspeech-0.6.1/README.html">
     mozilla-deepspeech-0.6.1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mozilla-deepspeech-0.8.2/README.html">
     mozilla-deepspeech-0.8.2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/mtcnn/README.html">
     mtcnn (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/nanodet-m-1.5x-416/README.html">
     nanodet-m-1.5x-416
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/nanodet-plus-m-1.5x-416/README.html">
     nanodet-plus-m-1.5x-416
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/netvlad-tf/README.html">
     netvlad-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/nfnet-f0/README.html">
     nfnet-f0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/ocrnet-hrnet-w48-paddle/README.html">
     ocrnet-hrnet-w48-paddle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/octave-resnet-26-0.25/README.html">
     octave-resnet-26-0.25
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/open-closed-eye-0001/README.html">
     open-closed-eye-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/pelee-coco/README.html">
     pelee-coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/pspnet-pytorch/README.html">
     pspnet-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/quartznet-15x5-en/README.html">
     quartznet-15x5-en
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/regnetx-3.2gf/README.html">
     regnetx-3.2gf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/repvgg-a0/README.html">
     repvgg-a0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/repvgg-b1/README.html">
     repvgg-b1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/repvgg-b3/README.html">
     repvgg-b3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/resnest-50-pytorch/README.html">
     resnest-50-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/resnet-18-pytorch/README.html">
     resnet-18-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/resnet-34-pytorch/README.html">
     resnet-34-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/resnet-50-pytorch/README.html">
     resnet-50-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/resnet-50-tf/README.html">
     resnet-50-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/retinaface-resnet50-pytorch/README.html">
     retinaface-resnet50-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/retinanet-tf/README.html">
     retinanet-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/rexnet-v1-x1.0/README.html">
     rexnet-v1-x1.0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/rfcn-resnet101-coco-tf/README.html">
     rfcn-resnet101-coco-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/robust-video-matting-mobilenetv3/README.html">
     robust-video-matting-mobilenetv3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/se-inception/README.html">
     se-inception
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/se-resnet-50/README.html">
     se-resnet-50
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/se-resnext-50/README.html">
     se-resnext-50
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/shufflenet-v2-x0.5/README.html">
     shufflenet-v2-x0.5
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/shufflenet-v2-x1.0/README.html">
     shufflenet-v2-x1.0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/single-human-pose-estimation-0001/README.html">
     single-human-pose-estimation-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/squeezenet1.0/README.html">
     squeezenet1.0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/squeezenet1.1/README.html">
     squeezenet1.1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/ssd-resnet34-1200-onnx/README.html">
     ssd-resnet34-1200-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/ssd300/README.html">
     ssd300
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/ssd512/README.html">
     ssd512
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/ssd_mobilenet_v1_coco/README.html">
     ssd_mobilenet_v1_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/ssd_mobilenet_v1_fpn_coco/README.html">
     ssd_mobilenet_v1_fpn_coco
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/ssdlite_mobilenet_v2/README.html">
     ssdlite_mobilenet_v2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/swin-tiny-patch4-window7-224/README.html">
     swin-tiny-patch4-window7-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/t2t-vit-14/README.html">
     t2t-vit-14
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/text-recognition-resnet-fc/README.html">
     text-recognition-resnet-fc
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/ultra-lightweight-face-detection-rfb-320/README.html">
     ultra-lightweight-face-detection-rfb-320
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/ultra-lightweight-face-detection-slim-320/README.html">
     ultra-lightweight-face-detection-slim-320
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/vehicle-license-plate-detection-barrier-0123/README.html">
     vehicle-license-plate-detection-barrier-0123
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/vehicle-reid-0001/README.html">
     vehicle-reid-0001
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/vgg16/README.html">
     vgg16
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/vgg19/README.html">
     vgg19
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/vitstr-small-patch16-224/README.html">
     vitstr-small-patch16-224
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/wav2vec2-base/README.html">
     wav2vec2-base
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/wavernn/README.html">
     wavernn (composite)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/yolact-resnet50-fpn-pytorch/README.html">
     yolact-resnet50-fpn-pytorch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/yolo-v1-tiny-tf/README.html">
     yolo-v1-tiny-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/yolo-v2-tf/README.html">
     yolo-v2-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/yolo-v2-tiny-tf/README.html">
     yolo-v2-tiny-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/yolo-v3-onnx/README.html">
     yolo-v3-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/yolo-v3-tf/README.html">
     yolo-v3-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/yolo-v3-tiny-onnx/README.html">
     yolo-v3-tiny-onnx
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/yolo-v3-tiny-tf/README.html">
     yolo-v3-tiny-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/yolo-v4-tf/README.html">
     yolo-v4-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/yolo-v4-tiny-tf/README.html">
     yolo-v4-tiny-tf
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/yolof/README.html">
     yolof
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../public/yolox-tiny/README.html">
     yolox-tiny
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Demo Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../demos/README.html">
   Open Model Zoo Demos
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/3d_segmentation_demo/python/README.html">
     3D Segmentation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/action_recognition_demo/python/README.html">
     Action Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/background_subtraction_demo/cpp_gapi/README.html">
     G-API Background Subtraction Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/background_subtraction_demo/python/README.html">
     Background subtraction Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/bert_named_entity_recognition_demo/python/README.html">
     BERT Named Entity Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/bert_question_answering_demo/python/README.html">
     BERT Question Answering Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/bert_question_answering_embedding_demo/python/README.html">
     BERT Question Answering Embedding Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/classification_benchmark_demo/cpp/README.html">
     Classification Benchmark C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/classification_demo/python/README.html">
     Classification Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/colorization_demo/python/README.html">
     Colorization Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/crossroad_camera_demo/cpp/README.html">
     Crossroad Camera C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/deblurring_demo/python/README.html">
     Image Deblurring Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/face_detection_mtcnn_demo/cpp_gapi/README.html">
     G-API Face Detection MTCNN Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/face_detection_mtcnn_demo/python/README.html">
     Face Detection MTCNN Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/face_recognition_demo/python/README.html">
     Face Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/formula_recognition_demo/python/README.html">
     Formula Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/gaze_estimation_demo/cpp/README.html">
     Gaze Estimation Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/gaze_estimation_demo/cpp_gapi/README.html">
     G-API Gaze Estimation Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/gesture_recognition_demo/cpp_gapi/README.html">
     G-API Gesture Recognition Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/gesture_recognition_demo/python/README.html">
     Gesture Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/gpt2_text_prediction_demo/python/README.html">
     GPT-2 Text Prediction Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/handwritten_text_recognition_demo/python/README.html">
     Handwritten Text Recognition Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/human_pose_estimation_3d_demo/python/README.html">
     3D Human Pose Estimation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/human_pose_estimation_demo/cpp/README.html">
     Human Pose Estimation C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/human_pose_estimation_demo/python/README.html">
     Human Pose Estimation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/image_inpainting_demo/python/README.html">
     Image Inpainting Python Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/image_processing_demo/cpp/README.html">
     Image Processing C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/image_retrieval_demo/python/README.html">
     Image Retrieval Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/image_translation_demo/python/README.html">
     Image Translation Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/instance_segmentation_demo/python/README.html">
     Instance Segmentation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/interactive_face_detection_demo/cpp/README.html">
     Interactive Face Detection C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/interactive_face_detection_demo/cpp_gapi/README.html">
     G-API Interactive Face Detection Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/machine_translation_demo/python/README.html">
     Machine Translation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/mask_rcnn_demo/cpp/README.html">
     TensorFlow* Object Detection Mask R-CNNs Segmentation C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/monodepth_demo/python/README.html">
     MonoDepth Python Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/mri_reconstruction_demo/cpp/README.html">
     MRI Reconstruction C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/mri_reconstruction_demo/python/README.html">
     MRI Reconstruction Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/multi_camera_multi_target_tracking_demo/python/README.html">
     Multi Camera Multi Target Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/multi_channel_face_detection_demo/cpp/README.html">
     Multi-Channel Face Detection C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/multi_channel_human_pose_estimation_demo/cpp/README.html">
     Multi-Channel Human Pose Estimation C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/multi_channel_object_detection_demo_yolov3/cpp/README.html">
     Multi-Channel Object Detection Yolov3 C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/noise_suppression_demo/cpp/README.html">
     Noise Suppression C++* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/noise_suppression_demo/python/README.html">
     Noise Suppression Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/object_detection_demo/cpp/README.html">
     Object Detection C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/object_detection_demo/python/README.html">
     Object Detection Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/pedestrian_tracker_demo/cpp/README.html">
     Pedestrian Tracker C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/place_recognition_demo/python/README.html">
     Place Recognition Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/security_barrier_camera_demo/cpp/README.html">
     Security Barrier Camera C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/segmentation_demo/cpp/README.html">
     Image Segmentation C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/segmentation_demo/python/README.html">
     Image Segmentation Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/single_human_pose_estimation_demo/python/README.html">
     Single Human Pose Estimation Demo (top-down pipeline)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/smart_classroom_demo/cpp/README.html">
     Smart Classroom C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/smart_classroom_demo/cpp_gapi/README.html">
     Smart Classroom C++ G-API Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/smartlab_demo/python/README.html">
     Smartlab Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/social_distance_demo/cpp/README.html">
     Social Distance C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/sound_classification_demo/python/README.html">
     Sound Classification Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/speech_recognition_deepspeech_demo/python/README.html">
     Speech Recognition DeepSpeech Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/speech_recognition_quartznet_demo/python/README.html">
     Speech Recognition QuartzNet Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/speech_recognition_wav2vec_demo/python/README.html">
     Speech Recognition Wav2Vec Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/text_detection_demo/cpp/README.html">
     Text Detection C++ Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/text_spotting_demo/python/README.html">
     Text Spotting Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/text_to_speech_demo/python/README.html">
     Text-to-speech Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/time_series_forecasting_demo/python/README.html">
     Time Series Forecasting Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/whiteboard_inpainting_demo/python/README.html">
     Whiteboard Inpainting Python* Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../demos/common/python/openvino/model_zoo/model_api/adapters/ovms_adapter.html">
     OpenVINO Model Server Adapter
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Model API
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../demos/common/python/openvino/model_zoo/model_api/adapters/ovms_adapter.html">
   OpenVINO Model Server Adapter
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#object-detection-models">
   Object Detection Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#object-recognition-models">
   Object Recognition Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reidentification-models">
   Reidentification Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#semantic-segmentation-models">
   Semantic Segmentation Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#instance-segmentation-models">
   Instance Segmentation Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#human-pose-estimation-models">
   Human Pose Estimation Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-processing">
   Image Processing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#text-detection">
   Text Detection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#text-recognition">
   Text Recognition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#text-spotting">
   Text Spotting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#action-recognition-models">
   Action Recognition Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-retrieval">
   Image Retrieval
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compressed-models">
   Compressed models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#question-answering">
   Question Answering
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-translation">
   Machine Translation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#text-to-speech">
   Text To Speech
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#speech-noise-suppression">
   Speech Noise Suppression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#time-series-forecasting">
   Time Series Forecasting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#action-sequence-modeling">
   Action Sequence Modeling
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#see-also">
   See Also
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#legal-information">
   Legal Information
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <section id="overview-of-openvino-toolkit-intel-s-pre-trained-models">
<h1>Overview of OpenVINO™ Toolkit Intel’s Pre-Trained Models<a class="headerlink" href="#overview-of-openvino-toolkit-intel-s-pre-trained-models" title="Permalink to this headline">¶</a></h1>
<!--
@sphinxdirective

.. toctree::
   :maxdepth: 1
   :hidden:

   omz_models_intel_device_support
   omz_models_model_action_recognition_0001
   omz_models_model_age_gender_recognition_retail_0013
   omz_models_model_asl_recognition_0004
   omz_models_model_bert_large_uncased_whole_word_masking_squad_0001
   omz_models_model_bert_large_uncased_whole_word_masking_squad_emb_0001
   omz_models_model_bert_large_uncased_whole_word_masking_squad_int8_0001
   omz_models_model_bert_small_uncased_whole_word_masking_squad_0001
   omz_models_model_bert_small_uncased_whole_word_masking_squad_0002
   omz_models_model_bert_small_uncased_whole_word_masking_squad_emb_int8_0001
   omz_models_model_bert_small_uncased_whole_word_masking_squad_int8_0002
   omz_models_model_common_sign_language_0002
   omz_models_model_driver_action_recognition_adas_0002
   omz_models_model_emotions_recognition_retail_0003
   omz_models_model_face_detection_0200
   omz_models_model_face_detection_0202
   omz_models_model_face_detection_0204
   omz_models_model_face_detection_0205
   omz_models_model_face_detection_0206
   omz_models_model_face_detection_adas_0001
   omz_models_model_face_detection_retail_0004
   omz_models_model_face_detection_retail_0005
   omz_models_model_face_reidentification_retail_0095
   omz_models_model_facial_landmarks_35_adas_0002
   omz_models_model_facial_landmarks_98_detection_0001
   omz_models_model_faster_rcnn_resnet101_coco_sparse_60_0001
   omz_models_model_formula_recognition_medium_scan_0001
   omz_models_model_formula_recognition_polynomials_handwritten_0001
   omz_models_model_gaze_estimation_adas_0002
   omz_models_model_handwritten_english_recognition_0001
   omz_models_model_handwritten_japanese_recognition_0001
   omz_models_model_handwritten_score_recognition_0003
   omz_models_model_handwritten_simplified_chinese_recognition_0001
   omz_models_model_head_pose_estimation_adas_0001
   omz_models_model_horizontal_text_detection_0001
   omz_models_model_human_pose_estimation_0001
   omz_models_model_human_pose_estimation_0005
   omz_models_model_human_pose_estimation_0006
   omz_models_model_human_pose_estimation_0007
   omz_models_model_icnet_camvid_ava_0001
   omz_models_model_icnet_camvid_ava_sparse_30_0001
   omz_models_model_icnet_camvid_ava_sparse_60_0001
   omz_models_model_image_retrieval_0001
   omz_models_model_instance_segmentation_person_0007
   omz_models_model_instance_segmentation_security_0002
   omz_models_model_instance_segmentation_security_0091
   omz_models_model_instance_segmentation_security_0228
   omz_models_model_instance_segmentation_security_1039
   omz_models_model_instance_segmentation_security_1040
   omz_models_model_landmarks_regression_retail_0009
   omz_models_model_license_plate_recognition_barrier_0001
   omz_models_model_machine_translation_nar_de_en_0002
   omz_models_model_machine_translation_nar_en_de_0002
   omz_models_model_machine_translation_nar_en_ru_0002
   omz_models_model_machine_translation_nar_ru_en_0002
   omz_models_model_noise_suppression_denseunet_ll_0001
   omz_models_model_noise_suppression_poconetlike_0001
   omz_models_model_pedestrian_and_vehicle_detector_adas_0001
   omz_models_model_pedestrian_detection_adas_0002
   omz_models_model_person_attributes_recognition_crossroad_0230
   omz_models_model_person_attributes_recognition_crossroad_0234
   omz_models_model_person_attributes_recognition_crossroad_0238
   omz_models_model_person_detection_0106
   omz_models_model_person_detection_0200
   omz_models_model_person_detection_0201
   omz_models_model_person_detection_0202
   omz_models_model_person_detection_0203
   omz_models_model_person_detection_0301
   omz_models_model_person_detection_0302
   omz_models_model_person_detection_0303
   omz_models_model_person_detection_action_recognition_0005
   omz_models_model_person_detection_action_recognition_0006
   omz_models_model_person_detection_action_recognition_teacher_0002
   omz_models_model_person_detection_asl_0001
   omz_models_model_person_detection_raisinghand_recognition_0001
   omz_models_model_person_detection_retail_0002
   omz_models_model_person_detection_retail_0013
   omz_models_model_person_reidentification_retail_0277
   omz_models_model_person_reidentification_retail_0286
   omz_models_model_person_reidentification_retail_0287
   omz_models_model_person_reidentification_retail_0288
   omz_models_model_person_vehicle_bike_detection_2000
   omz_models_model_person_vehicle_bike_detection_2001
   omz_models_model_person_vehicle_bike_detection_2002
   omz_models_model_person_vehicle_bike_detection_2003
   omz_models_model_person_vehicle_bike_detection_2004
   omz_models_model_person_vehicle_bike_detection_crossroad_0078
   omz_models_model_person_vehicle_bike_detection_crossroad_1016
   omz_models_model_person_vehicle_bike_detection_crossroad_yolov3_1020
   omz_models_model_product_detection_0001
   omz_models_model_resnet18_xnor_binary_onnx_0001
   omz_models_model_resnet50_binary_0001
   omz_models_model_road_segmentation_adas_0001
   omz_models_model_semantic_segmentation_adas_0001
   omz_models_model_single_image_super_resolution_1032
   omz_models_model_single_image_super_resolution_1033
   omz_models_model_smartlab_object_detection_0001
   omz_models_model_smartlab_object_detection_0002
   omz_models_model_smartlab_object_detection_0003
   omz_models_model_smartlab_object_detection_0004
   omz_models_model_smartlab_sequence_modelling_0001
   omz_models_model_text_detection_0003
   omz_models_model_text_detection_0004
   omz_models_model_text_image_super_resolution_0001
   omz_models_model_text_recognition_0012
   omz_models_model_text_recognition_0014
   omz_models_model_text_recognition_0015
   omz_models_model_text_recognition_0016
   omz_models_model_text_spotting_0005
   omz_models_model_text_to_speech_en_0001
   omz_models_model_text_to_speech_en_multi_0001
   omz_models_model_time_series_forecasting_electricity_0001
   omz_models_model_unet_camvid_onnx_0001
   omz_models_model_vehicle_attributes_recognition_barrier_0039
   omz_models_model_vehicle_attributes_recognition_barrier_0042
   omz_models_model_vehicle_detection_0200
   omz_models_model_vehicle_detection_0201
   omz_models_model_vehicle_detection_0202
   omz_models_model_vehicle_detection_adas_0002
   omz_models_model_vehicle_license_plate_detection_barrier_0106
   omz_models_model_weld_porosity_detection_0001
   omz_models_model_yolo_v2_ava_0001
   omz_models_model_yolo_v2_ava_sparse_35_0001
   omz_models_model_yolo_v2_ava_sparse_70_0001
   omz_models_model_yolo_v2_tiny_ava_0001
   omz_models_model_yolo_v2_tiny_ava_sparse_30_0001
   omz_models_model_yolo_v2_tiny_ava_sparse_60_0001
   omz_models_model_yolo_v2_tiny_vehicle_detection_0001
   

.. raw:: html

   <script>
      window.TABLE_SORT = true;
   </script>

@endsphinxdirective
-->
<p>OpenVINO™ toolkit provides a set of Intel’s pre-trained models
that you can use for learning and demo purposes or for developing deep learning
software. Most recent version is available in the <a class="reference external" href="https://github.com/openvinotoolkit/open_model_zoo">repo on GitHub</a>.
The table <a class="reference internal" href="device_support.html"><span class="doc std std-doc">Intel’s Pre-Trained Models Device Support</span></a> summarizes devices supported by each model.</p>
<p>The models can be downloaded via <span class="xref myst">Model Downloader</span>.</p>
<section id="object-detection-models">
<h2>Object Detection Models<a class="headerlink" href="#object-detection-models" title="Permalink to this headline">¶</a></h2>
<p>Several detection models can be used to detect a set of the most popular
objects - for example, faces, people, vehicles. Most of the networks are
SSD-based and provide reasonable accuracy/performance trade-offs. Networks that
detect the same types of objects (for example, <code class="docutils literal notranslate"><span class="pre">face-detection-adas-0001</span></code> and
<code class="docutils literal notranslate"><span class="pre">face-detection-retail-0004</span></code>) provide a choice for higher accuracy/wider
applicability at the cost of slower performance, so you can expect a “bigger”
network to detect objects of the same type better.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="faster-rcnn-resnet101-coco-sparse-60-0001/README.html"><span class="doc std std-doc">faster-rcnn-resnet101-coco-sparse-60-0001</span></a></p></td>
<td><p>364.21</p></td>
<td><p>52.79</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="face-detection-adas-0001/README.html"><span class="doc std std-doc">face-detection-adas-0001</span></a></p></td>
<td><p>2.835</p></td>
<td><p>1.053</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="face-detection-retail-0004/README.html"><span class="doc std std-doc">face-detection-retail-0004</span></a></p></td>
<td><p>1.067</p></td>
<td><p>0.588</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="face-detection-retail-0005/README.html"><span class="doc std std-doc">face-detection-retail-0005</span></a></p></td>
<td><p>0.982</p></td>
<td><p>1.021</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="face-detection-0200/README.html"><span class="doc std std-doc">face-detection-0200</span></a></p></td>
<td><p>0.785</p></td>
<td><p>1.828</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="face-detection-0202/README.html"><span class="doc std std-doc">face-detection-0202</span></a></p></td>
<td><p>1.767</p></td>
<td><p>1.842</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="face-detection-0204/README.html"><span class="doc std std-doc">face-detection-0204</span></a></p></td>
<td><p>2.405</p></td>
<td><p>1.851</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="face-detection-0205/README.html"><span class="doc std std-doc">face-detection-0205</span></a></p></td>
<td><p>2.853</p></td>
<td><p>2.392</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="face-detection-0206/README.html"><span class="doc std std-doc">face-detection-0206</span></a></p></td>
<td><p>339.597</p></td>
<td><p>69.920</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-detection-retail-0002/README.html"><span class="doc std std-doc">person-detection-retail-0002</span></a></p></td>
<td><p>12.427</p></td>
<td><p>3.244</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="person-detection-retail-0013/README.html"><span class="doc std std-doc">person-detection-retail-0013</span></a></p></td>
<td><p>2.300</p></td>
<td><p>0.723</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-detection-action-recognition-0005/README.html"><span class="doc std std-doc">person-detection-action-recognition-0005</span></a></p></td>
<td><p>7.140</p></td>
<td><p>1.951</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="person-detection-action-recognition-0006/README.html"><span class="doc std std-doc">person-detection-action-recognition-0006</span></a></p></td>
<td><p>8.225</p></td>
<td><p>2.001</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-detection-action-recognition-teacher-0002/README.html"><span class="doc std std-doc">person-detection-action-recognition-teacher-0002</span></a></p></td>
<td><p>7.140</p></td>
<td><p>1.951</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="person-detection-raisinghand-recognition-0001/README.html"><span class="doc std std-doc">person-detection-raisinghand-recognition-0001</span></a></p></td>
<td><p>7.138</p></td>
<td><p>1.951</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-detection-0200/README.html"><span class="doc std std-doc">person-detection-0200</span></a></p></td>
<td><p>0.786</p></td>
<td><p>1.817</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="person-detection-0201/README.html"><span class="doc std std-doc">person-detection-0201</span></a></p></td>
<td><p>1.768</p></td>
<td><p>1.817</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-detection-0202/README.html"><span class="doc std std-doc">person-detection-0202</span></a></p></td>
<td><p>3.143</p></td>
<td><p>1.817</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="person-detection-0203/README.html"><span class="doc std std-doc">person-detection-0203</span></a></p></td>
<td><p>6.519</p></td>
<td><p>2.394</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-detection-0301/README.html"><span class="doc std std-doc">person-detection-0301</span></a></p></td>
<td><p>79318.2158</p></td>
<td><p>55.557</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="person-detection-0302/README.html"><span class="doc std std-doc">person-detection-0302</span></a></p></td>
<td><p>370.208</p></td>
<td><p>51.164</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-detection-0303/README.html"><span class="doc std std-doc">person-detection-0303</span></a></p></td>
<td><p>24.758</p></td>
<td><p>3.630</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="person-detection-0106/README.html"><span class="doc std std-doc">person-detection-0106</span></a></p></td>
<td><p>404.264</p></td>
<td><p>71.565</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="pedestrian-detection-adas-0002/README.html"><span class="doc std std-doc">pedestrian-detection-adas-0002</span></a></p></td>
<td><p>2.836</p></td>
<td><p>1.165</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="pedestrian-and-vehicle-detector-adas-0001/README.html"><span class="doc std std-doc">pedestrian-and-vehicle-detector-adas-0001</span></a></p></td>
<td><p>3.974</p></td>
<td><p>1.650</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="vehicle-detection-adas-0002/README.html"><span class="doc std std-doc">vehicle-detection-adas-0002</span></a></p></td>
<td><p>2.798</p></td>
<td><p>1.079</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="vehicle-detection-0200/README.html"><span class="doc std std-doc">vehicle-detection-0200</span></a></p></td>
<td><p>0.786</p></td>
<td><p>1.817</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="vehicle-detection-0201/README.html"><span class="doc std std-doc">vehicle-detection-0201</span></a></p></td>
<td><p>1.768</p></td>
<td><p>1.817</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="vehicle-detection-0202/README.html"><span class="doc std std-doc">vehicle-detection-0202</span></a></p></td>
<td><p>3.143</p></td>
<td><p>1.817</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-vehicle-bike-detection-crossroad-0078/README.html"><span class="doc std std-doc">person-vehicle-bike-detection-crossroad-0078</span></a></p></td>
<td><p>3.964</p></td>
<td><p>1.178</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="person-vehicle-bike-detection-crossroad-1016/README.html"><span class="doc std std-doc">person-vehicle-bike-detection-crossroad-1016</span></a></p></td>
<td><p>3.560</p></td>
<td><p>2.887</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-vehicle-bike-detection-crossroad-yolov3-1020/README.html"><span class="doc std std-doc">person-vehicle-bike-detection-crossroad-yolov3-1020</span></a></p></td>
<td><p>65.984</p></td>
<td><p>61.922</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="person-vehicle-bike-detection-2000/README.html"><span class="doc std std-doc">person-vehicle-bike-detection-2000</span></a></p></td>
<td><p>0.787</p></td>
<td><p>1.821</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-vehicle-bike-detection-2001/README.html"><span class="doc std std-doc">person-vehicle-bike-detection-2001</span></a></p></td>
<td><p>1.770</p></td>
<td><p>1.821</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="person-vehicle-bike-detection-2002/README.html"><span class="doc std std-doc">person-vehicle-bike-detection-2002</span></a></p></td>
<td><p>3.163</p></td>
<td><p>1.821</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-vehicle-bike-detection-2003/README.html"><span class="doc std std-doc">person-vehicle-bike-detection-2003</span></a></p></td>
<td><p>6.550</p></td>
<td><p>2.416</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="person-vehicle-bike-detection-2004/README.html"><span class="doc std std-doc">person-vehicle-bike-detection-2004</span></a></p></td>
<td><p>1.811</p></td>
<td><p>2.327</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="vehicle-license-plate-detection-barrier-0106/README.html"><span class="doc std std-doc">vehicle-license-plate-detection-barrier-0106</span></a></p></td>
<td><p>0.349</p></td>
<td><p>0.634</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="product-detection-0001/README.html"><span class="doc std std-doc">product-detection-0001</span></a></p></td>
<td><p>3.598</p></td>
<td><p>3.212</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-detection-asl-0001/README.html"><span class="doc std std-doc">person-detection-asl-0001</span></a></p></td>
<td><p>0.986</p></td>
<td><p>1.338</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="yolo-v2-ava-0001/README.html"><span class="doc std std-doc">yolo-v2-ava-0001</span></a></p></td>
<td><p>29.38</p></td>
<td><p>48.29</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="yolo-v2-ava-sparse-35-0001/README.html"><span class="doc std std-doc">yolo-v2-ava-sparse-35-0001</span></a></p></td>
<td><p>29.38</p></td>
<td><p>48.29</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="yolo-v2-ava-sparse-70-0001/README.html"><span class="doc std std-doc">yolo-v2-ava-sparse-70-0001</span></a></p></td>
<td><p>29.38</p></td>
<td><p>48.29</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="yolo-v2-tiny-ava-0001/README.html"><span class="doc std std-doc">yolo-v2-tiny-ava-0001</span></a></p></td>
<td><p>6.975</p></td>
<td><p>15.12</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="yolo-v2-tiny-ava-sparse-30-0001/README.html"><span class="doc std std-doc">yolo-v2-tiny-ava-sparse-30-0001</span></a></p></td>
<td><p>6.975</p></td>
<td><p>15.12</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="yolo-v2-tiny-ava-sparse-60-0001/README.html"><span class="doc std std-doc">yolo-v2-tiny-ava-sparse-60-0001</span></a></p></td>
<td><p>6.975</p></td>
<td><p>15.12</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="yolo-v2-tiny-vehicle-detection-0001/README.html"><span class="doc std std-doc">yolo-v2-tiny-vehicle-detection-0001</span></a></p></td>
<td><p>5.424</p></td>
<td><p>11.229</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="smartlab-object-detection-0001/README.html"><span class="doc std std-doc">smartlab-object-detection-0001</span></a></p></td>
<td><p>1.077</p></td>
<td><p>0.8908</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="smartlab-object-detection-0002/README.html"><span class="doc std std-doc">smartlab-object-detection-0002</span></a></p></td>
<td><p>1.073</p></td>
<td><p>0.8894</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="smartlab-object-detection-0003/README.html"><span class="doc std std-doc">smartlab-object-detection-0003</span></a></p></td>
<td><p>1.077</p></td>
<td><p>0.8908</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="smartlab-object-detection-0004/README.html"><span class="doc std std-doc">smartlab-object-detection-0004</span></a></p></td>
<td><p>1.073</p></td>
<td><p>0.8894</p></td>
</tr>
</tbody>
</table>
</section>
<section id="object-recognition-models">
<h2>Object Recognition Models<a class="headerlink" href="#object-recognition-models" title="Permalink to this headline">¶</a></h2>
<p>Object recognition models are used for classification, regression, and character
recognition. Use these networks after a respective detector (for example,
Age/Gender recognition after Face Detection).</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="age-gender-recognition-retail-0013/README.html"><span class="doc std std-doc">age-gender-recognition-retail-0013</span></a></p></td>
<td><p>0.094</p></td>
<td><p>2.138</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="head-pose-estimation-adas-0001/README.html"><span class="doc std std-doc">head-pose-estimation-adas-0001</span></a></p></td>
<td><p>0.105</p></td>
<td><p>1.911</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="license-plate-recognition-barrier-0001/README.html"><span class="doc std std-doc">license-plate-recognition-barrier-0001</span></a></p></td>
<td><p>0.328</p></td>
<td><p>1.218</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="vehicle-attributes-recognition-barrier-0039/README.html"><span class="doc std std-doc">vehicle-attributes-recognition-barrier-0039</span></a></p></td>
<td><p>0.126</p></td>
<td><p>0.626</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="vehicle-attributes-recognition-barrier-0042/README.html"><span class="doc std std-doc">vehicle-attributes-recognition-barrier-0042</span></a></p></td>
<td><p>0.462</p></td>
<td><p>11.177</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="emotions-recognition-retail-0003/README.html"><span class="doc std std-doc">emotions-recognition-retail-0003</span></a></p></td>
<td><p>0.126</p></td>
<td><p>2.483</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="landmarks-regression-retail-0009/README.html"><span class="doc std std-doc">landmarks-regression-retail-0009</span></a></p></td>
<td><p>0.021</p></td>
<td><p>0.191</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="facial-landmarks-98-detection-0001/README.html"><span class="doc std std-doc">facial-landmarks-98-detection-0001</span></a></p></td>
<td><p>0.6</p></td>
<td><p>9.66</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="facial-landmarks-35-adas-0002/README.html"><span class="doc std std-doc">facial-landmarks-35-adas-0002</span></a></p></td>
<td><p>0.042</p></td>
<td><p>4.595</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-attributes-recognition-crossroad-0230/README.html"><span class="doc std std-doc">person-attributes-recognition-crossroad-0230</span></a></p></td>
<td><p>0.174</p></td>
<td><p>0.735</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="person-attributes-recognition-crossroad-0234/README.html"><span class="doc std std-doc">person-attributes-recognition-crossroad-0234</span></a></p></td>
<td><p>2.167</p></td>
<td><p>23.510</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-attributes-recognition-crossroad-0238/README.html"><span class="doc std std-doc">person-attributes-recognition-crossroad-0238</span></a></p></td>
<td><p>1.034</p></td>
<td><p>21.797</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="gaze-estimation-adas-0002/README.html"><span class="doc std std-doc">gaze-estimation-adas-0002</span></a></p></td>
<td><p>0.139</p></td>
<td><p>1.882</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reidentification-models">
<h2>Reidentification Models<a class="headerlink" href="#reidentification-models" title="Permalink to this headline">¶</a></h2>
<p>Precise tracking of objects in a video is a common application of Computer
Vision (for example, for people counting). It is often complicated by a set of
events that can be described as a “relatively long absence of an object”. For
example, it can be caused by occlusion or out-of-frame movement. In such cases,
it is better to recognize the object as “seen before” regardless of its current
position in an image or the amount of time passed since last known position.</p>
<p>The following networks can be used in such scenarios. They take an image of a
person and evaluate an embedding - a vector in high-dimensional space that
represents an appearance of this person. This vector can be used for further
evaluation: images that correspond to the same person will have embedding
vectors that are “close” by L2 metric (Euclidean distance).</p>
<p>There are multiple models that provide various trade-offs between performance
and accuracy (expect a bigger model to perform better).</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="face-reidentification-retail-0095/README.html"><span class="doc std std-doc">face-reidentification-retail-0095</span></a></p></td>
<td><p>0.588</p></td>
<td><p>1.107</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-reidentification-retail-0288/README.html"><span class="doc std std-doc">person-reidentification-retail-0288</span></a></p></td>
<td><p>0.174</p></td>
<td><p>0.183</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="person-reidentification-retail-0287/README.html"><span class="doc std std-doc">person-reidentification-retail-0287</span></a></p></td>
<td><p>0.564</p></td>
<td><p>0.595</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="person-reidentification-retail-0286/README.html"><span class="doc std std-doc">person-reidentification-retail-0286</span></a></p></td>
<td><p>1.170</p></td>
<td><p>1.234</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="person-reidentification-retail-0277/README.html"><span class="doc std std-doc">person-reidentification-retail-0277</span></a></p></td>
<td><p>1.993</p></td>
<td><p>2.103</p></td>
</tr>
</tbody>
</table>
</section>
<section id="semantic-segmentation-models">
<h2>Semantic Segmentation Models<a class="headerlink" href="#semantic-segmentation-models" title="Permalink to this headline">¶</a></h2>
<p>Semantic segmentation is an extension of object detection problem. Instead of
returning bounding boxes, semantic segmentation models return a “painted”
version of the input image, where the “color” of each pixel represents a certain
class. These networks are much bigger than respective object detection networks,
but they provide a better (pixel-level) localization of objects and they can
detect areas with complex shape (for example, free space on the road).</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="road-segmentation-adas-0001/README.html"><span class="doc std std-doc">road-segmentation-adas-0001</span></a></p></td>
<td><p>4.770</p></td>
<td><p>0.184</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="semantic-segmentation-adas-0001/README.html"><span class="doc std std-doc">semantic-segmentation-adas-0001</span></a></p></td>
<td><p>58.572</p></td>
<td><p>6.686</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="unet-camvid-onnx-0001/README.html"><span class="doc std std-doc">unet-camvid-onnx-0001</span></a></p></td>
<td><p>260.1</p></td>
<td><p>31.03</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="icnet-camvid-ava-0001/README.html"><span class="doc std std-doc">icnet-camvid-ava-0001</span></a></p></td>
<td><p>151.82</p></td>
<td><p>25.45</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="icnet-camvid-ava-sparse-30-0001/README.html"><span class="doc std std-doc">icnet-camvid-ava-sparse-30-0001</span></a></p></td>
<td><p>151.82</p></td>
<td><p>25.45</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="icnet-camvid-ava-sparse-60-0001/README.html"><span class="doc std std-doc">icnet-camvid-ava-sparse-60-0001</span></a></p></td>
<td><p>151.82</p></td>
<td><p>25.45</p></td>
</tr>
</tbody>
</table>
</section>
<section id="instance-segmentation-models">
<h2>Instance Segmentation Models<a class="headerlink" href="#instance-segmentation-models" title="Permalink to this headline">¶</a></h2>
<p>Instance segmentation is an extension of object detection and semantic
segmentation problems. Instead of predicting a bounding box around each object
instance instance segmentation model outputs pixel-wise masks for all instances.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="instance-segmentation-security-0002/README.html"><span class="doc std std-doc">instance-segmentation-security-0002</span></a></p></td>
<td><p>423.0842</p></td>
<td><p>48.3732</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="instance-segmentation-security-0091/README.html"><span class="doc std std-doc">instance-segmentation-security-0091</span></a></p></td>
<td><p>828.6324</p></td>
<td><p>101.236</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="instance-segmentation-security-0228/README.html"><span class="doc std std-doc">instance-segmentation-security-0228</span></a></p></td>
<td><p>147.2352</p></td>
<td><p>49.8328</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="instance-segmentation-security-1039/README.html"><span class="doc std std-doc">instance-segmentation-security-1039</span></a></p></td>
<td><p>13.9672</p></td>
<td><p>10.5674</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="instance-segmentation-security-1040/README.html"><span class="doc std std-doc">instance-segmentation-security-1040</span></a></p></td>
<td><p>29.334</p></td>
<td><p>13.5673</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="instance-segmentation-person-0007/README.html"><span class="doc std std-doc">instance-segmentation-person-0007</span></a></p></td>
<td><p>4.8492</p></td>
<td><p>7.2996</p></td>
</tr>
</tbody>
</table>
</section>
<section id="human-pose-estimation-models">
<h2>Human Pose Estimation Models<a class="headerlink" href="#human-pose-estimation-models" title="Permalink to this headline">¶</a></h2>
<p>Human pose estimation task is to predict a pose: body skeleton, which consists
of keypoints and connections between them, for every person in an input image or
video.  Keypoints are body joints, i.e. ears, eyes, nose, shoulders, knees, etc.
There are two major groups of such methods: top-down and bottom-up.  The first
detects persons in a given frame, crops or rescales detections, then runs pose
estimation network for every detection. These methods are very accurate. The
second finds all keypoints in a given frame, then groups them by person
instances, thus faster than previous, because network runs once.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="human-pose-estimation-0001/README.html"><span class="doc std std-doc">human-pose-estimation-0001</span></a></p></td>
<td><p>15.435</p></td>
<td><p>4.099</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="human-pose-estimation-0005/README.html"><span class="doc std std-doc">human-pose-estimation-0005</span></a></p></td>
<td><p>5.9393</p></td>
<td><p>8.1504</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="human-pose-estimation-0006/README.html"><span class="doc std std-doc">human-pose-estimation-0006</span></a></p></td>
<td><p>8.8720</p></td>
<td><p>8.1504</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="human-pose-estimation-0007/README.html"><span class="doc std std-doc">human-pose-estimation-0007</span></a></p></td>
<td><p>14.3707</p></td>
<td><p>8.1504</p></td>
</tr>
</tbody>
</table>
</section>
<section id="image-processing">
<h2>Image Processing<a class="headerlink" href="#image-processing" title="Permalink to this headline">¶</a></h2>
<p>Deep Learning models find their application in various image processing tasks to
increase the quality of the output.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="single-image-super-resolution-1032/README.html"><span class="doc std std-doc">single-image-super-resolution-1032</span></a></p></td>
<td><p>11.654</p></td>
<td><p>0.030</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="single-image-super-resolution-1033/README.html"><span class="doc std std-doc">single-image-super-resolution-1033</span></a></p></td>
<td><p>30.97</p></td>
<td><p>16.062</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="text-image-super-resolution-0001/README.html"><span class="doc std std-doc">text-image-super-resolution-0001</span></a></p></td>
<td><p>1.379</p></td>
<td><p>0.003</p></td>
</tr>
</tbody>
</table>
</section>
<section id="text-detection">
<h2>Text Detection<a class="headerlink" href="#text-detection" title="Permalink to this headline">¶</a></h2>
<p>Deep Learning models for text detection in various applications.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="text-detection-0003/README.html"><span class="doc std std-doc">text-detection-0003</span></a></p></td>
<td><p>51.256</p></td>
<td><p>6.747</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="text-detection-0004/README.html"><span class="doc std std-doc">text-detection-0004</span></a></p></td>
<td><p>23.305</p></td>
<td><p>4.328</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="horizontal-text-detection-0001/README.html"><span class="doc std std-doc">horizontal-text-detection-0001</span></a></p></td>
<td><p>7.718</p></td>
<td><p>2.259</p></td>
</tr>
</tbody>
</table>
</section>
<section id="text-recognition">
<h2>Text Recognition<a class="headerlink" href="#text-recognition" title="Permalink to this headline">¶</a></h2>
<p>Deep Learning models for text recognition in various applications.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="text-recognition-0012/README.html"><span class="doc std std-doc">text-recognition-0012</span></a></p></td>
<td><p>1.485</p></td>
<td><p>5.568</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="text-recognition-0014/README.html"><span class="doc std std-doc">text-recognition-0014</span></a></p></td>
<td><p>0.5442</p></td>
<td><p>2.839</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="text-recognition-0015/README.html"><span class="doc std std-doc">text-recognition-0015</span></a></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>encoder</p></td>
<td><p>12.4</p></td>
<td><p>398</p></td>
</tr>
<tr class="row-even"><td><p>decoder</p></td>
<td><p>0.03</p></td>
<td><p>4.33</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="text-recognition-0016/README.html"><span class="doc std std-doc">text-recognition-0016</span></a></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>encoder</p></td>
<td><p>9.27</p></td>
<td><p>88.1</p></td>
</tr>
<tr class="row-odd"><td><p>decoder</p></td>
<td><p>0.08</p></td>
<td><p>4.28</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="handwritten-score-recognition-0003/README.html"><span class="doc std std-doc">handwritten-score-recognition-0003</span></a></p></td>
<td><p>0.792</p></td>
<td><p>5.555</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="handwritten-japanese-recognition-0001/README.html"><span class="doc std std-doc">handwritten-japanese-recognition-0001</span></a></p></td>
<td><p>117.136</p></td>
<td><p>15.31</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="handwritten-simplified-chinese-recognition-0001/README.html"><span class="doc std std-doc">handwritten-simplified-chinese-recognition-0001</span></a></p></td>
<td><p>134.513</p></td>
<td><p>17.270</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="handwritten-english-recognition-0001/README.html"><span class="doc std std-doc">handwritten-english-recognition-0001</span></a></p></td>
<td><p>1.3182</p></td>
<td><p>0.1413</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="formula-recognition-medium-scan-0001/README.html"><span class="doc std std-doc">formula-recognition-medium-scan-0001</span></a></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>encoder</p></td>
<td><p>16.56</p></td>
<td><p>1.86</p></td>
</tr>
<tr class="row-even"><td><p>decoder</p></td>
<td><p>1.69</p></td>
<td><p>2.56</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="formula-recognition-polynomials-handwritten-0001/README.html"><span class="doc std std-doc">formula-recognition-polynomials-handwritten-0001</span></a></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>encoder</p></td>
<td><p>12.8447</p></td>
<td><p>0.2017</p></td>
</tr>
<tr class="row-odd"><td><p>decoder</p></td>
<td><p>8.6838</p></td>
<td><p>2.5449</p></td>
</tr>
</tbody>
</table>
</section>
<section id="text-spotting">
<h2>Text Spotting<a class="headerlink" href="#text-spotting" title="Permalink to this headline">¶</a></h2>
<p>Deep Learning models for text spotting (simultaneous detection and recognition).</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="text-spotting-0005/README.html"><span class="doc std std-doc">text-spotting-0005</span></a></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>text-spotting-0005-detector</p></td>
<td><p>184.495</p></td>
<td><p>27.010</p></td>
</tr>
<tr class="row-even"><td><p>text-spotting-0005-recognizer-encoder</p></td>
<td><p>2.082</p></td>
<td><p>1.328</p></td>
</tr>
<tr class="row-odd"><td><p>text-spotting-0005-recognizer-decoder</p></td>
<td><p>0.002</p></td>
<td><p>0.273</p></td>
</tr>
</tbody>
</table>
</section>
<section id="action-recognition-models">
<h2>Action Recognition Models<a class="headerlink" href="#action-recognition-models" title="Permalink to this headline">¶</a></h2>
<p>Action Recognition models predict action that is being performed on a short video clip
(tensor formed by stacking sampled frames from input video). Some models (for example <code class="docutils literal notranslate"><span class="pre">driver-action-recognition-adas-0002</span></code> may use precomputed high-level spatial
or spatio-temporal) features (embeddings) from individual clip fragments and then aggregate them in a temporal model
to predict a vector with classification scores. Models that compute embeddings are called <em>encoder</em>, while models
that predict an actual labels are called <em>decoder</em>.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="driver-action-recognition-adas-0002/README.html"><span class="doc std std-doc">driver-action-recognition-adas-0002</span></a></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>driver-action-recognition-adas-0002-encoder</p></td>
<td><p>0.676</p></td>
<td><p>2.863</p></td>
</tr>
<tr class="row-even"><td><p>driver-action-recognition-adas-0002-decoder</p></td>
<td><p>0.147</p></td>
<td><p>4.205</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="action-recognition-0001/README.html"><span class="doc std std-doc">action-recognition-0001</span></a></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>action-recognition-0001-encoder</p></td>
<td><p>7.340</p></td>
<td><p>21.276</p></td>
</tr>
<tr class="row-odd"><td><p>action-recognition-0001-decoder</p></td>
<td><p>0.147</p></td>
<td><p>4.405</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="asl-recognition-0004/README.html"><span class="doc std std-doc">asl-recognition-0004</span></a></p></td>
<td><p>6.660</p></td>
<td><p>4.133</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="common-sign-language-0002/README.html"><span class="doc std std-doc">common-sign-language-0002</span></a></p></td>
<td><p>4.227</p></td>
<td><p>4.113</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="weld-porosity-detection-0001/README.html"><span class="doc std std-doc">weld-porosity-detection-0001</span></a></p></td>
<td><p>3.636</p></td>
<td><p>11.173</p></td>
</tr>
</tbody>
</table>
</section>
<section id="image-retrieval">
<h2>Image Retrieval<a class="headerlink" href="#image-retrieval" title="Permalink to this headline">¶</a></h2>
<p>Deep Learning models for image retrieval (ranking ‘gallery’ images according to their similarity to some ‘probe’ image).</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="image-retrieval-0001/README.html"><span class="doc std std-doc">image-retrieval-0001</span></a></p></td>
<td><p>0.613</p></td>
<td><p>2.535</p></td>
</tr>
</tbody>
</table>
</section>
<section id="compressed-models">
<h2>Compressed models<a class="headerlink" href="#compressed-models" title="Permalink to this headline">¶</a></h2>
<p>Deep Learning compressed models</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="resnet50-binary-0001/README.html"><span class="doc std std-doc">resnet50-binary-0001</span></a></p></td>
<td><p>1.002</p></td>
<td><p>7.446</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="resnet18-xnor-binary-onnx-0001/README.html"><span class="doc std std-doc">resnet18-xnor-binary-onnx-0001</span></a></p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
</tbody>
</table>
</section>
<section id="question-answering">
<h2>Question Answering<a class="headerlink" href="#question-answering" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="bert-large-uncased-whole-word-masking-squad-0001/README.html"><span class="doc std std-doc">bert-large-uncased-whole-word-masking-squad-0001</span></a></p></td>
<td><p>246.93</p></td>
<td><p>333.96</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="bert-large-uncased-whole-word-masking-squad-int8-0001/README.html"><span class="doc std std-doc">bert-large-uncased-whole-word-masking-squad-int8-0001</span></a></p></td>
<td><p>246.93</p></td>
<td><p>333.96</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="bert-large-uncased-whole-word-masking-squad-emb-0001/README.html"><span class="doc std std-doc">bert-large-uncased-whole-word-masking-squad-emb-0001</span></a></p></td>
<td><p>246.93 (for [1,384] input size)</p></td>
<td><p>333.96</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="bert-small-uncased-whole-word-masking-squad-0001/README.html"><span class="doc std std-doc">bert-small-uncased-whole-word-masking-squad-0001</span></a></p></td>
<td><p>23.9</p></td>
<td><p>57.94</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="bert-small-uncased-whole-word-masking-squad-0002/README.html"><span class="doc std std-doc">bert-small-uncased-whole-word-masking-squad-0002</span></a></p></td>
<td><p>23.9</p></td>
<td><p>41.1</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="bert-small-uncased-whole-word-masking-squad-int8-0002/README.html"><span class="doc std std-doc">bert-small-uncased-whole-word-masking-squad-int8-0002</span></a></p></td>
<td><p>23.9</p></td>
<td><p>41.1</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="bert-small-uncased-whole-word-masking-squad-emb-int8-0001/README.html"><span class="doc std std-doc">bert-small-uncased-whole-word-masking-squad-emb-int8-0001</span></a></p></td>
<td><p>23.9 (for [1,384] input size)</p></td>
<td><p>41.1</p></td>
</tr>
</tbody>
</table>
</section>
<section id="machine-translation">
<h2>Machine Translation<a class="headerlink" href="#machine-translation" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="machine-translation-nar-en-ru-0002/README.html"><span class="doc std std-doc">machine-translation-nar-en-ru-0002</span></a></p></td>
<td><p>23.17</p></td>
<td><p>69.29</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="machine-translation-nar-ru-en-0002/README.html"><span class="doc std std-doc">machine-translation-nar-ru-en-0002</span></a></p></td>
<td><p>23.17</p></td>
<td><p>69.29</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="machine-translation-nar-en-de-0002/README.html"><span class="doc std std-doc">machine-translation-nar-en-de-0002</span></a></p></td>
<td><p>23.19</p></td>
<td><p>77.47</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="machine-translation-nar-de-en-0002/README.html"><span class="doc std std-doc">machine-translation-nar-de-en-0002</span></a></p></td>
<td><p>23.19</p></td>
<td><p>77.47</p></td>
</tr>
</tbody>
</table>
</section>
<section id="text-to-speech">
<h2>Text To Speech<a class="headerlink" href="#text-to-speech" title="Permalink to this headline">¶</a></h2>
<p>Deep Learning models for speech synthesis (mel spectrogram generation and wave form generation).</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="text-to-speech-en-0001/README.html"><span class="doc std std-doc">text-to-speech-en-0001</span></a></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>text-to-speech-en-0001-duration-prediction</p></td>
<td><p>15.84</p></td>
<td><p>13.569</p></td>
</tr>
<tr class="row-even"><td><p>text-to-speech-en-0001-regression</p></td>
<td><p>7.65</p></td>
<td><p>4.96</p></td>
</tr>
<tr class="row-odd"><td><p>text-to-speech-en-0001-generation</p></td>
<td><p>48.38</p></td>
<td><p>12.77</p></td>
</tr>
</tbody>
</table>
<p>Deep Learning models for speech synthesis (mel spectrogram generation and wave form generation).</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="text-to-speech-en-multi-0001/README.html"><span class="doc std std-doc">text-to-speech-en-multi-0001</span></a></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>text-to-speech-en-multi-0001-duration-prediction</p></td>
<td><p>28.75</p></td>
<td><p>26.18</p></td>
</tr>
<tr class="row-even"><td><p>text-to-speech-en-multi-0001-regression</p></td>
<td><p>7.81</p></td>
<td><p>5.12</p></td>
</tr>
<tr class="row-odd"><td><p>text-to-speech-en-multi-0001-generation</p></td>
<td><p>48.38</p></td>
<td><p>12.77</p></td>
</tr>
</tbody>
</table>
</section>
<section id="speech-noise-suppression">
<h2>Speech Noise Suppression<a class="headerlink" href="#speech-noise-suppression" title="Permalink to this headline">¶</a></h2>
<p>Deep Learning models for noise suppression.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="noise-suppression-poconetlike-0001/README.html"><span class="doc std std-doc">noise-suppression-poconetlike-0001</span></a></p></td>
<td><p>1.2</p></td>
<td><p>7.22</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="noise-suppression-denseunet-ll-0001/README.html"><span class="doc std std-doc">noise-suppression-denseunet-ll-0001</span></a></p></td>
<td><p>0.2</p></td>
<td><p>4.2</p></td>
</tr>
</tbody>
</table>
</section>
<section id="time-series-forecasting">
<h2>Time Series Forecasting<a class="headerlink" href="#time-series-forecasting" title="Permalink to this headline">¶</a></h2>
<p>Deep Learning models for time series forecasting.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="time-series-forecasting-electricity-0001/README.html"><span class="doc std std-doc">time-series-forecasting-electricity-0001</span></a></p></td>
<td><p>0.40</p></td>
<td><p>2.26</p></td>
</tr>
</tbody>
</table>
</section>
<section id="action-sequence-modeling">
<h2>Action Sequence Modeling<a class="headerlink" href="#action-sequence-modeling" title="Permalink to this headline">¶</a></h2>
<p>Deep Learning models for online sequence modeling.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Name</p></th>
<th class="head"><p>Complexity (GFLOPs)</p></th>
<th class="head"><p>Size (Mp)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="smartlab-sequence-modelling-0001/README.html"><span class="doc std std-doc">smartlab-sequence-modelling-0001</span></a></p></td>
<td><p>0.049</p></td>
<td><p>1.02</p></td>
</tr>
</tbody>
</table>
</section>
<section id="see-also">
<h2>See Also<a class="headerlink" href="#see-also" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="../../demos/README.html"><span class="doc std std-doc">Open Model Zoo Demos</span></a></p></li>
<li><p><span class="xref myst">Model Downloader</span></p></li>
<li><p><a class="reference internal" href="../public/index.html"><span class="doc std std-doc">Overview of OpenVINO™ Toolkit Public Pre-Trained Models</span></a></p></li>
</ul>
</section>
<section id="legal-information">
<h2>Legal Information<a class="headerlink" href="#legal-information" title="Permalink to this headline">¶</a></h2>
<p>Caffe, Caffe2, Keras, MXNet, PyTorch, and TensorFlow are trademarks or brand names of their respective owners.
All company, product and service names used in this website are for identification purposes only.
Use of these names,trademarks and brands does not imply endorsement.</p>
<div class="toctree-wrapper compound">
</div>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="../../model_zoo.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="action-recognition-0001/README.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>